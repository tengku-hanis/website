[{"authors":null,"categories":null,"content":"From medical graduate to data enthusiast\nI am a PhD student in the field of public health epidemiology in Universiti Sains Malaysia under the supervision of one of the well-established medical epidemiologist and biostatistician in Malaysia, Assoc Prof Kamarul Imran Musa.\nI did my degree in medicine. However, I believed that working as a doctor in a clinical setting is not for me. Thus, I continued my study in medical statistics. Data and analysis has sparked my interest since then. I believes that coming from medical background, give me an edge to see data in a new perspective.\n  Download my resumé.\n","date":1554595200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://tengkuhanis.netlify.app/author/tengku-muhammad-hanis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tengku-muhammad-hanis/","section":"authors","summary":"From medical graduate to data enthusiast\nI am a PhD student in the field of public health epidemiology in Universiti Sains Malaysia under the supervision of one of the well-established medical epidemiologist and biostatistician in Malaysia, Assoc Prof Kamarul Imran Musa.","tags":null,"title":"Tengku Muhammad Hanis","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bb560906b6a99893cc21387348c0b074","permalink":"https://tengkuhanis.netlify.app/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://tengkuhanis.netlify.app/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://tengkuhanis.netlify.app/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://tengkuhanis.netlify.app/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://tengkuhanis.netlify.app/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":["R","Machine Learning"],"content":"\r\rThis post will not go very detail in each of the approach of hyperparameter tuning. This post mainly aims to summarize a few things that I studied for the last couple of days.\rGenerally, there are two approaches to hyperparameter tuning in tidymodels.\nGrid search:\n– Regular grid search\n– Random grid search\n\rIterative search:\n– Bayesian optimization\n– Simulated annealing\r\rGrid search\rSo, in grid search, we provide the combination of parameters and the algorithm will go through each combination of parameters. There are two types of grid search:\n\rRegular grid search\n– The algorithm will go through each combinations of parameters.\r\rgrid_regular(mtry(c(1, 13)), trees(), min_n(),\rlevels = 3) # how many from each parameter\r## # A tibble: 27 x 3\r## mtry trees min_n\r## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 1 1 2\r## 2 7 1 2\r## 3 13 1 2\r## 4 1 1000 2\r## 5 7 1000 2\r## 6 13 1000 2\r## 7 1 2000 2\r## 8 7 2000 2\r## 9 13 2000 2\r## 10 1 1 21\r## # ... with 17 more rows\r\rRandom grid search\n– The algorithm will randomly select a number of combination of parameters instead of go through each of them.\r\rgrid_random(mtry(c(1, 13)),\rtrees(), min_n(), size = 100) # size of parameters combination\r## # A tibble: 100 x 3\r## mtry trees min_n\r## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 5 1216 40\r## 2 8 1374 13\r## 3 9 859 39\r## 4 6 282 12\r## 5 2 1210 9\r## 6 8 1828 39\r## 7 11 550 14\r## 8 13 1157 32\r## 9 5 282 6\r## 10 10 1018 28\r## # ... with 90 more rows\rBy default, tidymodels uses space-filling-design to make sure the combination of parameters are on “equidistance” to each other.\n\rIterative search\rIn iterative search, we need to specify some initial parameters/values to start the search.\n\rBayesian optimization\n– This algorithm/function will search the next best combination of parameters based on the previous combination of parameters (priori).\rSimulated annealing\n– Generally, this algorithm works relatively similar to bayesian optimization.\n– However, as the figure below illustrates this algorithm is able to explore in the worst combination of parameters for a short term (barrier of local search), in order to find the best combination of parameters (global minima).\r\r\rFuther details on iterative search or both methods above can be found here. So, as both iterative methods need a starting parameters, we can actually combine with any of the grid search methods.\n\rOther methods\rBy default, if we do not supply any combination of parameters, tidymodels will randomly pick 10 combinations of parameters from the default range of values from the model. Additionally, we can set this values to other values as shown below:\ntune_grid(\rresamples = dat_cv, # cross validation data set\rgrid = 20, # 20 combinations of parameters\rcontrol = control, # some control parameters\rmetrics = metrics # some metrics parameters (roc_auc, etc)\r)\rThere are another special cases of grid search; tune_race_anova() and tune_race_win_loss(). Both of these methods supposed to be more efficient way of grid search. In general, both methods evaluate the tuning parameters on a small initial set. The combination of parameters with a worst performance will be eliminated. Thus, makes them more efficient in grid search. The main difference between these two methods is how the worst combination of parameters are evaluated and eliminated.\n\rR codes\rLoad the packages.\n# Packages\rlibrary(tidyverse)\rlibrary(tidymodels)\rlibrary(finetune)\rWe will only use a small chunk of the data for ease of computation.\n# Data\rdata(income, package = \u0026quot;kernlab\u0026quot;)\r# Make data smaller for computation\rset.seed(2021)\rincome2 \u0026lt;- income %\u0026gt;% filter(INCOME == \u0026quot;[75.000-\u0026quot; | INCOME == \u0026quot;[50.000-75.000)\u0026quot;) %\u0026gt;% slice_sample(n = 600) %\u0026gt;% mutate(INCOME = fct_drop(INCOME), INCOME = fct_recode(INCOME, rich = \u0026quot;[75.000-\u0026quot;,\rless_rich = \u0026quot;[50.000-75.000)\u0026quot;), INCOME = factor(INCOME, ordered = F)) %\u0026gt;% mutate(across(-INCOME, fct_drop))\r# Summary of data\rglimpse(income2)\r## Rows: 600\r## Columns: 14\r## $ INCOME \u0026lt;fct\u0026gt; less_rich, rich, rich, rich, less_rich, rich, rich, les~\r## $ SEX \u0026lt;fct\u0026gt; F, M, F, M, F, F, F, M, F, M, M, M, F, F, F, F, M, M, M~\r## $ MARITAL.STATUS \u0026lt;fct\u0026gt; Married, Married, Married, Single, Single, NA, Married,~\r## $ AGE \u0026lt;ord\u0026gt; 35-44, 25-34, 45-54, 18-24, 18-24, 14-17, 25-34, 25-34,~\r## $ EDUCATION \u0026lt;ord\u0026gt; 1 to 3 years of college, Grad Study, College graduate, ~\r## $ OCCUPATION \u0026lt;fct\u0026gt; \u0026quot;Professional/Managerial\u0026quot;, \u0026quot;Professional/Managerial\u0026quot;, \u0026quot;~\r## $ AREA \u0026lt;ord\u0026gt; 10+ years, 7-10 years, 10+ years, -1 year, 4-6 years, 7~\r## $ DUAL.INCOMES \u0026lt;fct\u0026gt; Yes, Yes, Yes, Not Married, Not Married, Not Married, N~\r## $ HOUSEHOLD.SIZE \u0026lt;ord\u0026gt; Five, Two, Four, Two, Four, Two, Three, Two, Five, One,~\r## $ UNDER18 \u0026lt;ord\u0026gt; Three, None, None, None, None, None, One, None, Three, ~\r## $ HOUSEHOLDER \u0026lt;fct\u0026gt; Own, Own, Own, Rent, Family, Own, Own, Rent, Own, Own, ~\r## $ HOME.TYPE \u0026lt;fct\u0026gt; House, House, House, House, House, Apartment, House, Ho~\r## $ ETHNIC.CLASS \u0026lt;fct\u0026gt; White, White, White, White, White, White, White, White,~\r## $ LANGUAGE \u0026lt;fct\u0026gt; English, English, English, English, English, NA, Englis~\r# Outcome variable\rtable(income2$INCOME)\r## ## less_rich rich ## 362 238\r# Missing data\rDataExplorer::plot_missing(income)\rSplit the data and create a 10-fold cross validation.\nset.seed(2021)\rdat_index \u0026lt;- initial_split(income2, strata = INCOME)\rdat_train \u0026lt;- training(dat_index)\rdat_test \u0026lt;- testing(dat_index)\r## CV\rset.seed(2021)\rdat_cv \u0026lt;- vfold_cv(dat_train, v = 10, repeats = 1, strata = INCOME)\rWe going to impute the NAs with mode value since all the variable are categorical.\n# Recipe\rdat_rec \u0026lt;- recipe(INCOME ~ ., data = dat_train) %\u0026gt;% step_impute_mode(all_predictors()) %\u0026gt;% step_ordinalscore(AGE, EDUCATION, AREA, HOUSEHOLD.SIZE, UNDER18)\r# Model\rrf_mod \u0026lt;- rand_forest(mtry = tune(),\rtrees = tune(),\rmin_n = tune()) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;)\r# Workflow\rrf_wf \u0026lt;- workflow() %\u0026gt;% add_recipe(dat_rec) %\u0026gt;% add_model(rf_mod)\rParameters for grid search\n# Regular grid\rreg_grid \u0026lt;- grid_regular(mtry(c(1, 13)), trees(), min_n(), levels = 3)\r# Random grid\rrand_grid \u0026lt;- grid_random(mtry(c(1, 13)), trees(), min_n(), size = 100)\rTune models using regular grid search. We going to use doParallel library to do parallel processing.\nctrl \u0026lt;- control_grid(save_pred = T,\rextract = extract_model)\rmeasure \u0026lt;- metric_set(roc_auc) # Parallel for regular grid\rlibrary(doParallel)\r# Create a cluster object and then register: cl \u0026lt;- makePSOCKcluster(4)\rregisterDoParallel(cl)\r# Run tune\rset.seed(2021)\rtune_regular \u0026lt;- rf_wf %\u0026gt;% tune_grid(\rresamples = dat_cv, grid = reg_grid, control = ctrl, metrics = measure)\rstopCluster(cl)\rResult for regular grid search:\nautoplot(tune_regular)\rshow_best(tune_regular)\r## # A tibble: 5 x 9\r## mtry trees min_n .metric .estimator mean n std_err .config ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 7 1000 21 roc_auc binary 0.690 10 0.0148 Preprocessor1_Model14\r## 2 7 1000 40 roc_auc binary 0.689 10 0.0179 Preprocessor1_Model23\r## 3 7 2000 40 roc_auc binary 0.689 10 0.0178 Preprocessor1_Model26\r## 4 7 1000 2 roc_auc binary 0.688 10 0.0173 Preprocessor1_Model05\r## 5 7 2000 21 roc_auc binary 0.688 10 0.0159 Preprocessor1_Model17\rTune models using random grid search.\n# Parallel for random grid\r# Create a cluster object and then register: cl \u0026lt;- makePSOCKcluster(4)\rregisterDoParallel(cl)\r# Run tune\rset.seed(2021)\rtune_random \u0026lt;- rf_wf %\u0026gt;% tune_grid(\rresamples = dat_cv, grid = rand_grid, control = ctrl, metrics = measure)\rstopCluster(cl)\rResult for random grid search:\nautoplot(tune_random)\rshow_best(tune_random)\r## # A tibble: 5 x 9\r## mtry trees min_n .metric .estimator mean n std_err .config ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 4 1016 4 roc_auc binary 0.694 10 0.0164 Preprocessor1_Model0~\r## 2 5 1360 3 roc_auc binary 0.693 10 0.0168 Preprocessor1_Model0~\r## 3 6 129 14 roc_auc binary 0.693 10 0.0164 Preprocessor1_Model0~\r## 4 5 1235 3 roc_auc binary 0.692 10 0.0168 Preprocessor1_Model0~\r## 5 6 160 31 roc_auc binary 0.692 10 0.0172 Preprocessor1_Model0~\rRandom grid search has slightly a better result. Let’s use this random search result as a base for iterative search. Firstly, we limit the parameters based on the plot from a random grid search.\nrf_param \u0026lt;- rf_wf %\u0026gt;% parameters() %\u0026gt;% update(mtry = mtry(c(5, 13)), trees = trees(c(1, 500)), min_n = min_n(c(5, 30)))\rNow we do a bayesian optimization.\n# Parallel for bayesian optimization\r# Create a cluster object and then register: cl \u0026lt;- makePSOCKcluster(4)\rregisterDoParallel(cl)\r# Run tune\rset.seed(2021)\rbayes_tune \u0026lt;- rf_wf %\u0026gt;% tune_bayes( resamples = dat_cv,\rparam_info = rf_param,\riter = 60,\rinitial = tune_random, # result from random grid search control = control_bayes(no_improve = 30, verbose = T, save_pred = T), metrics = measure)\rstopCluster(cl)\rResult for bayesian optimization.\nautoplot(bayes_tune, \u0026quot;performance\u0026quot;)\rshow_best(bayes_tune)\r## # A tibble: 5 x 10\r## mtry trees min_n .metric .estimator mean n std_err .config .iter\r## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 4 1016 4 roc_auc binary 0.694 10 0.0164 Preprocessor1_~ 0\r## 2 5 1360 3 roc_auc binary 0.693 10 0.0168 Preprocessor1_~ 0\r## 3 6 129 14 roc_auc binary 0.693 10 0.0164 Preprocessor1_~ 0\r## 4 6 189 15 roc_auc binary 0.693 10 0.0153 Iter1 1\r## 5 5 1235 3 roc_auc binary 0.692 10 0.0168 Preprocessor1_~ 0\rWe get a slightly better result from bayesian optimization. I will not do a simulated annealing approach since I get an error, though I am not sure why.\nLastly, we do a race anova.\n# Parallel for race anova\r# Create a cluster object and then register: cl \u0026lt;- makePSOCKcluster(4)\rregisterDoParallel(cl)\r# Run tune\rset.seed(2021)\rtune_efficient \u0026lt;- rf_wf %\u0026gt;% tune_race_anova(\rresamples = dat_cv, grid = rand_grid, control = control_race(verbose_elim = T, save_pred = T), metrics = measure)\rstopCluster(cl)\rWe get a relatively similar result to random grid search but with faster computation.\nautoplot(tune_efficient)\rshow_best(tune_efficient)\r## # A tibble: 5 x 9\r## mtry trees min_n .metric .estimator mean n std_err .config ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 5 1425 5 roc_auc binary 0.695 10 0.0161 Preprocessor1_Model0~\r## 2 11 406 2 roc_auc binary 0.694 10 0.0183 Preprocessor1_Model0~\r## 3 6 631 3 roc_auc binary 0.692 10 0.0171 Preprocessor1_Model0~\r## 4 7 1264 4 roc_auc binary 0.692 10 0.0159 Preprocessor1_Model0~\r## 5 9 1264 3 roc_auc binary 0.692 10 0.0188 Preprocessor1_Model0~\rWe can also compare ROCs of all approaches. All approaches looks more or less similar.\r\r\rShow code\r\r# regular grid\rrf_reg \u0026lt;- tune_regular %\u0026gt;% select_best(metric = \u0026quot;roc_auc\u0026quot;)\rreg_auc \u0026lt;- tune_regular %\u0026gt;% collect_predictions(parameters = rf_reg) %\u0026gt;% roc_curve(INCOME, .pred_less_rich) %\u0026gt;% mutate(model = \u0026quot;regular_grid\u0026quot;)\r# random grid\rrf_rand \u0026lt;- tune_random %\u0026gt;% select_best(metric = \u0026quot;roc_auc\u0026quot;)\rrand_auc \u0026lt;- tune_random %\u0026gt;% collect_predictions(parameters = rf_rand) %\u0026gt;% roc_curve(INCOME, .pred_less_rich) %\u0026gt;% mutate(model = \u0026quot;random_grid\u0026quot;)\r# bayes\rrf_bayes \u0026lt;- bayes_tune %\u0026gt;% select_best(metric = \u0026quot;roc_auc\u0026quot;)\rbayes_auc \u0026lt;- bayes_tune %\u0026gt;% collect_predictions(parameters = rf_bayes) %\u0026gt;% roc_curve(INCOME, .pred_less_rich) %\u0026gt;% mutate(model = \u0026quot;bayes\u0026quot;)\r# race_anova\rrf_eff \u0026lt;- tune_efficient %\u0026gt;% select_best(metric = \u0026quot;roc_auc\u0026quot;)\reff_auc \u0026lt;- tune_efficient %\u0026gt;% collect_predictions(parameters = rf_eff) %\u0026gt;%\rroc_curve(INCOME, .pred_less_rich) %\u0026gt;% mutate(model = \u0026quot;race_anova\u0026quot;)\r# Compare ROC between all tuning approach\rbind_rows(reg_auc, rand_auc, bayes_auc, eff_auc) %\u0026gt;% ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + geom_path(lwd = 1.5, alpha = 0.8) +\rgeom_abline(lty = 3) + coord_equal() + scale_color_viridis_d(option = \u0026quot;plasma\u0026quot;, end = .6) +\rtheme_bw()\r\rFinally, we fit our best model (bayesian optimization) to the testing data.\n# Finalize workflow\rbest_rf \u0026lt;-\rselect_best(bayes_tune, \u0026quot;roc_auc\u0026quot;)\rfinal_wf \u0026lt;- rf_wf %\u0026gt;% finalize_workflow(best_rf)\rfinal_wf\r## == Workflow ====================================================================\r## Preprocessor: Recipe\r## Model: rand_forest()\r## ## -- Preprocessor ----------------------------------------------------------------\r## 2 Recipe Steps\r## ## * step_impute_mode()\r## * step_ordinalscore()\r## ## -- Model -----------------------------------------------------------------------\r## Random Forest Model Specification (classification)\r## ## Main Arguments:\r## mtry = 4\r## trees = 1016\r## min_n = 4\r## ## Computational engine: ranger\r# Last fit\rtest_fit \u0026lt;- final_wf %\u0026gt;%\rlast_fit(dat_index) # Evaluation metrics test_fit %\u0026gt;%\rcollect_metrics()\r## # A tibble: 2 x 4\r## .metric .estimator .estimate .config ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 accuracy binary 0.583 Preprocessor1_Model1\r## 2 roc_auc binary 0.611 Preprocessor1_Model1\rtest_fit %\u0026gt;%\rcollect_predictions() %\u0026gt;% roc_curve(INCOME, .pred_less_rich) %\u0026gt;% autoplot()\r\rConclusion\rThe result is not that good. Our AUC is quite lower. However, we did use only about 8% from the overall data. Nonetheless, the aim of this post is to cover an overview of hyperparameter tuning in tidymodels.\nAdditionally, there are another two function to construct parameter grids that I did not cover in this post; grid_max_entropy() and grid_latin_hypercube(). Both of these functions do not have much resources explaining them (or at least I did not found it), however, for those interested, a good start will be the tidymodels website.\nReferences:\nhttps://www.tmwr.org/grid-search.html\nhttps://www.tmwr.org/iterative-search.html\nhttps://oliviergimenez.github.io/learning-machine-learning/#\nhttps://towardsdatascience.com/optimization-techniques-simulated-annealing-d6a4785a1de7\n\r","date":1630800000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630824332,"objectID":"8110dc6d9444708148250c7c56beef54","permalink":"https://tengkuhanis.netlify.app/post/hyperparameter-tuning-in-tidymodels/","publishdate":"2021-09-05T00:00:00Z","relpermalink":"/post/hyperparameter-tuning-in-tidymodels/","section":"post","summary":"This post will not go very detail in each of the approach of hyperparameter tuning. This post mainly aims to summarize a few things that I studied for the last couple of days.","tags":["tidymodels"],"title":"Hyperparameter tuning in tidymodels","type":"post"},{"authors":null,"categories":["R"],"content":"\r\rThese are some of the packages that I find useful for data exploration. Basically, this post serves more as my note for future reference. I will list out packages (and some awesome functions from that particular package) rather than specific functions. Further, base R and tidyverse packages will not be included specifically in this list.\nLoad supporting packages\nlibrary(tidyverse)\rThe data we are going to use is from dlookr package:\nglimpse(heartfailure)\r## Rows: 299\r## Columns: 13\r## $ age \u0026lt;int\u0026gt; 75, 55, 65, 50, 65, 90, 75, 60, 65, 80, 75, 62, 45, ~\r## $ anaemia \u0026lt;fct\u0026gt; No, No, No, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, N~\r## $ cpk_enzyme \u0026lt;dbl\u0026gt; 582, 7861, 146, 111, 160, 47, 246, 315, 157, 123, 81~\r## $ diabetes \u0026lt;fct\u0026gt; No, No, No, No, Yes, No, No, Yes, No, No, No, No, No~\r## $ ejection_fraction \u0026lt;dbl\u0026gt; 20, 38, 20, 20, 20, 40, 15, 60, 65, 35, 38, 25, 30, ~\r## $ hblood_pressure \u0026lt;fct\u0026gt; Yes, No, No, No, No, Yes, No, No, No, Yes, Yes, Yes,~\r## $ platelets \u0026lt;dbl\u0026gt; 265000, 263358, 162000, 210000, 327000, 204000, 1270~\r## $ creatinine \u0026lt;dbl\u0026gt; 1.90, 1.10, 1.30, 1.90, 2.70, 2.10, 1.20, 1.10, 1.50~\r## $ sodium \u0026lt;dbl\u0026gt; 130, 136, 129, 137, 116, 132, 137, 131, 138, 133, 13~\r## $ sex \u0026lt;fct\u0026gt; Male, Male, Male, Male, Female, Male, Male, Male, Fe~\r## $ smoking \u0026lt;fct\u0026gt; No, No, Yes, No, No, Yes, No, Yes, No, Yes, Yes, Yes~\r## $ time \u0026lt;int\u0026gt; 4, 6, 7, 7, 8, 8, 10, 10, 10, 10, 10, 10, 11, 11, 12~\r## $ death_event \u0026lt;fct\u0026gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye~\rWe will create a few NAs in our data.\nset.seed(2021)\rheartfailure[sample(seq(nrow(heartfailure)), 20), \u0026quot;age\u0026quot;] \u0026lt;- NA\rheartfailure[sample(seq(nrow(heartfailure)), 10), \u0026quot;sex\u0026quot;] \u0026lt;- NA\r1) dataMaid\nlibrary(dataMaid)\rOne of the very useful function in dataMaid is makeDataReport() which give report on the data. By default it will give a pdf output, but other output options such as word and html are also available.\nmakeDataReport(heartfailure, replace = T)\rThis is the output example in pdf.\n2) DataExplorer\nlibrary(DataExplorer)\rGeneral visualization:\nheartfailure %\u0026gt;% plot_intro()\rSince we have missing data, we can further visualize it:\nheartfailure %\u0026gt;% plot_missing()\rheartfailure %\u0026gt;% profile_missing()\r## feature num_missing pct_missing\r## 1 age 20 0.06688963\r## 2 anaemia 0 0.00000000\r## 3 cpk_enzyme 0 0.00000000\r## 4 diabetes 0 0.00000000\r## 5 ejection_fraction 0 0.00000000\r## 6 hblood_pressure 0 0.00000000\r## 7 platelets 0 0.00000000\r## 8 creatinine 0 0.00000000\r## 9 sodium 0 0.00000000\r## 10 sex 10 0.03344482\r## 11 smoking 0 0.00000000\r## 12 time 0 0.00000000\r## 13 death_event 0 0.00000000\rWe can also do a correlation plot\nheartfailure %\u0026gt;% select_if(is.numeric) %\u0026gt;% drop_na() %\u0026gt;% plot_correlation()\rHowever, I do think correlation plot from corrplot packages gives a better and clean plot. Here is a plot from corrplot.\nlibrary(corrplot)\rheartfailure %\u0026gt;% select_if(is.numeric) %\u0026gt;% drop_na() %\u0026gt;% cor() %\u0026gt;% corrplot(type = \u0026quot;upper\u0026quot;)\rFinally, we can get an overall html report from DataExplorer package using the function create_report().\n3) dlookr\nlibrary(dlookr)\rWe can assess normality of the data using this package. The code below will plot normality for all numeric variable.\nheartfailure %\u0026gt;% plot_normality()\rHowever, for the sake of the simplicity in this post, we will run only for one variable.\nheartfailure %\u0026gt;% plot_normality(age)\rWe can also get a correlation matrix plot from this package, and no need to remove the NAs and filter the numeric variable before running the function.\nheartfailure %\u0026gt;% plot_correlate()\rLastly, from dlookr we can get the overall report of the data exploration in pdf (and other formats as well). This report is quite comprehensive, have a look.\nheartfailure %\u0026gt;% eda_paged_report(target = \u0026quot;death_event\u0026quot;)\r4) skimr\nskimr package, especially skim() function did not display correctly when using the blogdown. Hence, I included the screenshot of the result that we will typically see in the R console.\nlibrary(skimr)\rskim(heartfailure) \rSo, from skimr we can get an overview that includes the histogram for numerical data as well.\n5) outliertree\nThis package identify outlier using a decision tree. I will not go in detail about the approach, but for those who want to read further.\nlibrary(outliertree)\routlier.tree(heartfailure)\r## Reporting top 2 outliers [out of 2 found]\r## ## row [251] - suspicious column: [creatinine] - suspicious value: [0.50]\r## distribution: 96.000% \u0026gt;= 0.70 - [mean: 1.35] - [sd: 1.22] - [norm. obs: 24]\r## given:\r## [cpk_enzyme] \u0026gt; [1610.00] (value: 2522.00)\r## ## ## row [32] - suspicious column: [cpk_enzyme] - suspicious value: [23.00]\r## distribution: 98.958% \u0026gt;= 47.00 - [mean: 677.01] - [sd: 1321.86] - [norm. obs: 95]\r## given:\r## [death_event] = [Yes]\r## Outlier Tree model\r## Numeric variables: 7\r## Categorical variables: 6\r## ## Consists of 369 clusters, spread across 48 tree branches\rWe can further explore the detected outliers using histogram and boxplot. Let’s do for variable creatinine.\n# histogram\rhist(heartfailure$creatinine, breaks = 50, col = \u0026quot;navy\u0026quot;,\rxlab = \u0026quot;Creatinine\u0026quot;, main = \u0026quot;Creatinine level\u0026quot;)\r# boxplot\rboxplot(heartfailure$creatinine)\rProbably in the future I will delve into more detail about outlier detection and any awesome packages in R related to it. If I ever written any post about it, I will link it here.\nConclusion\rThese are some useful package that I find. I may edit this post in the future to add more additional data exploration package. Furthermore, there are shiny apps for data exploration as well, though I think it’s better to sticks with coded approach in data analysis/exploration. Thus, I did not explore those apps in this post. Another thing to remember is to set the variable type accordingly prior to the data exploration.\nHope this is useful!\nReferences:\nhttps://github.com/ekstroem/dataMaid\nhttps://finnstats.com/index.php/2021/05/04/exploratory-data-analysis/\nhttps://cran.r-project.org/web/packages/dlookr/vignettes/EDA.html\nhttps://cran.r-project.org/web/packages/outliertree/vignettes/Introducing_OutlierTree.html\n\r","date":1629590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629580254,"objectID":"70cd61b0f4b27096c2966c6a7320e76a","permalink":"https://tengkuhanis.netlify.app/post/data-exploration-in-r/","publishdate":"2021-08-22T00:00:00Z","relpermalink":"/post/data-exploration-in-r/","section":"post","summary":"These are some of the packages that I find useful for data exploration. Basically, this post serves more as my note for future reference. I will list out packages (and some awesome functions from that particular package) rather than specific functions.","tags":["Data exploration"],"title":"Data exploration in R","type":"post"},{"authors":[],"categories":["R"],"content":"\r\rI just watched a youtube video by Andrew Couch about his commonly used function in readr, stringr, and forcats packages. Although, I have used forcats package before, I realised that I have not fully utilised all of its function.\nSo, in this post, I have summarised main function of forcats that I find useful in my day-to-day R coding. Basically, more like a note to myself.\nMain functions\rWe will use mtcars data to demonstrate each function. forcats is part of tiyverse packages. So, it will load, once we load the tidyverse packages.\nlibrary(tidyverse)\rglimpse(mtcars)\r## Rows: 32\r## Columns: 11\r## $ mpg \u0026lt;dbl\u0026gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,~\r## $ cyl \u0026lt;dbl\u0026gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,~\r## $ disp \u0026lt;dbl\u0026gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16~\r## $ hp \u0026lt;dbl\u0026gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180~\r## $ drat \u0026lt;dbl\u0026gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,~\r## $ wt \u0026lt;dbl\u0026gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.~\r## $ qsec \u0026lt;dbl\u0026gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18~\r## $ vs \u0026lt;dbl\u0026gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,~\r## $ am \u0026lt;dbl\u0026gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,~\r## $ gear \u0026lt;dbl\u0026gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,~\r## $ carb \u0026lt;dbl\u0026gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,~\rThere are 9 forcats functions that I think very useful.\nfactor()\r\rfactor() changes variable type into a factor or categorical type\nmtcars$carb \u0026lt;- factor(mtcars$carb)\rglimpse(mtcars)\r## Rows: 32\r## Columns: 11\r## $ mpg \u0026lt;dbl\u0026gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,~\r## $ cyl \u0026lt;dbl\u0026gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,~\r## $ disp \u0026lt;dbl\u0026gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16~\r## $ hp \u0026lt;dbl\u0026gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180~\r## $ drat \u0026lt;dbl\u0026gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,~\r## $ wt \u0026lt;dbl\u0026gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.~\r## $ qsec \u0026lt;dbl\u0026gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18~\r## $ vs \u0026lt;dbl\u0026gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,~\r## $ am \u0026lt;dbl\u0026gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,~\r## $ gear \u0026lt;dbl\u0026gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,~\r## $ carb \u0026lt;fct\u0026gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,~\rfct_inorder()\r\rThis function sorts factor levels based on the order of appearance in the dataset.\nlevels(mtcars$carb) # original levels\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot;\rfct_inorder(mtcars$carb) # levels based on the order of appearance\r## [1] 4 4 1 1 2 1 4 2 2 4 4 3 3 3 4 4 4 1 2 1 1 2 2 4 2 1 2 2 4 6 8 2\r## Levels: 4 1 2 3 6 8\rfct_infreq()\r\rThis function sorts factor levels based on the frequency of values.\nfct_count(mtcars$carb) # this is forcats function as well, count factor level\r## # A tibble: 6 x 2\r## f n\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt;\r## 1 1 7\r## 2 2 10\r## 3 3 3\r## 4 4 10\r## 5 6 1\r## 6 8 1\rlevels(mtcars$carb) # original levels\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot;\rfct_infreq(mtcars$carb) # levels based on the frequency values\r## [1] 4 4 1 1 2 1 4 2 2 4 4 3 3 3 4 4 4 1 2 1 1 2 2 4 2 1 2 2 4 6 8 2\r## Levels: 2 4 1 3 6 8\rfct_relevel()\r\rThis function can be used to change the order manually.\nlevels(mtcars$carb) # original levels\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot;\rfct_relevel(mtcars$carb, c(\u0026quot;8\u0026quot;, \u0026quot;6\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;3\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;1\u0026quot;)) # manually changed new levels\r## [1] 4 4 1 1 2 1 4 2 2 4 4 3 3 3 4 4 4 1 2 1 1 2 2 4 2 1 2 2 4 6 8 2\r## Levels: 8 6 4 3 2 1\rfct_relevel() can also be used to change one factor level only.\nlevels(mtcars$carb) # original levels\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot;\rfct_relevel(mtcars$carb, \u0026quot;8\u0026quot;, after = 2) # change level 8 to the third place\r## [1] 4 4 1 1 2 1 4 2 2 4 4 3 3 3 4 4 4 1 2 1 1 2 2 4 2 1 2 2 4 6 8 2\r## Levels: 1 2 8 3 4 6\rfct_reorder()\r\rThis function changes the order based on another variable. Let’s change variable carb’s levels based on value of variable disp.\nlevels(mtcars$carb) # original levels\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot;\rfct_reorder(mtcars$carb, mtcars$disp, .fun = sum, .desc = TRUE) # new level based on disp value\r## [1] 4 4 1 1 2 1 4 2 2 4 4 3 3 3 4 4 4 1 2 1 1 2 2 4 2 1 2 2 4 6 8 2\r## Levels: 4 2 1 3 8 6\rmtcars %\u0026gt;% group_by(carb) %\u0026gt;% summarise(sum_disp = sum(disp)) %\u0026gt;% arrange(desc(sum_disp)) # this is basically what we do with fct_reorder() above\r## # A tibble: 6 x 2\r## carb sum_disp\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 4 3088.\r## 2 2 2082.\r## 3 1 940.\r## 4 3 827.\r## 5 8 301 ## 6 6 145\rAdditionally, fct_reorder() can be used with plotting as well.\n# Original plot\rggplot(mtcars, aes(x = carb, y = disp)) +\rgeom_col()\r# Plot with changed levels\rmtcars %\u0026gt;% mutate(carb = fct_reorder(carb, disp, .fun = sum, .desc = TRUE)) %\u0026gt;% ggplot(aes(x = carb, y = disp)) +\rgeom_col()\rfct_lump()\r\rThis function lumps factor levels into other factors. There are 5 variants of this function:\n\rfct_lump()\rfct_lump_min()\rfct_lump_n()\rfct_lump_lowfreq()\r\rThe remaining one variant is fct_lump_prop(). It is not in the example below as I do not find it useful at least for my current R coding routine.\nfct_lump() automatically lump small frequency factor group into one group.\nfct_count(mtcars$carb) # this is forcats function as well, count factor level\r## # A tibble: 6 x 2\r## f n\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt;\r## 1 1 7\r## 2 2 10\r## 3 3 3\r## 4 4 10\r## 5 6 1\r## 6 8 1\rfct_lump(mtcars$carb) %\u0026gt;% fct_count() \r## # A tibble: 4 x 2\r## f n\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt;\r## 1 1 7\r## 2 2 10\r## 3 4 10\r## 4 Other 5\rfct_lump_min() lump factor group into one group based on the given value.\ntable(fct_lump_min(mtcars$carb, min = 2)) # group 6 and 8 lump into one group\r## ## 1 2 3 4 Other ## 7 10 3 10 2\rfct_lump_n() lump all level except for the n most frequent factor groups.\ntable(fct_lump_n(mtcars$carb, n = 2)) # 2 frequent group only, others in one group\r## ## 2 4 Other ## 10 10 12\rfct_lump_lowfreq() lump small frequent groups into one group, while making sure that particular one group is still the smallest.\ntable(fct_lump_lowfreq(mtcars$carb, other_level = \u0026quot;low\u0026quot;)) # group low is still the smallest\r## ## 1 2 4 low ## 7 10 10 5\rfct_other()\r\rfct_other() is much like fct_lump(), except we manually choose which factor groups to be combined.\ntable(fct_other(mtcars$carb, keep = c(\u0026quot;8\u0026quot;, \u0026quot;6\u0026quot;))) \r## ## 6 8 Other ## 1 1 30\rfct_recode()\r\rThis function is used to rename or relabel the factor group.\ntable(fct_recode(mtcars$carb, hanis = \u0026quot;8\u0026quot;)) \r## ## 1 2 3 4 6 hanis ## 7 10 3 10 1 1\rfct_relabel()\r\rfct_relabel() is extremely useful if we want to rename quite a number of factor groups.\ntable(mtcars$carb) # original groups\r## ## 1 2 3 4 6 8 ## 7 10 3 10 1 1\rtable(fct_relabel(mtcars$carb, ~ c(\u0026quot;abu\u0026quot;, \u0026quot;ali\u0026quot;, \u0026quot;chong\u0026quot;, \u0026quot;siti\u0026quot;, \u0026quot;krish\u0026quot;, \u0026quot;lee\u0026quot;))) # new named groups\r## ## abu ali chong siti krish lee ## 7 10 3 10 1 1\rReference:\nhttps://forcats.tidyverse.org/index.html\n\r","date":1621296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621340126,"objectID":"0e7aac2e7cf98aeec9e4ada6fd43b9fb","permalink":"https://tengkuhanis.netlify.app/post/a-summary-of-forcats-package/","publishdate":"2021-05-18T00:00:00Z","relpermalink":"/post/a-summary-of-forcats-package/","section":"post","summary":"I just watched a youtube video by Andrew Couch about his commonly used function in readr, stringr, and forcats packages. Although, I have used forcats package before, I realised that I have not fully utilised all of its function.","tags":["tidyverse"],"title":"A summary of forcats package","type":"post"},{"authors":[],"categories":["R","Machine Learning"],"content":"\r\rOverview\rImbalance data happens when there is unequal distribution of data within a categorical outcome variable. Imbalance data occurs due to several reasons such as biased sampling method and measurement errors. However, the imbalance may also be the inherent characteristic of the data. For example, a rare disease predictive model, in this case, the imbalance is expected.\nGenerally, there are two types of imbalanced problem:\n\rSlight imbalance: the imbalance is small, like 4:6\rSevere imbalance: the imbalance is large, like 1:100 or more\r\rIn slight imbalanced cases, usually it is not a concern, while severe imbalanced cases require a more specialised method to to build a predictive model.\n\rThe problem\rWhat’s the problem with the imbalanced data?\nFirstly, a predictive model of an imbalanced data is bias towards the majority class. The minority class becomes harder to predict as there are few data from this class. So, the detection rate for a minority class will be very low.\rSecondly, accuracy is not a good measure in this case. We may get a good accuracy,but in reality the accuracy does not reflect the unequal distribution of the data. This is known as an accuracy paradox. Imagine we have 90% of data belong to the majority class, while the remaining 10% belong to the minority class. So, just by predicting all data as a majority class, the model can easily get 90% accuracy.\n\rHandling approach\rThe easiest approach is to collect more data, though this may not be practical in all situation. Fortunately, there are a few machine learning techniques available to tackle this problem.\nHere is a summary of resampling techniques available in themis package.\nOver-sampling approach is preferred when the dataset is small. The under-sampling approach can be used when the dataset is large, though this approach may lead to loss of information. Additionally, ensemble technique such as random forest is said to be able to model the imbalanced data, though some references/blogs say otherwise.\nSo, we are going to compare four of over-sampling techniques (upsample, SMOTE, ADASYN, and ROSE), and three of under-sampling techniques (downsample, nearmiss and tomek). The base model is a decision tree, which will be used for all the techniques. The decision trees are not going to be extensively hyperparameter tuned, for the sake of simplicity. Additionally, random forest is also going to be included in the comparison.\nThe dataset is from here. This is a summary of the dataset.\nsummary(df)\r## admit gre gpa rank ## 0:273 Min. :220.0 Min. :2.260 1: 61 ## 1:127 1st Qu.:520.0 1st Qu.:3.130 2:151 ## Median :580.0 Median :3.395 3:121 ## Mean :587.7 Mean :3.390 4: 67 ## 3rd Qu.:660.0 3rd Qu.:3.670 ## Max. :800.0 Max. :4.000\rAs we can see from the summary, variable admit has a moderate imbalanced data about 1:3 ratio.\nggplot(df, aes(admit)) + geom_bar() +\rtheme_bw()\rBelow is the code for each model.\n\r\rShow code\r\r# Packages\rlibrary(tidyverse)\rlibrary(magrittr)\rlibrary(tidymodels)\rlibrary(themis)\r# Data\rdf \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/finnstats/finnstats/main/binary.csv\u0026quot;)\r# Split data\rset.seed(1234)\rdf_split \u0026lt;- initial_split(df)\rdf_train \u0026lt;- training(df_split)\rdf_test \u0026lt;- testing(df_split)\r# 1) Decision tree ----\r# Recipe\rdt_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank)\rdf_train_rec \u0026lt;- dt_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec \u0026lt;- dt_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv \u0026lt;- vfold_cv(df_train_rec)\r# Tune and finalize workflow\r## Specify model\rdt_mod \u0026lt;- decision_tree(\rcost_complexity = tune(),\rtree_depth = tune(),\rmin_n = tune()\r) %\u0026gt;% set_engine(\u0026quot;rpart\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;)\r## Specify workflow\rdt_wf \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune \u0026lt;- dt_wf %\u0026gt;% tune_grid(resamples = df_cv,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune \u0026lt;- dt_tune %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final \u0026lt;- dt_wf %\u0026gt;% finalize_workflow(best_tune)\r# Fit on train data\rdt_train \u0026lt;- dt_wf_final %\u0026gt;% fit(data = df_train_rec)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train, new_data = df_test_rec)) %\u0026gt;% rename(pred = .pred_class)\r# 2) Oversampling ----\r## step_upsample() ----\r# Recipe\rup_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_upsample(admit,\rseed = 1234)\rdf_train_up \u0026lt;- up_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_up \u0026lt;- up_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_up \u0026lt;- vfold_cv(df_train_up)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_up \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_up \u0026lt;- dt_wf_up %\u0026gt;% tune_grid(resamples = df_cv_up,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_up \u0026lt;- dt_tune_up %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_up \u0026lt;- dt_wf_up %\u0026gt;% finalize_workflow(best_tune_up)\r# Fit on train data\rdt_train_up \u0026lt;- dt_wf_final_up %\u0026gt;% fit(data = df_train_up)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_up, new_data = df_test_rec_up)) %\u0026gt;% rename(pred_up = .pred_class)\r## step_smote() ----\r# Recipe\rsmote_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_smote(admit, seed = 1234)\rdf_train_smote \u0026lt;- smote_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_smote \u0026lt;- smote_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_smote \u0026lt;- vfold_cv(df_train_smote)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_smote \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_smote \u0026lt;- dt_wf_smote %\u0026gt;% tune_grid(resamples = df_cv_smote,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_smote \u0026lt;- dt_tune_smote %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_smote \u0026lt;- dt_wf_smote %\u0026gt;% finalize_workflow(best_tune_smote)\r# Fit on train data\rdt_train_smote \u0026lt;- dt_wf_final_smote %\u0026gt;% fit(data = df_train_smote)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_smote, new_data = df_test_rec_smote)) %\u0026gt;% rename(pred_smote = .pred_class)\r## step_rose() ----\r# Recipe\rrose_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_rose(admit, seed = 1234)\rdf_train_rose \u0026lt;- rose_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_rose \u0026lt;- rose_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_rose \u0026lt;- vfold_cv(df_train_rose)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_rose \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_rose \u0026lt;- dt_wf_rose %\u0026gt;% tune_grid(resamples = df_cv_rose,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_rose \u0026lt;- dt_tune_rose %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_rose \u0026lt;- dt_wf_rose %\u0026gt;% finalize_workflow(best_tune_rose)\r# Fit on train data\rdt_train_rose \u0026lt;- dt_wf_final_rose %\u0026gt;% fit(data = df_train_rose)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_rose, new_data = df_test_rec_rose)) %\u0026gt;% rename(pred_rose = .pred_class)\r## step_adasyn() ----\r# Recipe\radasyn_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_adasyn(admit, seed = 1234)\rdf_train_adasyn \u0026lt;- adasyn_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_adasyn \u0026lt;- adasyn_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_adasyn \u0026lt;- vfold_cv(df_train_adasyn)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_adasyn \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_adasyn \u0026lt;- dt_wf_adasyn %\u0026gt;% tune_grid(resamples = df_cv_adasyn,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_adasyn \u0026lt;- dt_tune_adasyn %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_adasyn \u0026lt;- dt_wf_adasyn %\u0026gt;% finalize_workflow(best_tune_adasyn)\r# Fit on train data\rdt_train_adasyn \u0026lt;- dt_wf_final_adasyn %\u0026gt;% fit(data = df_train_adasyn)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_adasyn, new_data = df_test_rec_adasyn)) %\u0026gt;% rename(pred_adasyn = .pred_class)\r# 3) Undersampling ----\r## step_downsample() ----\r# Recipe\rdown_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_downsample(admit,\rseed = 1234)\rdf_train_down \u0026lt;- down_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_down \u0026lt;- down_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_down \u0026lt;- vfold_cv(df_train_down)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_down \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_down \u0026lt;- dt_wf_down %\u0026gt;% tune_grid(resamples = df_cv_down,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_down \u0026lt;- dt_tune_down %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_down \u0026lt;- dt_wf_down %\u0026gt;% finalize_workflow(best_tune_down)\r# Fit on train data\rdt_train_down \u0026lt;- dt_wf_final_down %\u0026gt;% fit(data = df_train_down)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_down, new_data = df_test_rec_down)) %\u0026gt;% rename(pred_down = .pred_class)\r## step_nearmiss() ----\r# Recipe\rnearmiss_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_nearmiss(admit,\rseed = 1234)\rdf_train_nearmiss \u0026lt;- nearmiss_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_nearmiss \u0026lt;- nearmiss_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_nearmiss \u0026lt;- vfold_cv(df_train_nearmiss)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_nearmiss \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_nearmiss \u0026lt;- dt_wf_nearmiss %\u0026gt;% tune_grid(resamples = df_cv_nearmiss,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_nearmiss \u0026lt;- dt_tune_nearmiss %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_nearmiss \u0026lt;- dt_wf_nearmiss %\u0026gt;% finalize_workflow(best_tune_nearmiss)\r# Fit on train data\rdt_train_nearmiss \u0026lt;- dt_wf_final_nearmiss %\u0026gt;% fit(data = df_train_nearmiss)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_nearmiss, new_data = df_test_rec_nearmiss)) %\u0026gt;% rename(pred_nearmiss = .pred_class)\r## step_tomek() ----\r# Recipe\rtomek_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_tomek(admit,\rseed = 1234)\rdf_train_tomek \u0026lt;- tomek_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_tomek \u0026lt;- tomek_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_tomek \u0026lt;- vfold_cv(df_train_tomek)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_tomek \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_tomek \u0026lt;- dt_wf_tomek %\u0026gt;% tune_grid(resamples = df_cv_tomek,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_tomek \u0026lt;- dt_tune_tomek %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_tomek \u0026lt;- dt_wf_tomek %\u0026gt;% finalize_workflow(best_tune_tomek)\r# Fit on train data\rdt_train_tomek \u0026lt;- dt_wf_final_tomek %\u0026gt;% fit(data = df_train_tomek)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_tomek, new_data = df_test_rec_tomek)) %\u0026gt;% rename(pred_tomek = .pred_class)\r# 4) Ensemble approach: random forest ----\r## 10-folds CV\rset.seed(1234)\rdf_cv \u0026lt;- vfold_cv(df_train_rec)\r# Tune and finalize workflow\r## Specify model\rrf_mod \u0026lt;- rand_forest(\rmtry = tune(),\rtrees = tune(),\rmin_n = tune()\r) %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;)\r## Specify workflow\rrf_wf \u0026lt;- workflow() %\u0026gt;% add_model(rf_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rrf_tune \u0026lt;- rf_wf %\u0026gt;% tune_grid(resamples = df_cv,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune \u0026lt;- rf_tune %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rrf_wf_final \u0026lt;- rf_wf %\u0026gt;% finalize_workflow(best_tune)\r# Fit on train data\rrf_train \u0026lt;- rf_wf_final %\u0026gt;% fit(data = df_train_rec)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(rf_train, new_data = df_test_rec)) %\u0026gt;% rename(pred_rf = .pred_class)\r\rNow, let’s get the accuracy, sensitivity, specificity, and Mathews Correlation Coefficient (MCC) for each model.\n\r\rShow code\r\r# Get all measurements\rdf_test$admit %\u0026lt;\u0026gt;% as_factor()\rpred_col \u0026lt;- colnames(df_test)[5:13]\rresult \u0026lt;- vector(\u0026quot;list\u0026quot;, 0)\rsensi \u0026lt;- vector(\u0026quot;list\u0026quot;, 0)\rspecif \u0026lt;- vector(\u0026quot;list\u0026quot;, 0)\rmathew \u0026lt;- vector(\u0026quot;list\u0026quot;, 0)\rfor (i in seq_along(pred_col)) {\r# accuracy\rresult[[i]] \u0026lt;-\rdf_test %\u0026gt;% accuracy(admit, df_test[,pred_col[i]])\r# sensitivity\rsensi[[i]] \u0026lt;-\rdf_test %\u0026gt;% sensitivity(admit, df_test[,pred_col[i]])\r# specificity\rspecif[[i]] \u0026lt;-\rdf_test %\u0026gt;% specificity(admit, df_test[,pred_col[i]])\r# MCC\rmathew[[i]] \u0026lt;-\rdf_test %\u0026gt;% mcc(admit, df_test[,pred_col[i]])\r}\r## Turn into dataframe\rresult %\u0026lt;\u0026gt;% enframe() %\u0026gt;% unnest(cols = c(\u0026quot;value\u0026quot;)) %\u0026gt;% rename(model = name, accuracy = .estimate) %\u0026gt;% select(model, accuracy) %\u0026gt;% mutate(model = factor(model,labels = c(\r\u0026quot;1\u0026quot; = \u0026quot;base\u0026quot;,\r\u0026quot;2\u0026quot; = \u0026quot;upsample\u0026quot;,\r\u0026quot;3\u0026quot; = \u0026quot;smote\u0026quot;,\r\u0026quot;4\u0026quot; = \u0026quot;rose\u0026quot;,\r\u0026quot;5\u0026quot; = \u0026quot;adasyn\u0026quot;,\r\u0026quot;6\u0026quot; = \u0026quot;downsample\u0026quot;,\r\u0026quot;7\u0026quot; = \u0026quot;nearmiss\u0026quot;,\r\u0026quot;8\u0026quot; = \u0026quot;tomek\u0026quot;,\r\u0026quot;9\u0026quot; = \u0026quot;random_forest\u0026quot;\r)\r))\rsensi %\u0026lt;\u0026gt;% enframe() %\u0026gt;% unnest(cols = c(\u0026quot;value\u0026quot;))\rspecif %\u0026lt;\u0026gt;% enframe() %\u0026gt;% unnest(cols = c(\u0026quot;value\u0026quot;))\rmathew %\u0026lt;\u0026gt;% enframe() %\u0026gt;% unnest(cols = c(\u0026quot;value\u0026quot;))\rresult %\u0026lt;\u0026gt;% bind_cols(sensitive = sensi$.estimate, specific = specif$.estimate, mathew = mathew$.estimate)\r# Plot the result\rresult %\u0026gt;% pivot_longer(cols = 2:5, names_to = \u0026quot;measure\u0026quot;) %\u0026gt;% ggplot(aes(x = model, y = value, fill = measure)) +\rgeom_bar(position = \u0026quot;dodge\u0026quot;, stat = \u0026quot;identity\u0026quot;) +\rtheme_bw() +\rcoord_flip() +\rgeom_text(aes(label = paste0(round(value*100, digits = 1), \u0026quot;%\u0026quot;)), position = position_dodge(0.9), vjust = 0.3, size = 2.7, hjust = -0.1) +\rlabs(title = \u0026quot;Comparison of unbalanced data techniques\u0026quot;, x = \u0026quot;Techniques\u0026quot;, y = \u0026quot;Performance\u0026quot;) +\rscale_fill_discrete(name = \u0026quot;Metrics:\u0026quot;,\rlabels = c(\u0026quot;Accuracy\u0026quot;, \u0026quot;MCC\u0026quot;, \u0026quot;Sensitivity\u0026quot;, \u0026quot;Specificity\u0026quot;)) +\rtheme(legend.position = \u0026quot;bottom\u0026quot;)\r\rWe can see from the above plot, the base model (decision tree) clearly has a low detection rate for a minority class (specificity). All methods able to increase the specificity, while sacrificing the accuracy and sensitivity. As mentioned earlier, accuracy is not a good metrics for this kind of model (ie; accuracy paradox). MCC on the other hand, takes into account all values of confusion matrix; true positive, false positive, true negative, and false negative. Hence, MCC is more informative compared to accuracy (and F score, which has not been included in the plot, for the sake of simplicity).\nA more balanced model probably downsample approach based on MCC, specificity, and sensitivity. However, this does not mean that downsample technique is the best as I believes each technique behaves differently from one data to another.\nReferences:\nhttps://themis.tidymodels.org/reference/index.html\n\rhttps://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n\rhttps://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-6413-7\r\r\r","date":1620950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620955774,"objectID":"0bee3c59d405022795d0c23d2d3cc134","permalink":"https://tengkuhanis.netlify.app/post/handling-imbalanced-data/","publishdate":"2021-05-14T00:00:00Z","relpermalink":"/post/handling-imbalanced-data/","section":"post","summary":"Overview\rImbalance data happens when there is unequal distribution of data within a categorical outcome variable. Imbalance data occurs due to several reasons such as biased sampling method and measurement errors.","tags":["Machine Learning"],"title":"Handling imbalanced data","type":"post"},{"authors":[],"categories":["R","Deep Learning"],"content":"\r\rI have been reading about lost functions and optimisers in deep learning for the last couple of days when I stumble upon the term Exponentially Weighted Average (EWA). So, in this post I aims to explain my understanding of EWA.\nOverview of EWA\rEWA basically is an important concept in deep learning and have been used in several optimisers to smoothen the noise of the data.\nLet’s see the formula for EWA:\nVt is some smoothen value at point t, while St is a data point at point t. B here is a hyperparameter that we need to tune in our network. So, the choice of B will determine how many data points that we average the value of Vt as shown below:\n\rEWA in deep learnings’ optimiser\rSo, some of the optimisers that adopt the approach of EWA are (red box indicates the EWA part in each formula):\nStochastic gradient descent (SGD) with momentum\r\rThe issue with SGD is the present of noise while searching for global minima. So, SGD with momentum integrated the EWA, which reduces these noises and helps the network converges faster.\nAdaptive delta (Adadelta) and Root Mean Square Propagation (RMSprop)\r\rAdadelta and RMSprop are proposed in attempt to solve the issue of diminishing learning rate of adaptive gradient (Adagrad) optimiser. The use of EWA in both optimisers actually helps to achieve this. Both optimisers have quite a similar formula, but attached below is the formula for Adadelta.\nAdaptive moment estimation (ADAM)\r\rADAM basically combined the SGD with momentum with Adadelta. As shown earlier, both optimisers use EWA.\n\rMore details on EWA\rNow, let’s go back to EWA. Here is the example of calculation of EWA:\nKeep in mind that t3 is the latest time point, followed by t2 and t1, respectively. So, if we want to calculate V3:\nSo, if we were to varies the value of B across the equation (while the values of a1…an remain constant), we can do so in R.\nlibrary(tidyverse) func \u0026lt;- function(b) (1 - b) * b^((20:1) - 1)\rbeta \u0026lt;- seq(0.1, 0.9, by=0.2)\rdat \u0026lt;- t(sapply(beta, func)) %\u0026gt;% as.data.frame()\rcolnames(dat)[1:20] \u0026lt;- 1:20\rdat %\u0026gt;% mutate(beta = as_factor(beta)) %\u0026gt;%\rpivot_longer(cols = 1:20, names_to = \u0026quot;data_point\u0026quot;, values_to = \u0026quot;weight\u0026quot;) %\u0026gt;% ggplot(aes(x=as.numeric(data_point), y=weight, color=beta)) +\rgeom_line() +\rgeom_point() +\rscale_x_continuous(breaks = 1:20) +\rlabs(title = \u0026quot;Change of Exponentially Weighted Average function\u0026quot;, subtitle = \u0026quot;Time at t20 is the recent time, and t1 is the initial time\u0026quot;) +\rscale_colour_discrete(\u0026quot;Beta:\u0026quot;) +\rxlab(\u0026quot;Time(t)\u0026quot;) +\rylab(\u0026quot;Weights/Coefficients\u0026quot;) +\rtheme_bw()\rNote that time at t20 is the recent time, and t1 is the initial time. Thus, two main points from the above plot are:\nThe EWA function acts in a decaying manner.\n\rAs beta, B increases we actually put more emphasize on the recent data point.\r\rSide note: I have tried to do the plot in plotly, not sure why it did not work 😕\nReferences:\n1) https://towardsdatascience.com/deep-learning-optimizers-436171c9e23f (all the equations are from this reference)\n2) https://youtu.be/NxTFlzBjS-4\n3) https://medium.com/@dhartidhami/exponentially-weighted-averages-5de212b5be46\n\r","date":1620518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620542031,"objectID":"877769ed65026a159fb60d317a14861d","permalink":"https://tengkuhanis.netlify.app/post/exponentially-weighted-average-in-deep-learning/","publishdate":"2021-05-09T00:00:00Z","relpermalink":"/post/exponentially-weighted-average-in-deep-learning/","section":"post","summary":"I have been reading about lost functions and optimisers in deep learning for the last couple of days when I stumble upon the term Exponentially Weighted Average (EWA). So, in this post I aims to explain my understanding of EWA.","tags":["Deep Learning"],"title":"Exponentially Weighted Average in Deep Learning","type":"post"},{"authors":null,"categories":["R"],"content":"\r\rFirst of all, this write up is mean for a beginner in R.\nThings can be done in many ways in R. In facts, R has been very flexible in this regard compared to other statistical softwares. Basic things such as selecting a column, slicing a row, filtering a data based on certain condition can be done using a base R function. However, all these things can also be done using a tidyverse approach.\nTidyverse basically, a collection of packages that can be loaded in a line of function.\nlibrary(tidyverse)\rTidyverse is developed by “RStudio people” pioneered by Hadley Wickham, which means that these packages will be continuously maintained and updated.\nSo, without further ado, these are the comparisons between these two approaches for some very basic thingy:\nSelect or deselect a column and a row\r\r# Base R\riris[1:5, c(\u0026quot;Sepal.Length\u0026quot;, \u0026quot;Sepal.Width\u0026quot;)]\riris[1:5,c(1,2)] # similar to above\riris[1:5, -1]\r# Tidyverse\riris %\u0026gt;% select(Sepal.Length, Sepal.Width) %\u0026gt;% slice(1:5)\riris %\u0026gt;% select(-Sepal.Length) %\u0026gt;% slice(1:5)\rFilter based on condition\r\r# Base R\riris[iris$Species == \u0026quot;setosa\u0026quot;, ]\r# Tidyverse\riris %\u0026gt;% filter(Species == \u0026quot;setosa\u0026quot;)\rMutate (transmute replace the variable)\r\r# Base R\riris$SL_minus10 \u0026lt;- iris$Sepal.Length - 10\r# Tidyverse\riris %\u0026gt;% mutate(SL_minus10 = Sepal.Length - 10)\rSort variable\r\r# Base R\riris[order(-iris$Sepal.Width),]\r# Tidyverse\riris %\u0026gt;% arrange(desc(Sepal.Length))\rGroup by (and get mean for variable Sepal.Width)\r\r# Not really base R\rdoBy::summaryBy(Sepal.Width~Species, iris, FUN = mean) # Tidyverse\riris %\u0026gt;% group_by(Species) %\u0026gt;% summarise(mean_SW = mean(Sepal.Width))\rRename variable\r\r# Base R\rcolnames(iris)[6] \u0026lt;- \u0026quot;hanis\u0026quot;\r# Tidyverse\riris %\u0026gt;% rename(Species = hanis)\rSo, that’s it. Overall, tidyverse give a clarity in understanding the code as it reads from left to right. On the contrary, the base R approach reads from inside to outside, especially for a more complicated code.\n","date":1620086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620118129,"objectID":"df69df02c5380516ce81421c1f0ba162","permalink":"https://tengkuhanis.netlify.app/post/2021-05-04-base-r-vs-tidyverse/","publishdate":"2021-05-04T00:00:00Z","relpermalink":"/post/2021-05-04-base-r-vs-tidyverse/","section":"post","summary":"First of all, this write up is mean for a beginner in R.\nThings can be done in many ways in R. In facts, R has been very flexible in this regard compared to other statistical softwares.","tags":["base R","comparison","tidyverse"],"title":"Base R vs tidyverse","type":"post"},{"authors":[],"categories":["R"],"content":"\r\rI have heard quite a several times that apply function is faster than loop function in R. Loop function is said to be inefficient, though in certain situation loop is the only way.\nLet’s compare between loop function and apply function in R.\nFirst, make a very big fake data contain a list of vector.\nset.seed(2021)\rxlist \u0026lt;- list(col1 = rnorm(10000000), col2 = rnorm(10000000),\rcol3 = rnorm(100000000),\rcol4 = rnorm(1000000)) # this will take a few seconds\rThen, calculate the mean of each vector using for loop().\nptm \u0026lt;- proc.time() #-- start the clock\rmean_loop \u0026lt;- vector(\u0026quot;list\u0026quot;, 0) # place holder for a value\rfor (i in seq_along(xlist)) {\rmean_loop[[i]] \u0026lt;- mean(xlist[[i]])\r}\rproc.time() - ptm #-- stop the clock (time in seconds)\r## user system elapsed ## 0.38 0.00 0.37\rNow, using lapply() function.\nptm \u0026lt;- proc.time() #-- start the clock\rmean_apply \u0026lt;- lapply(xlist, mean)\rproc.time() - ptm #-- stop the clock\r## user system elapsed ## 0.34 0.00 0.35\rSo, lapply() is a little bit faster. Obviously, with a very big dataset and a more complicated objective, lapply() is the right choice, but for a “normal” size dataset, the use of any of the two functions probably do not make much different.\n","date":1620086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620121215,"objectID":"c64e5453754ef81813c7c52a0bf44ddb","permalink":"https://tengkuhanis.netlify.app/post/loop-vs-apply-in-r/","publishdate":"2021-05-04T00:00:00Z","relpermalink":"/post/loop-vs-apply-in-r/","section":"post","summary":"I have heard quite a several times that apply function is faster than loop function in R. Loop function is said to be inefficient, though in certain situation loop is the only way.","tags":["base R","comparison"],"title":"Loop vs apply in R","type":"post"},{"authors":null,"categories":["Epidemiology"],"content":"\r\rRecently I have read an article that the Malaysian government have made a deal with Pfizer for 6.4 million Malaysian to be vaccinated. So, I am wondering what is the minimal number of people should be vaccinated.\nI have also come across this interesting article, which explains how we can calculate a minimal number of people to be vaccinated to achieves herd immunity based on R naught (R0).\nR naught (R0)\nThe basic idea of R0 or basic reproduction number is quite simple. It describes how many secondary infections will derive from the first case. I think Figure 1 below describes this idea very well.\n\rFigure 1: Basic idea of R0(image from https://www.atrainceu.com/content/3-basic-reproduction-number-r-naught)\r\rSo, R0 can be affected by a few factors, such as:\n\rproportion of susceptible people at the initial outbreak\rinfectiousness of the virus or the disease\rrate of recovery or death\rand a few other factors\r\rAs R0 increases more than 1, the spread of the disease will increases, while R0 below 1 indicates the spread of the disease will decrease and eventually dies out.\nHowever, I noticed that quite a few including KKM (Ministry of Health, Malaysia) have used the term R0 in their reports instead of Re or Rt which is the effective reproduction number or time-varying reproduction number. R0 refers to the initial reproduction number at the beginning of the outbreak. The “naught” or “zero” in R naught (R0) is referring to population condition that has zero immunity to the disease.\nHerd immunity\nHerd immunity is said to occur when a significant proportion of the population is immunized. Subsequently, those whose susceptible (not immunized) will be protected.\nHow many should be vaccinated\nSo, back to the initial topic. We can use the formula below to answer this question.\n\\[P_i \u0026gt; 1 - \\frac{1}{R_0}\\]\nPi refers to the number of proportion that should be immunized or in this case, vaccinated.\nSo, after googling, I found one calculation by my lecturer in Biostat Unit, USM, Dr Wan Arifin and his colleague. The R0 based on his calculation is 2.673. Also, I found another article reported that the R0 is 3.55 in March, according to KKM.\nMalaysian’s population is estimated at 32.7 million by the Department of Statistics, Malaysia (DOSM). So, using the formula above, about 63% to 72% of Malaysian population should vaccinated, and this translates to about 20.6 to 23.5 million people.\nThe deal that the Malaysian government made with Pfizer is far from enough, but of course, this is a very good and quick decision. We also have other vaccines like Moderna’s vaccine coming up.\nDisclaimer: This is just my opinion. Please take it with a massive grain of salt.\n","date":1607299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607310799,"objectID":"071dd7efb1b705ccf90c1ab3a155e21a","permalink":"https://tengkuhanis.netlify.app/post/how-many-malaysian-should-be-vaccinated-to-get-herd-immunity-from-covid-19/","publishdate":"2020-12-07T00:00:00Z","relpermalink":"/post/how-many-malaysian-should-be-vaccinated-to-get-herd-immunity-from-covid-19/","section":"post","summary":"Recently I have read an article that the Malaysian government have made a deal with Pfizer for 6.4 million Malaysian to be vaccinated. So, I am wondering what is the minimal number of people should be vaccinated.","tags":["COVID-19","Vaccine"],"title":"How many Malaysian should be vaccinated to get herd immunity from COVID-19?","type":"post"},{"authors":["Tengku Muhammad Hanis"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://tengkuhanis.netlify.app/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://tengkuhanis.netlify.app/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://tengkuhanis.netlify.app/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":[],"title":"External Project","type":"project"},{"authors":["Tengku Muhammad Hanis","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://tengkuhanis.netlify.app/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example journal article","type":"publication"},{"authors":["Tengku Muhammad Hanis","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://tengkuhanis.netlify.app/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"}]