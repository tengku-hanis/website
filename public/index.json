[{"authors":null,"categories":null,"content":"From a medical graduate to a quantitative researcher\nI did my PhD in the field of public health epidemiology in Universiti Sains Malaysia. During my PhD, I explored the potential application of machine learning and deep learning in the screening and diagnosis of breast cancer among Asian women population.\nI did my degree in medicine. However, I believed that working as a doctor in a clinical setting is not for me. Thus, I continued my master study in medical statistics. Data and analysis has sparked my interest since then. I believes that coming from medical background, give me an edge to see data in a new perspective.\nAs of now, I am working with Jom Research to provides easily accessible webinars centered around research and statistical analysis at a reasonable price point. Do check out our webinars!\n  Download my resum√©.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://tengkuhanis.netlify.app/author/tengku-muhammad-hanis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tengku-muhammad-hanis/","section":"authors","summary":"From a medical graduate to a quantitative researcher\nI did my PhD in the field of public health epidemiology in Universiti Sains Malaysia. During my PhD, I explored the potential application of machine learning and deep learning in the screening and diagnosis of breast cancer among Asian women population.","tags":null,"title":"Tengku Muhammad Hanis","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]]\rname = \u0026quot;Courses\u0026quot;\rurl = \u0026quot;courses/\u0026quot;\rweight = 50\r Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]]\rname = \u0026quot;Docs\u0026quot;\rurl = \u0026quot;docs/\u0026quot;\rweight = 50\r Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://tengkuhanis.netlify.app/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://tengkuhanis.netlify.app/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://tengkuhanis.netlify.app/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":[],"content":"","date":1700731800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701025501,"objectID":"806de4ebbbfa4ad5cef7d41c6b2dcc74","permalink":"https://tengkuhanis.netlify.app/talk/webinar-how-to-do-a-meta-analysis/","publishdate":"2023-11-23T09:30:00Z","relpermalink":"/talk/webinar-how-to-do-a-meta-analysis/","section":"talk","summary":"This webinar was organised by [Jom Research](https://jomresearch.netlify.app/). The slides and recording of the webinar is available for purchase at [Jom Research website](https://jomresearch.netlify.app/webinar_detail/2023-11-09-how-to-do-a-meta-analysis/).","tags":[],"title":"Webinar - How to do a meta-analysis","type":"talk"},{"authors":[],"categories":[],"content":"","date":1699349400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699439066,"objectID":"90db1d0c6a0e99b29c1ec613dbc2d324","permalink":"https://tengkuhanis.netlify.app/talk/webinar-how-to-write-a-bibliometric-paper/","publishdate":"2023-11-07T09:30:00Z","relpermalink":"/talk/webinar-how-to-write-a-bibliometric-paper/","section":"talk","summary":"This webinar was organised by [Jom Research](https://jomresearch.netlify.app/). The slides and recording of the webinar is available for purchase at [Jom Research website](https://jomresearch.netlify.app/webinar_detail/2023-10-26-how-to-write-a-bibliometric-paper/).","tags":[],"title":"Webinar - How to write a bibliometric paper","type":"talk"},{"authors":[],"categories":[],"content":"","date":1698742800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698733005,"objectID":"d0cc65caab68a5709c9bcea419a81ddb","permalink":"https://tengkuhanis.netlify.app/talk/webinar-bibliometric-analysis/","publishdate":"2023-10-31T09:00:00Z","relpermalink":"/talk/webinar-bibliometric-analysis/","section":"talk","summary":"This webinar was organised by Biostatistics Consultancy Service (BSC), Fakulti Perubatan, Universiti Teknologi Malaysia.","tags":[],"title":"Webinar - Bibliometric analysis","type":"talk"},{"authors":[],"categories":[],"content":"","date":1697880600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698771655,"objectID":"5f27d12820057cb2519e63bb95035af7","permalink":"https://tengkuhanis.netlify.app/talk/webinar-intro-to-r-for-non-coders/","publishdate":"2023-10-21T09:30:00Z","relpermalink":"/talk/webinar-intro-to-r-for-non-coders/","section":"talk","summary":"This webinar was organised by [Jom Research](https://jomresearch.netlify.app/). The slides and recording of the webinar is available for purchase at [Jom Research website](https://jomresearch.netlify.app/webinar_detail/2023-10-12-intro-to-r-for-non-coders/).","tags":[],"title":"Webinar - Intro to R (for non-coders)","type":"talk"},{"authors":[],"categories":[],"content":"","date":1696622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698773103,"objectID":"5a5d699d1f65913c61cdc81021f19978","permalink":"https://tengkuhanis.netlify.app/talk/webinar-an-introduction-to-spss-jamovi/","publishdate":"2023-10-06T20:00:00Z","relpermalink":"/talk/webinar-an-introduction-to-spss-jamovi/","section":"talk","summary":"This webinar was organised by [Jom Research](https://jomresearch.netlify.app/). The slides and recording of the webinar is available for purchase at [Jom Research website](https://jomresearch.netlify.app/webinar_detail/2023-09-29-an-introduction-to-spss-jamovi/).","tags":[],"title":"Webinar - An introduction to SPSS \u0026 Jamovi","type":"talk"},{"authors":[],"categories":[],"content":"","date":1695461400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698773551,"objectID":"934aa03299797f198eff0db077869b62","permalink":"https://tengkuhanis.netlify.app/talk/webinar-how-to-conduct-a-systematic-review/","publishdate":"2023-09-23T09:30:00Z","relpermalink":"/talk/webinar-how-to-conduct-a-systematic-review/","section":"talk","summary":"This webinar was organised by [Jom Research](https://jomresearch.netlify.app/). The slides and recording of the webinar is available for purchase at [Jom Research website](https://jomresearch.netlify.app/webinar_detail/2023-09-10-how-to-conduct-a-systematic-review/).","tags":[],"title":"Webinar - How to conduct a systematic review","type":"talk"},{"authors":[],"categories":[],"content":"","date":1693648800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694932198,"objectID":"57dc2a0b70410f7ad2d202da590ea958","permalink":"https://tengkuhanis.netlify.app/talk/webinar-how-to-write-a-scientific-paper-for-beginners-for-health-sciences/","publishdate":"2023-09-02T10:00:00Z","relpermalink":"/talk/webinar-how-to-write-a-scientific-paper-for-beginners-for-health-sciences/","section":"talk","summary":"This webinar was organised by [Jom Research](https://jomresearch.netlify.app/). The slides and recording of the webinar is available for purchase at [Jom Research website](https://jomresearch.netlify.app/webinar_detail/2023-09-07-how-to-write-a-scientific-paper-for-beginners-for-health-sciences/).","tags":[],"title":"Webinar - How to write a scientific paper for beginners (for health sciences)","type":"talk"},{"authors":[],"categories":[],"content":"","date":1692304200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694931452,"objectID":"0b4c85e7d5c4d375f294aa616feabd8d","permalink":"https://tengkuhanis.netlify.app/talk/webinar-how-to-read-a-paper-for-beginners/","publishdate":"2023-08-17T20:30:00Z","relpermalink":"/talk/webinar-how-to-read-a-paper-for-beginners/","section":"talk","summary":"This webinar was organised by [Jom Research](https://jomresearch.netlify.app/). The slides and recording of the webinar is available for purchase at [Jom Research website](https://jomresearch.netlify.app/webinar_detail/2023-09-07-how-to-read-a-paper-for-beginners/).","tags":[],"title":"Webinar - How to read a paper for beginners","type":"talk"},{"authors":["Wan Shakira Rodzlan Hasani","Nor Asiah Muhamad","Tengku Muhammad Hanis","Nur Hasnah Maamor","Xin Wee Chen","Mohd Azahadi Omar","Yee Cheng Kueh","Zulkarnain Abd Karim","Muhammad Radzi Abu Hassan","Kamarul Imran Musa"],"categories":null,"content":"","date":1692144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692144000,"objectID":"61a6a01168117b53e40c42aea015b136","permalink":"https://tengkuhanis.netlify.app/publication/2023-08-16-the-global-estimate-of-premature-cardiovascular-mortality-a-systematic-review-and-meta-analysis-of-age-standardized-mortality-rate/","publishdate":"2023-08-16T00:00:00Z","relpermalink":"/publication/2023-08-16-the-global-estimate-of-premature-cardiovascular-mortality-a-systematic-review-and-meta-analysis-of-age-standardized-mortality-rate/","section":"publication","summary":"","tags":[],"title":"The global estimate of premature cardiovascular mortality: a systematic review and meta-analysis of age-standardized mortality rate","type":"publication"},{"authors":[],"categories":[],"content":"","date":1691226000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694930472,"objectID":"bfefa71da4a1f7daba354c7d4ec778b5","permalink":"https://tengkuhanis.netlify.app/talk/how-to-publish-a-paper-for-beginners-and-newbies-for-health-sciences/","publishdate":"2023-08-05T09:00:00Z","relpermalink":"/talk/how-to-publish-a-paper-for-beginners-and-newbies-for-health-sciences/","section":"talk","summary":"This webinar was organised by [Jom Research](https://jomresearch.netlify.app/). The slides and recording of the webinar is available for purchase at [Jom Research website](https://jomresearch.netlify.app/webinar_detail/2023-09-07-how-to-publish-a-paper-for-beginners-and-newbies-for-health-sciences/).","tags":null,"title":"Webinar - How to publish a paper for beginners and newbies (for health sciences)","type":"talk"},{"authors":[],"categories":[],"content":"","date":1690458000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690870764,"objectID":"9f5998d7b02c8898ab5fd928b52a976d","permalink":"https://tengkuhanis.netlify.app/talk/talk-a-guide-to-conducting-systematic-review-on-stroke/","publishdate":"2023-07-27T11:40:00Z","relpermalink":"/talk/talk-a-guide-to-conducting-systematic-review-on-stroke/","section":"talk","summary":"This talk was part of [Malaysia Stroke Conference 2023](https://strokeconference.com.my/index.php) organised by [Malaysia Stroke Council](https://www.strokecouncil.org/).","tags":null,"title":"Talk - A guide to conducting systematic review on stroke","type":"talk"},{"authors":["Tengku Muhammad Hanis","Nur Intan Raihana Ruhaiyem","Wan Nor Arifin","Juhara Haron","Wan Faiziah Wan Abdul Rahman","Rosni Abdullah","Kamarul Imran Musa"],"categories":null,"content":"","date":1684368000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684368000,"objectID":"1d0e6e53c04e87143d41261fb85bc8e7","permalink":"https://tengkuhanis.netlify.app/publication/2023-05-18-developing-a-supplementary-diagnostic-tool-for-breast-cancer-risk-estimation-using-ensemble-transfer-learning/","publishdate":"2023-05-18T00:00:00Z","relpermalink":"/publication/2023-05-18-developing-a-supplementary-diagnostic-tool-for-breast-cancer-risk-estimation-using-ensemble-transfer-learning/","section":"publication","summary":"","tags":[],"title":"Developing a Supplementary Diagnostic Tool for Breast Cancer Risk Estimation Using Ensemble Transfer Learning","type":"publication"},{"authors":["Wan Shakira Rodzlan Hasani","Nor Asiah Muhamad","Nur Hasnah Maamor","Tengku Muhammad Hanis","Chen Xin Wee","Muhammad Radzi Abu Hassan","Zulkarnain Abdul Karim","Kamarul Imran Musa"],"categories":null,"content":"","date":1683072000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683072000,"objectID":"963c8c0704f5826d532ff7a2b046373b","permalink":"https://tengkuhanis.netlify.app/publication/2023-05-04-premature-mortality-and-years-of-potential-life-lost-from-cardiovascular-diseases-protocol-of-a-systematic-review-and-meta-analysis/","publishdate":"2023-05-03T00:00:00Z","relpermalink":"/publication/2023-05-04-premature-mortality-and-years-of-potential-life-lost-from-cardiovascular-diseases-protocol-of-a-systematic-review-and-meta-analysis/","section":"publication","summary":"","tags":[],"title":"Premature mortality and years of potential life lost from cardiovascular diseases: Protocol of a systematic review and meta-analysis","type":"publication"},{"authors":["Wan Shakira Rodzlan Hasani","Nor Asiah Muhamad","Tengku Muhammad Hanis","Nur Hasnah Maamor","Chen Xin Wee","Mohd Azahadi Omar","Shubash Shander Ganapathy","Zulkarnain Abdul Karim","Kamarul Imran Musa"],"categories":null,"content":"","date":1682035200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682035200,"objectID":"78785250cd5de32f73febf9eddb74fd0","permalink":"https://tengkuhanis.netlify.app/publication/2023-04-30-the-burden-of-premature-mortality-from-cardiovascular-diseases-a-systematic-review-of-years-of-life-lost/","publishdate":"2023-04-21T00:00:00Z","relpermalink":"/publication/2023-04-30-the-burden-of-premature-mortality-from-cardiovascular-diseases-a-systematic-review-of-years-of-life-lost/","section":"publication","summary":"","tags":[],"title":"The burden of premature mortality from cardiovascular diseases: A systematic review of years of life lost","type":"publication"},{"authors":["Mohd Azmi Bin Suliman","Tengku Muhammad Hanis","Mohd Khairul Anwar Kamdi","Mohd Ismail Ibrahim","Kamarul Imran Musa"],"categories":null,"content":"","date":1678060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678060800,"objectID":"dc93eaa6e0aacdfb95368f8f07578517","permalink":"https://tengkuhanis.netlify.app/publication/2023-03-07-a-bibliometric-analysis-of-stroke-caregiver-research-from-1989-to-2022/","publishdate":"2023-03-06T00:00:00Z","relpermalink":"/publication/2023-03-07-a-bibliometric-analysis-of-stroke-caregiver-research-from-1989-to-2022/","section":"publication","summary":"","tags":[],"title":"A Bibliometric Analysis of Stroke Caregiver Research from 1989 to 2022","type":"publication"},{"authors":[],"categories":["R","Epidemiology","Map"],"content":"\rI have written two blog posts about making map in R:\nMaking maps with R (my first attempt ever!)\rMy first interactive map with {leaflet}\r\rThis post is sort of a continuation to the first blog post. I have shown how to plot a coordinate to a map in that post specifically for Malaysia.\nHowever, using the two approaches in the previous blog post, we cannot plot the coordinate to a certain states in Malaysia. At least I am not unable to find how to do that after googling around. But, we can plot the borneo or peninsular of Malaysia using the two approaches.\nPlot the peninsular of Malaysia (not the best way)\rLoad the necessary packages.\nlibrary(rworldmap) library(tidyverse)\rFirst, we get the data. The data is about desa clinic (klinik desa) in Malaysia.\nclinicDesa \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/tengku-hanis/clinic-data/main/clinicdesa.csv\u0026quot;)\rhead(clinicDesa)\r## id facilities_id name address postcode\r## 1 1 KD01010019 KLINIK DESA ASSAM BUBOK Jalan Batu Pahat 86400\r## 2 2 KD01010020 KLINIK DESA BATU PUTIH Jalan Behor Temak 83000\r## 3 3 KD01010021 KLINIK DESA BEROLEH Jalan Parit Besar 83300\r## 4 4 KD01010022 KLINIK DESA BINDU Jalan Tongkang Pecah 83010\r## 5 5 KD01010023 KLINIK DESA KAMPUNG BARU Jalan Parit Kemang 83710\r## 6 6 KD01010024 KLINIK DESA KANGKAR BARU Jalan Meng Seng 85400\r## city district state tel fax website email image latitude\r## 1 Ayer Hitam Batu Pahat Johor NA NA NA NA 1.933330\r## 2 Bagan Batu Pahat Johor NA NA NA NA 1.889100\r## 3 Sri Gading Batu Pahat Johor NA NA NA NA 1.877890\r## 4 Tongkang Pecah Batu Pahat Johor NA NA NA NA 1.901515\r## 5 Parit Yaani Batu Pahat Johor NA NA NA NA 1.905120\r## 6 Yong Peng Batu Pahat Johor NA NA NA NA 2.065310\r## longitude likes rating status\r## 1 103.1167 0 0 NEW\r## 2 102.8778 0 0 NEW\r## 3 102.9858 0 0 NEW\r## 4 102.9665 0 0 NEW\r## 5 103.0372 0 0 NEW\r## 6 103.1248 0 0 NEW\rFirst we plot the data.\nggplot(clinicDesa, aes(longitude, latitude)) +\rgeom_point() +\rtheme_minimal()\rRemove the two points.\nclinicDesa2 \u0026lt;- clinicDesa %\u0026gt;% filter(longitude \u0026gt; 25)\rAgain, plot the updated data.\nggplot(clinicDesa2, aes(longitude, latitude)) +\rgeom_point() +\rtheme_minimal()\rFrom the plot, we already know the left side consists of the coordinates in the peninsular of Malaysia. So, we can limit our plot by limit the longitude \u0026lt; 105 and longitude \u0026gt; 97.\n# Get base map\rglobal \u0026lt;- map_data(\u0026quot;world\u0026quot;) # Plot\rggplot() + geom_polygon(data = global %\u0026gt;% filter(region == \u0026quot;Malaysia\u0026quot;), aes(x=long, y = lat, group = group), fill = \u0026quot;gray85\u0026quot;) + coord_fixed(1.3) +\rgeom_point(data = clinicDesa2, aes(x = longitude, y = latitude)) +\rtheme_minimal() + xlab(\u0026quot;Longitude\u0026quot;) +\rylab(\u0026quot;Latitude\u0026quot;) +\rlabs(title = \u0026quot;Desa clinic in the peninsular of Malaysia\u0026quot;, subtitle = \u0026quot;(Data last updated: Klinik Desa - 9 Mac 2021)\u0026quot;,\rcaption = expression(paste(italic(\u0026quot;Sumber data: https://www.data.gov.my/data/ms_MY/group/pemetaan\u0026quot;)))) +\rxlim(97, 105) #limit overall map to peninsular of Malaysia\rI am not going to re-explain the above and below codes as I have explain it in the previous blog post.\nThis approach also works with rworldmap.\n# Get base map\rworld \u0026lt;- getMap(resolution = \u0026quot;low\u0026quot;)\rmsia \u0026lt;- world[world@data$ADMIN == \u0026quot;Malaysia\u0026quot;, ]\r# Plot\rggplot() +\rgeom_polygon(data = msia, aes(x = long, y = lat, group = group), fill = NA, colour = \u0026quot;black\u0026quot;) +\rgeom_point(data = clinicDesa2, aes(x = longitude, y = latitude)) +\rcoord_quickmap() + theme_minimal() + xlab(\u0026quot;Longitude\u0026quot;) +\rylab(\u0026quot;Latitude\u0026quot;) +\rlabs(title = \u0026quot;Desa clinic in the peninsular of Malaysia\u0026quot;, subtitle = \u0026quot;(Data last updated: Klinik Desa - 9 Mac 2021)\u0026quot;,\rcaption = expression(paste(italic(\u0026quot;Sumber data: https://www.data.gov.my/data/ms_MY/group/pemetaan\u0026quot;)))) +\rxlim(97, 105) #limit overall map to peninsular of Malaysia\rAs we can see using the two approaches, we can plot the borne and peninsular sides of Malaysia. But, at least to my knowledge we cannot apply this approach if we want to plot a coordinate to certain states in Malaysia.\n\rPlot the states in Malaysia\rLoad the necessary package.\nlibrary(geodata)\rlibrary(tidyterra)\rAs we can see from the package, we going to use a geodata package. tidyterra is used to supplements the ggplot. First, let‚Äôs limit the data into desa clinics in Terengganu only.\nclinic_trg \u0026lt;- clinicDesa %\u0026gt;% filter(state == \u0026quot;Terengganu\u0026quot;) %\u0026gt;% dplyr::select(latitude, longitude) head(clinic_trg)\r## latitude longitude\r## 1 5.48533 102.4914\r## 2 5.81578 102.5778\r## 3 5.70886 102.4892\r## 4 5.75722 102.5303\r## 5 5.67444 102.6289\r## 6 5.69875 102.5430\rNow we get the map from the geodata package with the boundaries at the district level.\nMalaysia \u0026lt;- gadm(country = \u0026quot;MYS\u0026quot;, level = 2, path=tempdir())\rWe can use the below information to limit the map to Terengganu state only.\nMalaysia$NAME_1\r## [1] \u0026quot;Johor\u0026quot; \u0026quot;Johor\u0026quot; \u0026quot;Johor\u0026quot; \u0026quot;Johor\u0026quot; ## [5] \u0026quot;Johor\u0026quot; \u0026quot;Johor\u0026quot; \u0026quot;Johor\u0026quot; \u0026quot;Johor\u0026quot; ## [9] \u0026quot;Johor\u0026quot; \u0026quot;Johor\u0026quot; \u0026quot;Kedah\u0026quot; \u0026quot;Kedah\u0026quot; ## [13] \u0026quot;Kedah\u0026quot; \u0026quot;Kedah\u0026quot; \u0026quot;Kedah\u0026quot; \u0026quot;Kedah\u0026quot; ## [17] \u0026quot;Kedah\u0026quot; \u0026quot;Kedah\u0026quot; \u0026quot;Kedah\u0026quot; \u0026quot;Kedah\u0026quot; ## [21] \u0026quot;Kedah\u0026quot; \u0026quot;Kedah\u0026quot; \u0026quot;Kelantan\u0026quot; \u0026quot;Kelantan\u0026quot; ## [25] \u0026quot;Kelantan\u0026quot; \u0026quot;Kelantan\u0026quot; \u0026quot;Kelantan\u0026quot; \u0026quot;Kelantan\u0026quot; ## [29] \u0026quot;Kelantan\u0026quot; \u0026quot;Kelantan\u0026quot; \u0026quot;Kelantan\u0026quot; \u0026quot;Kelantan\u0026quot; ## [33] \u0026quot;Kuala Lumpur\u0026quot; \u0026quot;Labuan\u0026quot; \u0026quot;Melaka\u0026quot; \u0026quot;Melaka\u0026quot; ## [37] \u0026quot;Melaka\u0026quot; \u0026quot;Negeri Sembilan\u0026quot; \u0026quot;Negeri Sembilan\u0026quot; \u0026quot;Negeri Sembilan\u0026quot;\r## [41] \u0026quot;Negeri Sembilan\u0026quot; \u0026quot;Negeri Sembilan\u0026quot; \u0026quot;Negeri Sembilan\u0026quot; \u0026quot;Negeri Sembilan\u0026quot;\r## [45] \u0026quot;Pahang\u0026quot; \u0026quot;Pahang\u0026quot; \u0026quot;Pahang\u0026quot; \u0026quot;Pahang\u0026quot; ## [49] \u0026quot;Pahang\u0026quot; \u0026quot;Pahang\u0026quot; \u0026quot;Pahang\u0026quot; \u0026quot;Pahang\u0026quot; ## [53] \u0026quot;Pahang\u0026quot; \u0026quot;Pahang\u0026quot; \u0026quot;Pahang\u0026quot; \u0026quot;Perak\u0026quot; ## [57] \u0026quot;Perak\u0026quot; \u0026quot;Perak\u0026quot; \u0026quot;Perak\u0026quot; \u0026quot;Perak\u0026quot; ## [61] \u0026quot;Perak\u0026quot; \u0026quot;Perak\u0026quot; \u0026quot;Perak\u0026quot; \u0026quot;Perak\u0026quot; ## [65] \u0026quot;Perak\u0026quot; \u0026quot;Perlis\u0026quot; \u0026quot;Pulau Pinang\u0026quot; \u0026quot;Pulau Pinang\u0026quot; ## [69] \u0026quot;Pulau Pinang\u0026quot; \u0026quot;Pulau Pinang\u0026quot; \u0026quot;Pulau Pinang\u0026quot; \u0026quot;Putrajaya\u0026quot; ## [73] \u0026quot;Sabah\u0026quot; \u0026quot;Sabah\u0026quot; \u0026quot;Sabah\u0026quot; \u0026quot;Sabah\u0026quot; ## [77] \u0026quot;Sabah\u0026quot; \u0026quot;Sabah\u0026quot; \u0026quot;Sabah\u0026quot; \u0026quot;Sabah\u0026quot; ## [81] \u0026quot;Sabah\u0026quot; \u0026quot;Sabah\u0026quot; \u0026quot;Sabah\u0026quot; \u0026quot;Sabah\u0026quot; ## [85] \u0026quot;Sabah\u0026quot; \u0026quot;Sabah\u0026quot; \u0026quot;Sabah\u0026quot; \u0026quot;Sabah\u0026quot; ## [89] \u0026quot;Sabah\u0026quot; \u0026quot;Sabah\u0026quot; \u0026quot;Sabah\u0026quot; \u0026quot;Sabah\u0026quot; ## [93] \u0026quot;Sabah\u0026quot; \u0026quot;Sabah\u0026quot; \u0026quot;Sabah\u0026quot; \u0026quot;Sabah\u0026quot; ## [97] \u0026quot;Sabah\u0026quot; \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; ## [101] \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; ## [105] \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; ## [109] \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; ## [113] \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; ## [117] \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; ## [121] \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; ## [125] \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; \u0026quot;Sarawak\u0026quot; ## [129] \u0026quot;Selangor\u0026quot; \u0026quot;Selangor\u0026quot; \u0026quot;Selangor\u0026quot; \u0026quot;Selangor\u0026quot; ## [133] \u0026quot;Selangor\u0026quot; \u0026quot;Selangor\u0026quot; \u0026quot;Selangor\u0026quot; \u0026quot;Selangor\u0026quot; ## [137] \u0026quot;Selangor\u0026quot; \u0026quot;Trengganu\u0026quot; \u0026quot;Trengganu\u0026quot; \u0026quot;Trengganu\u0026quot; ## [141] \u0026quot;Trengganu\u0026quot; \u0026quot;Trengganu\u0026quot; \u0026quot;Trengganu\u0026quot; \u0026quot;Trengganu\u0026quot;\rSo, this is the plot for Terengganu.\nTrg \u0026lt;- Malaysia[138:144,]\rplot(Trg)\rWe going to the map this in ggplot, and stacked the map layer with the coordinate layer.\nggplot() +\rgeom_spatvector(data = Trg, color = \u0026quot;grey\u0026quot;, fill = NA) +\rgeom_point(data = clinic_trg, aes(x = longitude, y = latitude, color = \u0026quot;red\u0026quot;)) +\rtheme_minimal() +\rtheme(legend.position = \u0026quot;none\u0026quot;) +\rxlab(\u0026quot;Longitude\u0026quot;) +\rylab(\u0026quot;Latitude\u0026quot;) +\rlabs(title = \u0026quot;Desa clinic in Terengganu, Malaysia\u0026quot;, subtitle = \u0026quot;(Data last updated: Klinik Desa - 9 Mac 2021)\u0026quot;,\rcaption = expression(paste(italic(\u0026quot;Sumber data: https://www.data.gov.my/data/ms_MY/group/pemetaan\u0026quot;)))) \rgeom_spatvector is from tidyterra package. Alternatively, we can plot using geom_sfbut we need to convert the SpatVector data into sf object using sf::st_as_sf.\nggplot(data = sf::st_as_sf(Trg)) +\rgeom_sf(color = \u0026quot;grey\u0026quot;, fill = NA) +\rgeom_point(data = clinic_trg, aes(x = longitude, y = latitude, color = \u0026quot;red\u0026quot;)) +\rtheme_minimal() +\rtheme(legend.position = \u0026quot;none\u0026quot;) +\rxlab(\u0026quot;Longitude\u0026quot;) +\rylab(\u0026quot;Latitude\u0026quot;) +\rlabs(title = \u0026quot;Desa clinic in Terengganu, Malaysia\u0026quot;, subtitle = \u0026quot;(Data last updated: Klinik Desa - 9 Mac 2021)\u0026quot;,\rcaption = expression(paste(italic(\u0026quot;Sumber data: https://www.data.gov.my/data/ms_MY/group/pemetaan\u0026quot;)))) \rBoth approaches produce the same plot.\nWe can further add district labels to the plots. For example, using the geom_sf, we can stack it with geom_sf_label layer. We can alternatively use theme_void to remove the background and the map axis.\nggplot(data = sf::st_as_sf(Trg)) +\rgeom_sf(color = \u0026quot;grey\u0026quot;, fill = NA) +\rgeom_sf_label(aes(label = NAME_2)) +\rgeom_point(data = clinic_trg, aes(x = longitude, y = latitude, color = \u0026quot;red\u0026quot;)) +\rtheme_void() +\rtheme(legend.position = \u0026quot;none\u0026quot;) +\rxlab(\u0026quot;Longitude\u0026quot;) +\rylab(\u0026quot;Latitude\u0026quot;) +\rlabs(title = \u0026quot;Desa clinic in Terengganu, Malaysia\u0026quot;, subtitle = \u0026quot;(Data last updated: Klinik Desa - 9 Mac 2021)\u0026quot;,\rcaption = expression(paste(italic(\u0026quot;Sumber data: https://www.data.gov.my/data/ms_MY/group/pemetaan\u0026quot;)))) \r\r","date":1677024000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677094161,"objectID":"a4cce3795529c0a25dd53bc68d24fb58","permalink":"https://tengkuhanis.netlify.app/post/mapping-the-states-in-malaysia/","publishdate":"2023-02-22T00:00:00Z","relpermalink":"/post/mapping-the-states-in-malaysia/","section":"post","summary":"I have written two blog posts about making map in R:\nMaking maps with R (my first attempt ever!)\rMy first interactive map with {leaflet}\r\rThis post is sort of a continuation to the first blog post.","tags":["map","spatial"],"title":"Mapping the states in Malaysia","type":"post"},{"authors":[],"categories":[],"content":"\rData augmentation\rData augmentation is been used in deep learning for many reasons. One of the reason is to reduce overfitting and makes the model more robust. Data augmentation can be done relatively easy in keras package in R. However, I have not found any resources on how to visualise the augmented image in R except in Python. Visualising the augmented image can be quite useful to get an idea of how the image looks like. So, this post covers a simple to do this in R.\n\rR code\rLet‚Äôs load the keras library\nlibrary(keras)\r## Warning: package \u0026#39;keras\u0026#39; was built under R version 4.2.2\rNext, we load the image from the internet.\nr_logo \u0026lt;- get_file(\u0026quot;img\u0026quot;, \u0026quot;https://ih1.redbubble.net/image.522493300.6771/st,small,507x507-pad,600x600,f8f8f8.jpg\u0026quot;) %\u0026gt;% image_load()\rOur image right now is 600x 600 x 3. The 3 at the back because the image is coloured (RGB channels).\nr_logo$size\r## [[1]]\r## [1] 600\r## ## [[2]]\r## [1] 600\rSo, we need to change the image into an array with the dimension of 1 x 600 x 600 x 3. The number 1 indicates we have only one image.\nr_logo \u0026lt;- r_logo %\u0026gt;% image_to_array() %\u0026gt;% array_reshape(c(1, dim(.)))\rdim(r_logo)\r## [1] 1 600 600 3\rOnce we have a correct dimension, we can specify the parameters for the data augmentation.\naugment_params \u0026lt;- image_data_generator(horizontal_flip = T, vertical_flip = T,\rrotation_range = 0.5,\rzoom_range = 0.5,\rfill_mode = \u0026quot;reflect\u0026quot;)\rI am not going to into the details of the parameters. For those interested, the TensorFlow for R website explain this very well.\nNext, we can generate the batch of augmented data at random. This function, however, will only run once we fit the model.\nimg_gen \u0026lt;- flow_images_from_data(r_logo,\rgenerator = augment_params, batch_size = 1)\rFinally, we can plot the image. Firstly, this is our original image.\nimg_gen$x [1,,,] %\u0026gt;% as.raster(max = 255) %\u0026gt;% as.array() %\u0026gt;% plot()\rNow, we going to loop the augmentation process. Here, we going to generate six augmented images. The set.seed for reproducibility.\nset.seed(123)\rpar(mfrow = c(3, 2), mar = c(1, 0, 1, 0))\rfor (i in 1:6) {\rIMG \u0026lt;- img_gen$`next`()\rIMG[1,,,] %\u0026gt;% as.raster(max = 255) %\u0026gt;% as.array() %\u0026gt;% plot()\r}\r\rConclusion\rI believe this is quite useful to get a sense of how your data is augmented. Consequently, this may help in selecting the parameters for the data augmentation.\n\r","date":1672185600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672227465,"objectID":"ef2420e13b2ae116af4fecaccc1c9a0b","permalink":"https://tengkuhanis.netlify.app/post/visualising-augmented-images-in-keras/","publishdate":"2022-12-28T00:00:00Z","relpermalink":"/post/visualising-augmented-images-in-keras/","section":"post","summary":"Data augmentation\rData augmentation is been used in deep learning for many reasons. One of the reason is to reduce overfitting and makes the model more robust. Data augmentation can be done relatively easy in keras package in R.","tags":["Deep Learning","Image analysis","keras"],"title":"Visualising augmented images in Keras","type":"post"},{"authors":["Tengku Muhammad Hanis","Wan Nor Arifin","Kamarul Imran Musa","Wan Shakira Rodzlan Hasani","Che Muhammad Nur Hidayat Che Nawi","Shahnon Anuar Shahrani","Xin Wee Chen","Mohd Azmi Suliman","Erwan Ershad Ahmad Khan","Wira Alfatah Ab Aziz","Mohamad Zarudin Mat Said"],"categories":null,"content":"","date":1671926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671926400,"objectID":"32acc40d97f40eb18e678a09be25d4d5","permalink":"https://tengkuhanis.netlify.app/publication/2022-12-25-risk-factors-for-covid-19-mortality-in-malaysia/","publishdate":"2022-12-25T00:00:00Z","relpermalink":"/publication/2022-12-25-risk-factors-for-covid-19-mortality-in-malaysia/","section":"publication","summary":"","tags":[],"title":"Risk Factors for COVID-19 Mortality in Malaysia","type":"publication"},{"authors":["Tengku Muhammad Hanis","Juhara Haron","Wan Faiziah Wan Abdul Rahman","Wan Nor Arifin","Nur Intan Raihana Ruhaiyem","Rosni Abdullah","Kamarul Imran Musa"],"categories":null,"content":"","date":1670976000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670976000,"objectID":"44adc38b3c923ffe2d6046f97b5a3fe2","permalink":"https://tengkuhanis.netlify.app/publication/2022-12-14-mapping-breast-cancer-research/","publishdate":"2022-12-14T00:00:00Z","relpermalink":"/publication/2022-12-14-mapping-breast-cancer-research/","section":"publication","summary":"","tags":[],"title":"Mapping Breast Cancer Research in Malaysia: A Scientometric Analysis","type":"publication"},{"authors":[],"categories":[],"content":"","date":1669528800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669533328,"objectID":"80147e1c4f9384a2291e610e5bb16ee2","permalink":"https://tengkuhanis.netlify.app/talk/talk-exploring-past-literature-a-bibliometric-approach/","publishdate":"2022-11-27T14:00:00+08:00","relpermalink":"/talk/talk-exploring-past-literature-a-bibliometric-approach/","section":"talk","summary":"This talk was part of R confeRence 2022 organised by [Malaysia R User Group (MyRUG)](https://www.facebook.com/rusergroupmalaysia).","tags":[],"title":"Talks - Exploring past literature: A bibliometric approach","type":"talk"},{"authors":["Tengku Muhammad Hanis","Nur Intan Raihana Ruhaiyem","Wan Nor Arifin","Juhara Haron","Wan Faiziah Wan Abdul Rahman","Rosni Abdullah","Kamarul Imran Musa"],"categories":null,"content":"","date":1668556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668556800,"objectID":"506f14167b679815469f3e1dfe7e99e3","permalink":"https://tengkuhanis.netlify.app/publication/2022-01-01_over-the-counter_bre/","publishdate":"2022-11-16T00:00:00Z","relpermalink":"/publication/2022-01-01_over-the-counter_bre/","section":"publication","summary":"","tags":[],"title":"Over-the-Counter Breast Cancer Classification Using Machine Learning and Patient Registration Records","type":"publication"},{"authors":[],"categories":[],"content":"","date":1666260000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666108227,"objectID":"d3c95fdfcbd9178d4d0e44441f917257","permalink":"https://tengkuhanis.netlify.app/talk/workshop-on-scoping-review-bibliometric-analysis/","publishdate":"2022-10-20T10:00:00Z","relpermalink":"/talk/workshop-on-scoping-review-bibliometric-analysis/","section":"talk","summary":"Organised by Department of Community Medicine, USM and Innovative Network of Community Club (INCOME).","tags":[],"title":"Workshop on scoping review \u0026 bibliometric analysis","type":"talk"},{"authors":["Wan Shakira Rodzlan Hasani","Tengku Muhammad Hanis","Nor Asiah Muhamad","Md Asiful Islam","Chen Xin Wee","Kamarul Imran Musa"],"categories":null,"content":"","date":1664755200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664755200,"objectID":"7d0c923da03e8a3c7709b3a87f9d456b","permalink":"https://tengkuhanis.netlify.app/publication/2022-01-01_bibliometric_analysi/","publishdate":"2022-10-03T00:00:00Z","relpermalink":"/publication/2022-01-01_bibliometric_analysi/","section":"publication","summary":"","tags":[],"title":"Bibliometric Analysis of Global Research Activity on Premature Mortality","type":"publication"},{"authors":["Md Asiful Islam","Shoumik Kundu","Tengku Muhammad Hanis","Khalid Hajissa","Kamarul Imran Musa"],"categories":null,"content":"","date":1658880000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658880000,"objectID":"b986f26629b8488264a349d12c707628","permalink":"https://tengkuhanis.netlify.app/publication/2022-01-01_a_global_bibliometri/","publishdate":"2022-07-27T00:00:00Z","relpermalink":"/publication/2022-01-01_a_global_bibliometri/","section":"publication","summary":"","tags":[],"title":"A Global Bibliometric Analysis on Antibiotic-Resistant Active Pulmonary Tuberculosis Over the Last 25 Years (1996-2020)","type":"publication"},{"authors":["Tengku Muhammad Hanis","Md Asiful Islam","Kamarul Imran Musa"],"categories":null,"content":"","date":1656979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656979200,"objectID":"9ac05f52d6f29a8a0f4172c101c7a407","permalink":"https://tengkuhanis.netlify.app/publication/2021-11-06-a-meta-analysis-of-diagnostic-accuracy-of-machine-learning-models-on-mammography-in-breast-cancer-classification/","publishdate":"2022-07-05T00:00:00Z","relpermalink":"/publication/2021-11-06-a-meta-analysis-of-diagnostic-accuracy-of-machine-learning-models-on-mammography-in-breast-cancer-classification/","section":"publication","summary":"","tags":[],"title":"Diagnostic Accuracy of Machine Learning Models on Mammography in Breast Cancer Classification: A Meta-Analysis","type":"publication"},{"authors":["Tengku Muhammad Hanis","Nur Intan Raihana Ruhaiyem","Wan Nor Arifin","Juhara Haron","Wan Faiziah Wan Abdul Rahman","Rosni Abdullah","Kamarul Imran Musa"],"categories":null,"content":"","date":1655078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655078400,"objectID":"9cb6f97de511af698174b59a01ef24ae","permalink":"https://tengkuhanis.netlify.app/publication/2022-01-01_developing_an_over-t/","publishdate":"2022-06-13T00:00:00Z","relpermalink":"/publication/2022-01-01_developing_an_over-t/","section":"publication","summary":"","tags":[],"title":"Developing an over-the-counter screening model for breast cancer among the Asian women population","type":"publication"},{"authors":[],"categories":["R","Machine Learning"],"content":"\r\rUMAP\rUniform manifold approximation and projection or in short UMAP is a type of dimension reduction techniques. So, basically UMAP will project a set of features into a smaller space. UMAP can be a supervised technique in which we give a label or an outcome or an unsupervised one. For those interested to know in detail how UMAP works can refer to this reference. For those prefer a much simpler or shorter version of it, I recommend a YouTube video by Joshua Starmer.\n\rExample in R\rWe going to see how to apply a UMAP techniques for image preprocessing and further classify the images using kNN and naive bayes.\nThese are the packages that we need.\nlibrary(keras) #for data and reshape to tabular format\rlibrary(tidymodels)\rlibrary(embed) #for umap\rlibrary(discrim) #for naive bayes model\rWe going to use the famous MNIST dataset. This dataset contained a handwritten digit from 0 to 9. This dataset is available in keras package.\nmnist_data \u0026lt;- dataset_mnist()\r## Loaded Tensorflow version 2.2.0\rimage_data \u0026lt;- mnist_data$train$x\rimage_labels \u0026lt;- mnist_data$train$y\rimage_data %\u0026gt;% dim()\r## [1] 60000 28 28\rFor example this is the image for the second row.\nimage_data[2, 1:28, 1:28] %\u0026gt;% t() %\u0026gt;% image(col = gray.colors(256))\rNext, we going to change the image into a tabular data frame format. We going to limit the data to the first 1000 rows or images out of the total 6000 images.\n# Reformat to tabular format\rimage_data \u0026lt;- array_reshape(image_data, dim = c(60000, 28*28))\rimage_data %\u0026gt;% dim()\r## [1] 60000 784\rimage_data \u0026lt;- image_data[1:10000,]\rimage_labels \u0026lt;- image_labels[1:10000]\r# Reformat to data frame\rfull_data \u0026lt;- data.frame(image_data) %\u0026gt;% bind_cols(label = image_labels) %\u0026gt;% mutate(label = as.factor(label))\rThen, we going to split the data and create a 3-folds cross-validation sets for the sake of simplicity.\n# Split data\rset.seed(123)\rind \u0026lt;- initial_split(full_data)\rdata_train \u0026lt;- training(ind) data_test \u0026lt;- testing(ind)\r# 10-folds CV\rset.seed(123)\rdata_cv \u0026lt;- vfold_cv(data_train, v = 3)\rFor recipe specification, we going to scale and center all the predictor after creating a new variable using step_umap(). Notice that in step_umap() we supply the outcome and we tune the number of components (num_comp).\nrec \u0026lt;- recipe(label ~ ., data = data_train) %\u0026gt;% step_umap(all_predictors(), outcome = vars(label), num_comp = tune()) %\u0026gt;% step_center(all_predictors()) %\u0026gt;% step_scale(all_predictors())\rWe create a a base workflow.\nwf \u0026lt;- workflow() %\u0026gt;% add_recipe(rec) \rWe going to use two models as classifier:\nkNN\n\rNaive bayes\r\rFor each classifier, we going to create a regular grid of parameters to be tuned and further run a regular grid search.\nFor kNN.\n# knn model\rknn_mod \u0026lt;- nearest_neighbor(neighbors = tune()) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) %\u0026gt;% set_engine(\u0026quot;kknn\u0026quot;)\r# knn grid\rknn_grid \u0026lt;- grid_regular(neighbors(), num_comp(range = c(2, 8)), levels = 3)\r# Tune grid search\rknn_tune \u0026lt;- tune_grid(\rwf %\u0026gt;% add_model(knn_mod),\rresamples = data_cv,\rgrid = knn_grid, control = control_grid(verbose = F)\r)\rFor naive bayes.\n# nb model\rnb_mod \u0026lt;- naive_Bayes(smoothness = tune()) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) %\u0026gt;% set_engine(\u0026quot;naivebayes\u0026quot;)\r# nb grid\rnb_grid \u0026lt;- grid_regular(smoothness(), num_comp(range = c(2, 10)), levels = 3)\r# Tune grid search\rnb_tune \u0026lt;- tune_grid(\rwf %\u0026gt;% add_model(nb_mod),\rresamples = data_cv,\rgrid = nb_grid, control = control_grid(verbose = F)\r)\rLet‚Äôs see our tuning performance of our model.\n# knn model\rknn_tune %\u0026gt;% show_best(\u0026quot;roc_auc\u0026quot;)\r## # A tibble: 5 x 8\r## neighbors num_comp .metric .estimator mean n std_err .config ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 10 8 roc_auc hand_till 0.961 3 0.000268 Preprocessor3_Mode~\r## 2 10 5 roc_auc hand_till 0.961 3 0.000421 Preprocessor2_Mode~\r## 3 5 8 roc_auc hand_till 0.959 3 0.000757 Preprocessor3_Mode~\r## 4 10 2 roc_auc hand_till 0.959 3 0.000737 Preprocessor1_Mode~\r## 5 5 5 roc_auc hand_till 0.958 3 0.000740 Preprocessor2_Mode~\rknn_tune %\u0026gt;% show_best(\u0026quot;accuracy\u0026quot;)\r## # A tibble: 5 x 8\r## neighbors num_comp .metric .estimator mean n std_err .config ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 10 8 accuracy multiclass 0.914 3 0.00104 Preprocessor3_Mode~\r## 2 5 8 accuracy multiclass 0.913 3 0.00315 Preprocessor3_Mode~\r## 3 10 5 accuracy multiclass 0.912 3 0.00114 Preprocessor2_Mode~\r## 4 5 5 accuracy multiclass 0.91 3 0.00139 Preprocessor2_Mode~\r## 5 10 2 accuracy multiclass 0.910 3 0.00175 Preprocessor1_Mode~\r# nb model\rnb_tune %\u0026gt;% show_best(\u0026quot;roc_auc\u0026quot;)\r## # A tibble: 5 x 8\r## smoothness num_comp .metric .estimator mean n std_err .config ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1.5 10 roc_auc hand_till 0.971 3 0.000400 Preprocessor3_Mod~\r## 2 1.5 6 roc_auc hand_till 0.971 3 0.000997 Preprocessor2_Mod~\r## 3 1 10 roc_auc hand_till 0.971 3 0.000634 Preprocessor3_Mod~\r## 4 1 6 roc_auc hand_till 0.970 3 0.00124 Preprocessor2_Mod~\r## 5 0.5 10 roc_auc hand_till 0.969 3 0.000808 Preprocessor3_Mod~\rnb_tune %\u0026gt;% show_best(\u0026quot;accuracy\u0026quot;)\r## # A tibble: 5 x 8\r## smoothness num_comp .metric .estimator mean n std_err .config ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 10 accuracy multiclass 0.913 3 0.000481 Preprocessor3_Mo~\r## 2 1.5 10 accuracy multiclass 0.913 3 0.000267 Preprocessor3_Mo~\r## 3 0.5 10 accuracy multiclass 0.912 3 0.000462 Preprocessor3_Mo~\r## 4 1.5 6 accuracy multiclass 0.911 3 0.00135 Preprocessor2_Mo~\r## 5 1 6 accuracy multiclass 0.910 3 0.00157 Preprocessor2_Mo~\rNext, we going to select the best model from the tuned parameters and finalise our model using last_fit().\nFor knn model.\n# Finalize\rknn_best \u0026lt;- knn_tune %\u0026gt;% select_best(\u0026quot;roc_auc\u0026quot;)\rknn_rec \u0026lt;- recipe(label ~ ., data = data_train) %\u0026gt;% step_umap(all_predictors(), outcome = vars(label), num_comp = knn_best$num_comp) %\u0026gt;% step_center(all_predictors()) %\u0026gt;% step_scale(all_predictors())\rknn_wf \u0026lt;- workflow() %\u0026gt;% add_recipe(knn_rec) %\u0026gt;% add_model(knn_mod) %\u0026gt;% finalize_workflow(knn_best) # Last fit\rknn_lastfit \u0026lt;- knn_wf %\u0026gt;% last_fit(ind)\rFor naive bayes model.\n# Finalize\rnb_best \u0026lt;- nb_tune %\u0026gt;% select_best(\u0026quot;roc_auc\u0026quot;)\rnb_rec \u0026lt;- recipe(label ~ ., data = data_train) %\u0026gt;% step_umap(all_predictors(), outcome = vars(label), num_comp = nb_best$num_comp) %\u0026gt;% step_center(all_predictors()) %\u0026gt;% step_scale(all_predictors())\rnb_wf \u0026lt;- workflow() %\u0026gt;% add_recipe(nb_rec) %\u0026gt;% add_model(nb_mod) %\u0026gt;% finalize_workflow(nb_best) # Last fit\rnb_lastfit \u0026lt;- nb_wf %\u0026gt;% last_fit(ind)\rLet‚Äôs see the model performance on the testing data.\nknn_lastfit %\u0026gt;% collect_metrics() %\u0026gt;% mutate(model = \u0026quot;knn\u0026quot;) %\u0026gt;% dplyr::bind_rows(nb_lastfit %\u0026gt;% collect_metrics() %\u0026gt;% mutate(model = \u0026quot;nb\u0026quot;)) %\u0026gt;% select(-.config)\r## # A tibble: 4 x 4\r## .metric .estimator .estimate model\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;\r## 1 accuracy multiclass 0.938 knn ## 2 roc_auc hand_till 0.971 knn ## 3 accuracy multiclass 0.936 nb ## 4 roc_auc hand_till 0.980 nb\rThese are the confusion matrices.\nknn_lastfit %\u0026gt;% collect_predictions() %\u0026gt;%\rconf_mat(label, .pred_class) %\u0026gt;% autoplot(type = \u0026quot;heatmap\u0026quot;) +\rlabs(title = \u0026quot;Confusion matrix - kNN\u0026quot;)\rnb_lastfit %\u0026gt;% collect_predictions() %\u0026gt;%\rconf_mat(label, .pred_class) %\u0026gt;% autoplot(type = \u0026quot;heatmap\u0026quot;) +\rlabs(title = \u0026quot;Confusion matrix - naive bayes\u0026quot;)\rLastly, we can compare the ROC plots for each class.\nknn_lastfit %\u0026gt;% collect_predictions() %\u0026gt;%\rmutate(id = \u0026quot;knn\u0026quot;) %\u0026gt;% bind_rows(\rnb_lastfit %\u0026gt;% collect_predictions() %\u0026gt;% mutate(id = \u0026quot;nb\u0026quot;)\r) %\u0026gt;% group_by(id) %\u0026gt;% roc_curve(label, .pred_0:.pred_9) %\u0026gt;% autoplot()\r\rConclusion\rI believe UMAP is quite good and can be used as one of preprocessing step in image classification. We are able to get a pretty good performance result in this post. I believe if the the parameter tuning approach is a bit more rigorous, the performance result will be a lot better.\n\r","date":1647388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647404230,"objectID":"75e6747bc54f431c0a98bcca22c01dc9","permalink":"https://tengkuhanis.netlify.app/post/using-umap-preprocessing-for-image-classification/","publishdate":"2022-03-16T00:00:00Z","relpermalink":"/post/using-umap-preprocessing-for-image-classification/","section":"post","summary":"UMAP\rUniform manifold approximation and projection or in short UMAP is a type of dimension reduction techniques. So, basically UMAP will project a set of features into a smaller space.","tags":["Machine Learning","tidymodels","Image analysis"],"title":"Using UMAP preprocessing for image classification","type":"post"},{"authors":null,"categories":["Machine Learning"],"content":"\r\rPrincipal component analysis (PCA)\rPCA is a dimension reduction techniques. So, if we have a large number of predictors, instead of using all the predictors for modelling or other analysis, we can compressed all the information from the variables and create a new set of variables. This new set of variables are known as components or principal component (PC). So, now we have a smaller number of variables which contain the information from the original variables.\nPCA usually used for a dataset with a large features or predictors like genomic data. Additionally, PCA is a good pre-processing option if you have a correlated variable or have a multicollinearity issue in the model. Also, we can use PCA for exploration of the data and have a better understanding of our data.\nFor those who want to study the theoretical side of PCA can further read on this link. We going to focus more on the coding part in the machine learning framework (using tidymodels package) in this post.\n\rExample in R\rThese are the packages that we going to use.\nlibrary(tidymodels)\rlibrary(tidyverse)\rlibrary(mlbench) #data\rWe going to use diabetes dataset. The outcome is binary; positive = diabetes and negative = non-diabetes/healthy. All other variables are numerical values.\ndata(\u0026quot;PimaIndiansDiabetes\u0026quot;)\rglimpse(PimaIndiansDiabetes)\r## Rows: 768\r## Columns: 9\r## $ pregnant \u0026lt;dbl\u0026gt; 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, 0, 7, 1, 1~\r## $ glucose \u0026lt;dbl\u0026gt; 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 168, 139,~\r## $ pressure \u0026lt;dbl\u0026gt; 72, 66, 64, 66, 40, 74, 50, 0, 70, 96, 92, 74, 80, 60, 72, 0,~\r## $ triceps \u0026lt;dbl\u0026gt; 35, 29, 0, 23, 35, 0, 32, 0, 45, 0, 0, 0, 0, 23, 19, 0, 47, 0~\r## $ insulin \u0026lt;dbl\u0026gt; 0, 0, 0, 94, 168, 0, 88, 0, 543, 0, 0, 0, 0, 846, 175, 0, 230~\r## $ mass \u0026lt;dbl\u0026gt; 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5, 0.0, 37~\r## $ pedigree \u0026lt;dbl\u0026gt; 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.134, 0.158~\r## $ age \u0026lt;dbl\u0026gt; 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57, 59, 51, 3~\r## $ diabetes \u0026lt;fct\u0026gt; pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, neg, pos, n~\rWe going to split the data and extract the training dataset. We going to explore only the training set since we going to do this in a machine learning framework.\nset.seed(1)\rind \u0026lt;- initial_split(PimaIndiansDiabetes)\rdat_train \u0026lt;- training(ind)\rWe create a recipe and apply normalization and PCA techniques. Then, we prep it.\n# Recipe\rpca_rec \u0026lt;- recipe(diabetes ~ ., data = dat_train) %\u0026gt;% step_normalize(all_numeric_predictors()) %\u0026gt;% step_pca(all_numeric_predictors())\r# Prep\rpca_prep \u0026lt;- prep(pca_rec)\rSo, we can extract the PCA data using tidy(). type = \"coef\" indicates that we want the loadings values. So, the values in the data are the loadings.\npca_tidied \u0026lt;- tidy(pca_prep, 2, type = \u0026quot;coef\u0026quot;)\rpca_tidied\r## # A tibble: 64 x 4\r## terms value component id ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 pregnant 0.107 PC1 pca_JtuLZ\r## 2 glucose 0.357 PC1 pca_JtuLZ\r## 3 pressure 0.330 PC1 pca_JtuLZ\r## 4 triceps 0.460 PC1 pca_JtuLZ\r## 5 insulin 0.466 PC1 pca_JtuLZ\r## 6 mass 0.447 PC1 pca_JtuLZ\r## 7 pedigree 0.315 PC1 pca_JtuLZ\r## 8 age 0.158 PC1 pca_JtuLZ\r## 9 pregnant -0.597 PC2 pca_JtuLZ\r## 10 glucose -0.192 PC2 pca_JtuLZ\r## # ... with 54 more rows\rSo, basically the loadings indicate how much each variable contributes to each component (PC). A large loading (positive or negative) indicates a strong relationship between the variables and the related components. The sign indicates a negative or positive correlation between the variables and components.\nWe can further visualise these loadings.\npca_tidied %\u0026gt;% ggplot(aes(value, terms, fill = terms)) +\rgeom_col(show.legend = F) +\rfacet_wrap(~ component) +\rylab(\u0026quot;\u0026quot;) +\rxlab(\u0026quot;Loadings\u0026quot;) + theme_minimal()\rBesides the loadings, we can also get a variance information. Variance of each component (or PC) measures how much that particular component explains the variability in the data. For example, PC1 explain 26.2% variance in the data.\npca_tidied2 \u0026lt;- tidy(pca_prep, 2, type = \u0026quot;variance\u0026quot;)\rpca_tidied2 %\u0026gt;% pivot_wider(names_from = component, values_from = value, names_prefix = \u0026quot;PC\u0026quot;) %\u0026gt;% select(-id) %\u0026gt;% mutate_if(is.numeric, round, digits = 1) %\u0026gt;% kableExtra::kable(\u0026quot;simple\u0026quot;)\r\r\rterms\rPC1\rPC2\rPC3\rPC4\rPC5\rPC6\rPC7\rPC8\r\r\r\rvariance\r2.1\r1.7\r1.0\r0.8\r0.8\r0.7\r0.5\r0.4\r\rcumulative variance\r2.1\r3.8\r4.9\r5.7\r6.5\r7.2\r7.6\r8.0\r\rpercent variance\r26.2\r21.5\r12.9\r10.6\r9.9\r8.5\r5.7\r4.7\r\rcumulative percent variance\r26.2\r47.7\r60.7\r71.2\r81.1\r89.6\r95.3\r100.0\r\r\r\rNext, we can visualise PC1 and PC2 in a scatter plot and see how each variable influences both PCs. First, we need to extract the loadings and convert into a wide format for our arrow coordinate in the scatter plot.\npca_tidied3 \u0026lt;- pca_tidied %\u0026gt;% filter(component %in% c(\u0026quot;PC1\u0026quot;, \u0026quot;PC2\u0026quot;)) %\u0026gt;% select(-id) %\u0026gt;% pivot_wider(names_from = component, values_from = value)\rpca_tidied3\r## # A tibble: 8 x 3\r## terms PC1 PC2\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 pregnant 0.107 -0.597\r## 2 glucose 0.357 -0.192\r## 3 pressure 0.330 -0.234\r## 4 triceps 0.460 0.279\r## 5 insulin 0.466 0.200\r## 6 mass 0.447 0.121\r## 7 pedigree 0.315 0.110\r## 8 age 0.158 -0.638\rNow, we can make a scatter plot using training set data (juice(pca_prep)) and the loadings data (pca_tidied3). Also, we going to add percentage of variance for PC1 and PC2 in the axis labels.\njuice(pca_prep) %\u0026gt;% ggplot(aes(PC1, PC2)) +\rgeom_point(aes(color = diabetes, shape = diabetes), size = 2, alpha = 0.6) +\rgeom_segment(data = pca_tidied3, aes(x = 0, y = 0, xend = PC1 * 5, yend = PC2 * 5), arrow = arrow(length = unit(1/2, \u0026quot;picas\u0026quot;)),\rcolor = \u0026quot;blue\u0026quot;) +\rannotate(\u0026quot;text\u0026quot;, x = pca_tidied3$PC1 * 5.2, y = pca_tidied3$PC2 * 5.2, label = pca_tidied3$terms) +\rtheme_minimal() +\rxlab(\u0026quot;PC1 (26.2%)\u0026quot;) +\rylab(\u0026quot;PC2 (21.5%)\u0026quot;) \rSo, from this scatter plot we learn that:\n\r(triceps, insulin, pedigree and mass), (glucose and pressure) and (pregnant and age) are correlated as their lines are close to each other\n\rAs PC1 and PC2 increase, triceps, insulin, pedigree and mass also increase\rAs PC2 decreases, pregnant and age increase\r\rReferences:\n\rhttp://strata.uga.edu/8370/lecturenotes/principalComponents.html\n\rhttps://juliasilge.com/blog/cocktail-recipes-umap/\r\r\r","date":1644364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644432369,"objectID":"5423e4796b692e1fd623a04d1b79f5ae","permalink":"https://tengkuhanis.netlify.app/post/explore-data-using-pca/","publishdate":"2022-02-09T00:00:00Z","relpermalink":"/post/explore-data-using-pca/","section":"post","summary":"Principal component analysis (PCA)\rPCA is a dimension reduction techniques. So, if we have a large number of predictors, instead of using all the predictors for modelling or other analysis, we can compressed all the information from the variables and create a new set of variables.","tags":["Data exploration","Machine Learning","tidymodels"],"title":"Explore data using PCA","type":"post"},{"authors":[],"categories":[],"content":"","date":1643301000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643342563,"objectID":"2a0de91e3781172a5a37b93a71638f6f","permalink":"https://tengkuhanis.netlify.app/talk/invited-lecture-university-of-cyberjaya/","publishdate":"2022-01-27T16:30:00Z","relpermalink":"/talk/invited-lecture-university-of-cyberjaya/","section":"talk","summary":"This virtual lecture session at University of Cyberjaya was organised for Research \u0026 Evidence Based Medicine III (REBMI III) for systematic review topic.","tags":[],"title":"Invited lecture session  with University of Cyberjaya","type":"talk"},{"authors":[],"categories":["applied statistics","R"],"content":"\r\rThere are two functions in R that seems almost similar yet different:\nfitted()\n\rpredict()\r\rFirst let‚Äôs prepare some data first.\n# Packages\rlibrary(dplyr)\r# Data\rset.seed(123)\rdat \u0026lt;- iris %\u0026gt;% mutate(twoGp = sample(c(\u0026quot;Gp1\u0026quot;, \u0026quot;Gp2\u0026quot;), 150, replace = T), #create two group factor\rtwoGp = as.factor(twoGp))\rThis is our data.\nsummary(dat)\r## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species twoGp ## setosa :50 Gp1:76 ## versicolor:50 Gp2:74 ## virginica :50 ## ## ## \rfitted() is used to get a predicted values or \\(\\hat{y}\\) based on the data. Let‚Äôs see this on the logistic regression.\nlogR \u0026lt;- glm(twoGp ~ ., family = binomial(), data = dat)\rThese are the fitted values.\nfitted(logR) %\u0026gt;% head()\r## 1 2 3 4 5 6 ## 0.4074988 0.3385228 0.3772767 0.3555640 0.4255196 0.4602198\rFor predict(), we have three types:\nResponse\n\rLink - default\n\rTerms\r\rIf no new data supplied to predict(), it will use the original data used to fit the model.\n1. Response\nThe type response is identical to fitted().\npredict(logR, type = \u0026quot;response\u0026quot;) %\u0026gt;% head()\r## 1 2 3 4 5 6 ## 0.4074988 0.3385228 0.3772767 0.3555640 0.4255196 0.4602198\rWe can confirm this as below.\nall.equal(fitted(logR), predict(logR, type = \u0026quot;response\u0026quot;))\r## [1] TRUE\rThus, fitted() and predict(type = \"response\") give use predicted probabilities on the scale of the response variable. The first observation of this values can be interpreted as probability of Gp2 (Gp1 is a reference group) for first observation is 0.41.\n2. Link\npredict(type = \"link\") gives us predicted probabilities on the logit scale or log odds prediction.\npredict(logR, type = \u0026quot;link\u0026quot;) %\u0026gt;% head() #similar to predict(logR)\r## 1 2 3 4 5 6 ## -0.3743150 -0.6698840 -0.5011235 -0.5946702 -0.3001551 -0.1594578\rSo, the log odds prediction of Gp2 for the first observation is -0.37. Hence, we can get the same values if we apply a link function to the fitted values.\nThe link function for logistic regression is:\n\\[\rln(\\frac{\\mu}{1 - \\mu})\r\\]\rSo, we apply this link function to the fitted values.\nlogOddsProb \u0026lt;- log(fitted(logR) / (1 - fitted(logR))) head(logOddsProb)\r## 1 2 3 4 5 6 ## -0.3743150 -0.6698840 -0.5011235 -0.5946702 -0.3001551 -0.1594578\rWe can further confirm this as we did previously.\nall.equal(logOddsProb, predict(logR, type = \u0026quot;link\u0026quot;))\r## [1] TRUE\rAlso, we can conclude predict(type = \"link\") give use a fitted values before an application of link function (log odds).\n3. Terms\nLastly, we have predict(type = \"terms\"). This type gives us a matrix of fitted values for each variable of each observation in the model on the scale of linear predictor.\npredict(logR, type = \u0026quot;terms\u0026quot;) %\u0026gt;% head() \r## Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r## 1 0.07988782 0.28070682 0.4819893 -0.2736677 -0.9178543\r## 2 0.10138230 -0.03635661 0.4819893 -0.2736677 -0.9178543\r## 3 0.12287679 0.09046877 0.5024299 -0.2736677 -0.9178543\r## 4 0.13362403 0.02705608 0.4615487 -0.2736677 -0.9178543\r## 5 0.09063506 0.34411951 0.4819893 -0.2736677 -0.9178543\r## 6 0.04764610 0.53435757 0.4206675 -0.2188976 -0.9178543\rSo, if we add up the values of the first observation and the constant (or intercept), we will get the same values as the log odds prediction (predict(type = \"link\")).\npredTerm \u0026lt;- predict(logR, type = \u0026quot;terms\u0026quot;)\rsum(predTerm[1, ], attr(predTerm, \u0026quot;constant\u0026quot;)) #add up the first observation and the constant\r## [1] -0.374315\rlogOddsProb[1]\r## 1 ## -0.374315\rThose values also similar to if we calculate manually using coefficient from the summary().\n\\[\rLogOdds(Gp2) = \\beta_0 + \\beta_1(Sepal.Length) + \\beta_2(Sepal.Width) + \\]\r\\[\r\\beta_3(Petal.Length) + \\beta_4(Petal.Width) + \\beta_5(Species)\r\\]\rSo, this is the values we get from the first observation.\ncoef(logR)[1] + coef(logR)[2]*dat$Sepal.Length[1] + coef(logR)[3]*dat$Sepal.Width[1] + coef(logR)[4]*dat$Petal.Length[1] + coef(logR)[5]*dat$Petal.Width[1] + 0 #setosa species\r## (Intercept) ## -0.374315\rHowever, in predict(type = \"terms\") the values is centered, thus we have a different values for constant/intercept and for \\(\\beta_1(Sepal.Length)\\),\\(\\beta_2(Sepal.Width)\\) and so on. For example, the values for intercept for both models are:\n# Intercept/constant from predict(type = \u0026quot;terms\u0026quot;)\rattr(predTerm, \u0026quot;constant\u0026quot;)\r## [1] -0.02537694\r# Intercept/constant from summary()\rcoef(logR)[1]\r## (Intercept) ## -1.814251\rReferences:\n\rhttps://stackoverflow.com/a/12201502/11215767\n\rhttps://stackoverflow.com/a/47854088/11215767\r\r","date":1641686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641724238,"objectID":"7942c148072e68ac5366e1999061f7bc","permalink":"https://tengkuhanis.netlify.app/post/fitted-vs-predict-in-r/","publishdate":"2022-01-09T00:00:00Z","relpermalink":"/post/fitted-vs-predict-in-r/","section":"post","summary":"There are two functions in R that seems almost similar yet different:\nfitted()\n\rpredict()\r\rFirst let‚Äôs prepare some data first.\n# Packages\rlibrary(dplyr)\r# Data\rset.seed(123)\rdat \u0026lt;- iris %\u0026gt;% mutate(twoGp = sample(c(\u0026quot;Gp1\u0026quot;, \u0026quot;Gp2\u0026quot;), 150, replace = T), #create two group factor\rtwoGp = as.","tags":["base R","comparison"],"title":"Fitted vs predict in R","type":"post"},{"authors":[],"categories":["Machine Learning","variable selection"],"content":"\r\rVariable selection\rVariable or feature selection is one of the important step whether in machine learning or statistical analysis. This post is geared more to the machine learning side. Certain machine learning models such as Support vector machine (SVM) and neural network do not handle irrelevant predictors very well, whereas models such as linear and logistic regression do not handle correlated predictors very well. Thus, careful selection of the variables will help mitigate this issue and further improve the predictive performance.\nThere are three types of approaches in variable selection:\n1. Intrinsic (or built-in feature selection)\nAn intrinsic feature selection is a feature selection embedded in the algorithm. Some examples include:\nTree-and-rule-based model - decision tree, random forest, etc\n\rMultivariate adaptive regression spline (MARS)\n\rRegularization method such as least absolute shrinkage and selection operator (LASSO or L1)\r\rAdvantages of this type of approach are they are fast and computationally efficient. However, the best variable selected in this approach is model dependent.\n2. Filter\nIn filter approach we determine the variable importance, usually separately though not necessarily. An example of this approach is univariate filter. If the outcome is two categories, we can use t-test to assess the numerical predictors. Variables with a significant p-value or a large t-statistics will be chosen.\nThis approach is very simple and fast. However, the best subset of variables selected using some filtering criteria such as statistical significance may not reflect the best predictive performance of the model. Additionally, this approach is prone to over-selection of the predictors.\n3. Wrapper\nThere two types of wrapper approaches:\nGreedy wrapper\r\rGreedy approach or algorithm direct a search path towards the best at times to achieve the best immediate benefit. Due to this reason this approach cannot escape local minima. We can assume in Figure 1 below local minima represents locally best predictors and global minima represents globally best predictors.\n\rFigure 1: Local minima and global minima\r\rAn example of this approach is recursive feature elimination or backward selection. The main weakness of this greedy approach is the selected subset of features identified by this approach may not has the best predictive performance.\nNon-greedy wrapper\r\rThe examples of this approach are simulated annealing and genetic algorithm. Both of these algorithm incorporate a randomness in their approach. Hence, it is classified as non-greedy wrapper. Due to this randomness, it can escape a local minima (see Figure 1 above).\nThe wrapper type has the best chance to find the globally best predictors. However, this approach is computationally expensive. Not to mention, this approach has a tendency to overfit (some packages like caret use resampling to avoid this issue).\n\rSuggested approach\rKuhn \u0026amp; Johnson (2019) suggested this approach:\nStart with an intrinsic approach\n\rThen, do a wrapper approach:\n\rIf a linear intrinsic approach has a better performance - proceed to wrapper method with a linear model\n\rIf non-linear intrinsic approach has a better performance - proceed to wrapper method with a non-linear model\n\r\rIf several approach select a large number of predictors, it may not feasible to reduce the number of features\n\r\rReferences:\n\rhttps://bookdown.org/max/FES/classes-of-feature-selection-methodologies.html\n\rhttp://topepo.github.io/caret/feature-selection-overview.html\r\r\r","date":1641600000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641652546,"objectID":"45988f9efc67350b299f4bfc1cc1bda4","permalink":"https://tengkuhanis.netlify.app/post/a-short-note-on-variable-selection/","publishdate":"2022-01-08T00:00:00Z","relpermalink":"/post/a-short-note-on-variable-selection/","section":"post","summary":"Variable selection\rVariable or feature selection is one of the important step whether in machine learning or statistical analysis. This post is geared more to the machine learning side.","tags":["variable selection"],"title":"A short note on variable selection","type":"post"},{"authors":[],"categories":["applied statistics","variable selection"],"content":"\r\rSome note\rI have written two post previously about multiple imputation using mice package:\nA short note on multiple imputation\n\rVariable selection for imputation model in {mice}\r\rThis post probably my last post about multiple imputation using mice package.\n\rStepwise selection\rThe general steps in mice package are:\nmice() - impute the NAs\n\rwith() - run the analysis (lm, glm, etc)\n\rpool() - pool the results\r\rFor backward and forward selection, we can do it manually after pooling the results in step 3, but we cannot do this for stepwise selection.\nBrand (1999) proposed this solution:\nPerform stepwise selection separately on each imputed dataset\n\rFit a preliminary model that contains all variables that present in at least half of the models in the step 1\n\rApply backward elimination on the variables in the preliminary model (the variable is removed one by one if p \u0026gt; 0.05)\n\rRepeat step 3 until all variables have p values \u0026lt; 0.05\r\rSo, we going to do this solution and use multivariate Wald test (D1() in mice package) for model comparison instead of pooled likelihood ratio p value.\n\rExample in R\rLoad the packages.\nlibrary(mice)\rlibrary(tidyverse)\rCreate a missing data. We going to use the famous mtcars dataset, which already available in R.\nset.seed(123)\rdat \u0026lt;- mtcars %\u0026gt;% mutate(across(c(vs, am), as.factor)) %\u0026gt;% select(-mpg) %\u0026gt;% missForest::prodNA(0.1) %\u0026gt;% bind_cols(mpg = mtcars$mpg)\rsummary(dat)\r## cyl disp hp drat ## Min. :4.000 Min. : 71.1 Min. : 52.0 Min. :2.760 ## 1st Qu.:4.000 1st Qu.:120.7 1st Qu.:103.0 1st Qu.:3.150 ## Median :6.000 Median :225.0 Median :123.0 Median :3.715 ## Mean :6.148 Mean :232.8 Mean :147.4 Mean :3.642 ## 3rd Qu.:8.000 3rd Qu.:334.0 3rd Qu.:180.0 3rd Qu.:3.920 ## Max. :8.000 Max. :472.0 Max. :335.0 Max. :4.930 ## NA\u0026#39;s :5 NA\u0026#39;s :1 NA\u0026#39;s :4 NA\u0026#39;s :2 ## wt qsec vs am gear ## Min. :1.513 Min. :14.50 0 :17 0 :18 Min. :3.00 ## 1st Qu.:2.429 1st Qu.:16.88 1 :11 1 :10 1st Qu.:3.00 ## Median :3.203 Median :17.51 NA\u0026#39;s: 4 NA\u0026#39;s: 4 Median :4.00 ## Mean :3.112 Mean :17.75 Mean :3.71 ## 3rd Qu.:3.533 3rd Qu.:18.83 3rd Qu.:4.00 ## Max. :5.424 Max. :22.90 Max. :5.00 ## NA\u0026#39;s :4 NA\u0026#39;s :2 NA\u0026#39;s :1 ## carb mpg ## Min. :1.000 Min. :10.40 ## 1st Qu.:2.000 1st Qu.:15.43 ## Median :2.000 Median :19.20 ## Mean :2.667 Mean :20.09 ## 3rd Qu.:4.000 3rd Qu.:22.80 ## Max. :6.000 Max. :33.90 ## NA\u0026#39;s :5\rRun mice() on missing data with 10 imputed datasets (m = 10).\ndatImp \u0026lt;- mice(dat, m = 10, printFlag = F, seed = 123)\rdatImp\r## Class: mids\r## Number of multiple imputations: 10 ## Imputation methods:\r## cyl disp hp drat wt qsec vs am ## \u0026quot;pmm\u0026quot; \u0026quot;pmm\u0026quot; \u0026quot;pmm\u0026quot; \u0026quot;pmm\u0026quot; \u0026quot;pmm\u0026quot; \u0026quot;pmm\u0026quot; \u0026quot;logreg\u0026quot; \u0026quot;logreg\u0026quot; ## gear carb mpg ## \u0026quot;pmm\u0026quot; \u0026quot;pmm\u0026quot; \u0026quot;\u0026quot; ## PredictorMatrix:\r## cyl disp hp drat wt qsec vs am gear carb mpg\r## cyl 0 1 1 1 1 1 1 1 1 1 1\r## disp 1 0 1 1 1 1 1 1 1 1 1\r## hp 1 1 0 1 1 1 1 1 1 1 1\r## drat 1 1 1 0 1 1 1 1 1 1 1\r## wt 1 1 1 1 0 1 1 1 1 1 1\r## qsec 1 1 1 1 1 0 1 1 1 1 1\rRun stepwise selection on each imputed dataset.\nsc \u0026lt;- list(upper = ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb, lower = ~ 1)\rexp \u0026lt;- expression(f1 \u0026lt;- lm(mpg ~ 1),\rf2 \u0026lt;- step(f1, scope = sc, trace = 0))\rfit \u0026lt;- with(datImp, exp)\rNext, we calculate how many times each variable selected in the each model by stepwise selection.\nfit$analyses %\u0026gt;% map(formula) %\u0026gt;% #get the formula\rmap(terms) %\u0026gt;% #get the terms\rmap(labels) %\u0026gt;% #get the name of variables\runlist() %\u0026gt;% table()\r## .\r## am carb cyl disp drat hp qsec vs wt ## 7 5 3 2 4 5 3 4 7\rWe going to select:\nam\n\rcarb\n\rhp\n\rwt\r\rThese variables appear at least in the half of the models. We have 10 imputed datasets, so, 10 models. Next, we fit a preliminary model.\nfit_full1 \u0026lt;- with(datImp, lm(mpg ~ am + carb + hp + wt))\rpool(fit_full1) %\u0026gt;% summary()\r## term estimate std.error statistic df p.value\r## 1 (Intercept) 33.33683070 3.30280913 10.093478 15.81838 2.688191e-08\r## 2 am1 3.06689135 1.94363342 1.577917 13.06329 1.384846e-01\r## 3 carb -0.64791214 0.65564816 -0.988201 11.64959 3.431353e-01\r## 4 hp -0.03414274 0.01159828 -2.943777 20.47239 7.895170e-03\r## 5 wt -2.39586280 1.22218829 -1.960306 13.54830 7.085513e-02\rWe exclude carb variable in the next model as it has the largest non-significant p value.\nfit_full2 \u0026lt;- with(datImp, lm(mpg ~ am + hp + wt))\rNext, we compare using multivariate Wald test.\nD1(fit_full1, fit_full2)\r## test statistic df1 df2 dfcom p.value riv\r## 1 ~~ 2 0.9765411 1 9.21378 27 0.3482934 0.6935655\rP \u0026gt; 0.05. So, we opt for the simpler model.\npool(fit_full2) %\u0026gt;% summary()\r## term estimate std.error statistic df p.value\r## 1 (Intercept) 33.75666324 3.30083213 10.226713 16.87762 1.195383e-08\r## 2 am1 2.50264907 1.79966590 1.390619 15.31418 1.842201e-01\r## 3 hp -0.03950216 0.01162689 -3.397482 17.65719 3.280147e-03\r## 4 wt -2.75412354 1.15870950 -2.376889 15.03403 3.116779e-02\rWe see that am variable has the largest non-significant p value. So, we exclude this variable in the next model and compare the two latest models using multivariate Wald test.\nfit_full3 \u0026lt;- with(datImp, lm(mpg ~ hp + wt))\rD1(fit_full2, fit_full3)\r## test statistic df1 df2 dfcom p.value riv\r## 1 ~~ 2 1.93382 1 12.90982 28 0.1878483 0.4392918\rAgain, we opt for the simple model.\npool(fit_full3) %\u0026gt;% summary()\r## term estimate std.error statistic df p.value\r## 1 (Intercept) 37.50546490 1.91102857 19.625800 23.65472 4.440892e-16\r## 2 hp -0.03263534 0.01042989 -3.129021 21.20234 5.031751e-03\r## 3 wt -3.92792051 0.75157304 -5.226266 19.78033 4.238231e-05\rThere is no non-significant variable in the model anymore. Thus, this is our final model.\ngtsummary::tbl_regression(fit_full3)\rhtml {\rfont-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r}\r#ybehlmrayy .gt_table {\rdisplay: table;\rborder-collapse: collapse;\rmargin-left: auto;\rmargin-right: auto;\rcolor: #333333;\rfont-size: 16px;\rfont-weight: normal;\rfont-style: normal;\rbackground-color: #FFFFFF;\rwidth: auto;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #A8A8A8;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #A8A8A8;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\r}\r#ybehlmrayy .gt_heading {\rbackground-color: #FFFFFF;\rtext-align: center;\rborder-bottom-color: #FFFFFF;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#ybehlmrayy .gt_title {\rcolor: #333333;\rfont-size: 125%;\rfont-weight: initial;\rpadding-top: 4px;\rpadding-bottom: 4px;\rborder-bottom-color: #FFFFFF;\rborder-bottom-width: 0;\r}\r#ybehlmrayy .gt_subtitle {\rcolor: #333333;\rfont-size: 85%;\rfont-weight: initial;\rpadding-top: 0;\rpadding-bottom: 6px;\rborder-top-color: #FFFFFF;\rborder-top-width: 0;\r}\r#ybehlmrayy .gt_bottom_border {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#ybehlmrayy .gt_col_headings {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#ybehlmrayy .gt_col_heading {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\rpadding-left: 5px;\rpadding-right: 5px;\roverflow-x: hidden;\r}\r#ybehlmrayy .gt_column_spanner_outer {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rpadding-top: 0;\rpadding-bottom: 0;\rpadding-left: 4px;\rpadding-right: 4px;\r}\r#ybehlmrayy .gt_column_spanner_outer:first-child {\rpadding-left: 0;\r}\r#ybehlmrayy .gt_column_spanner_outer:last-child {\rpadding-right: 0;\r}\r#ybehlmrayy .gt_column_spanner {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 5px;\roverflow-x: hidden;\rdisplay: inline-block;\rwidth: 100%;\r}\r#ybehlmrayy .gt_group_heading {\rpadding: 8px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\r}\r#ybehlmrayy .gt_empty_group_heading {\rpadding: 0.5px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: middle;\r}\r#ybehlmrayy .gt_from_md  :first-child {\rmargin-top: 0;\r}\r#ybehlmrayy .gt_from_md  :last-child {\rmargin-bottom: 0;\r}\r#ybehlmrayy .gt_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rmargin: 10px;\rborder-top-style: solid;\rborder-top-width: 1px;\rborder-top-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\roverflow-x: hidden;\r}\r#ybehlmrayy .gt_stub {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-right-style: solid;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rpadding-left: 12px;\r}\r#ybehlmrayy .gt_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#ybehlmrayy .gt_first_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\r}\r#ybehlmrayy .gt_grand_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#ybehlmrayy .gt_first_grand_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: double;\rborder-top-width: 6px;\rborder-top-color: #D3D3D3;\r}\r#ybehlmrayy .gt_striped {\rbackground-color: rgba(128, 128, 128, 0.05);\r}\r#ybehlmrayy .gt_table_body {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#ybehlmrayy .gt_footnotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#ybehlmrayy .gt_footnote {\rmargin: 0px;\rfont-size: 90%;\rpadding: 4px;\r}\r#ybehlmrayy .gt_sourcenotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#ybehlmrayy .gt_sourcenote {\rfont-size: 90%;\rpadding: 4px;\r}\r#ybehlmrayy .gt_left {\rtext-align: left;\r}\r#ybehlmrayy .gt_center {\rtext-align: center;\r}\r#ybehlmrayy .gt_right {\rtext-align: right;\rfont-variant-numeric: tabular-nums;\r}\r#ybehlmrayy .gt_font_normal {\rfont-weight: normal;\r}\r#ybehlmrayy .gt_font_bold {\rfont-weight: bold;\r}\r#ybehlmrayy .gt_font_italic {\rfont-style: italic;\r}\r#ybehlmrayy .gt_super {\rfont-size: 65%;\r}\r#ybehlmrayy .gt_footnote_marks {\rfont-style: italic;\rfont-weight: normal;\rfont-size: 65%;\r}\r\r\rCharacteristic\rBeta\r95% CI1\rp-value\r\r\rhp\r-0.03\r-0.05, -0.01\r0.005\rwt\r-3.9\r-5.5, -2.4\r\r\r\r1\r\rCI = Confidence Interval\r\r\r\r\r\r\nReference:\nhttps://stefvanbuuren.name/fimd/sec-stepwise.html\n\r","date":1641254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641275636,"objectID":"54a20d7ce99966ee577032ad10ed9407","permalink":"https://tengkuhanis.netlify.app/post/stepwise-selection-after-multiple-imputation/","publishdate":"2022-01-04T00:00:00Z","relpermalink":"/post/stepwise-selection-after-multiple-imputation/","section":"post","summary":"Some note\rI have written two post previously about multiple imputation using mice package:\nA short note on multiple imputation\n\rVariable selection for imputation model in {mice}\r\rThis post probably my last post about multiple imputation using mice package.","tags":["missing data","variable selection"],"title":"Stepwise selection after multiple imputation","type":"post"},{"authors":[],"categories":["Machine Learning","applied statistics","variable selection"],"content":"\r\rBackground\rGenetic algorithm is inspired by a natural selection process by which the fittest individuals be selected to reproduce. This algorithm has been used in optimization and search problem, and also, can be used for variable selection.\nGenetic algorithm - gene, chromosome, population, crossover (upper right), offspring (lower right)\n\rFirst, let‚Äôs go into a few terms related to genetic algorithm theory.\nPopulation - a set of chromosomes\n\rChromosome - a subset of variables (also known as individual by some reference)\n\rGene - a variable or feature\n\rFitness function - give fitness score to each chromosome and guide the selection\n\rSelection - a process to select the two chromosome known as parents\n\rCrossover - a process to generate offspring by parents (illustrate in the picture above, on the upper right side)\n\rMutation - the process by which the gene in the chromosome is randomly flipped into 1 or 0\r\rMutation\n\rSo, the basic flow of genetic algorithm:\nAlgorithm starts with an initial population, often randomly generated\n\rCreate a successive generation by selecting a portion of the initial population (the selection is guided by the fitness function) - this includes selection -\u0026gt; crossover -\u0026gt; mutation\n\rThe algorithm terminates if certain predetermined criteria are met such as:\n\rSolution satisfies the minimum criteria\n\rFixed number of generation reached\n\rSuccessive iteration no longer produce a better result\r\r\r\rExample in R\rThere is GA package in R, where we can implement the genetic algorithm a bit more manually where we can specify our own fitness function. However, I think it is easier to use a genetic algorithm implemented in caret package for variable selection.\nLoad the packages.\nlibrary(caret)\rlibrary(tidyverse)\rlibrary(rsample)\rlibrary(recipes)\rThe data.\ndat \u0026lt;- mtcars %\u0026gt;% mutate(across(c(vs, am), as.factor),\ram = fct_recode(am, auto = \u0026quot;0\u0026quot;, man = \u0026quot;1\u0026quot;))\rstr(dat)\r## \u0026#39;data.frame\u0026#39;: 32 obs. of 11 variables:\r## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\r## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ...\r## $ disp: num 160 160 108 258 360 ...\r## $ hp : num 110 110 93 110 175 105 245 62 95 123 ...\r## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\r## $ wt : num 2.62 2.88 2.32 3.21 3.44 ...\r## $ qsec: num 16.5 17 18.6 19.4 17 ...\r## $ vs : Factor w/ 2 levels \u0026quot;0\u0026quot;,\u0026quot;1\u0026quot;: 1 1 2 2 1 2 1 2 2 2 ...\r## $ am : Factor w/ 2 levels \u0026quot;auto\u0026quot;,\u0026quot;man\u0026quot;: 2 2 2 1 1 1 1 1 1 1 ...\r## $ gear: num 4 4 4 3 3 3 3 4 4 4 ...\r## $ carb: num 4 4 1 1 2 1 4 2 2 4 ...\rFor this, we going to use random forest (rfGA). Other options are bagged tree (treebagGA) and caretGA. We are able to use other method in caret if we use caretGA.\n# specify control\rga_ctrl \u0026lt;- gafsControl(functions = rfGA, method = \u0026quot;cv\u0026quot;, number = 5)\r# run random forest\rset.seed(123)\rrf_ga \u0026lt;- gafs(x = dat %\u0026gt;% select(-am), y = dat$am,\riters = 5,\rgafsControl = ga_ctrl)\rrf_ga\r## ## Genetic Algorithm Feature Selection\r## ## 32 samples\r## 10 predictors\r## 2 classes: \u0026#39;auto\u0026#39;, \u0026#39;man\u0026#39; ## ## Maximum generations: 5 ## Population per generation: 50 ## Crossover probability: 0.8 ## Mutation probability: 0.1 ## Elitism: 0 ## ## Internal performance values: Accuracy, Kappa\r## Subset selection driven to maximize internal Accuracy ## ## External performance values: Accuracy, Kappa\r## Best iteration chose by maximizing external Accuracy ## External resampling method: Cross-Validated (5 fold) ## ## During resampling:\r## * the top 5 selected variables (out of a possible 10):\r## qsec (60%), wt (60%), disp (40%), gear (40%), vs (40%)\r## * on average, 3.2 variables were selected (min = 1, max = 7)\r## ## In the final search using the entire training set:\r## * 7 features selected at iteration 3 including:\r## cyl, hp, drat, qsec, vs ... ## * external performance at this iteration is\r## ## Accuracy Kappa ## 0.9429 0.8831\rThe optimal features/variables:\nrf_ga$optVariables\r## [1] \u0026quot;cyl\u0026quot; \u0026quot;hp\u0026quot; \u0026quot;drat\u0026quot; \u0026quot;qsec\u0026quot; \u0026quot;vs\u0026quot; \u0026quot;gear\u0026quot; \u0026quot;carb\u0026quot;\rThis is the time taken for random forest approach.\nrf_ga$times\r## $everything\r## user system elapsed ## 51.22 1.25 52.92\rBy default the algorithm will find a solution or a set of variable that reduce RMSE for numerical outcome, and accuracy for categorical outcome. Also, genetic algorithm tend to overfit, that‚Äôs why for the implementation in caret we have internal and external performance. So, for the 10-fold cross-validation, 10 genetic algorithm will be run separately. All the first nine folds are used for the genetic algorithm, and the 10th for external performance evaluation.\nLet‚Äôs try a variable selection using linear regression model.\n# specify control\rlm_ga_ctrl \u0026lt;- gafsControl(functions = caretGA, method = \u0026quot;cv\u0026quot;, number = 5)\r# run lm\rset.seed(123)\rlm_ga \u0026lt;- gafs(x = dat %\u0026gt;% select(-mpg), y = dat$mpg,\riters = 5,\rgafsControl = lm_ga_ctrl,\r# below is the option for `train`\rmethod = \u0026quot;lm\u0026quot;,\rtrControl = trainControl(method = \u0026quot;cv\u0026quot;, allowParallel = F))\rlm_ga\r## ## Genetic Algorithm Feature Selection\r## ## 32 samples\r## 10 predictors\r## ## Maximum generations: 5 ## Population per generation: 50 ## Crossover probability: 0.8 ## Mutation probability: 0.1 ## Elitism: 0 ## ## Internal performance values: RMSE, Rsquared, MAE\r## Subset selection driven to minimize internal RMSE ## ## External performance values: RMSE, Rsquared, MAE\r## Best iteration chose by minimizing external RMSE ## External resampling method: Cross-Validated (5 fold) ## ## During resampling:\r## * the top 5 selected variables (out of a possible 10):\r## wt (100%), hp (80%), carb (60%), cyl (60%), am (40%)\r## * on average, 4.4 variables were selected (min = 4, max = 5)\r## ## In the final search using the entire training set:\r## * 5 features selected at iteration 5 including:\r## cyl, disp, hp, wt, qsec ## * external performance at this iteration is\r## ## RMSE Rsquared MAE ## 3.3434 0.7624 2.6037\rNow, let‚Äôs see how to integrate this in machine learning flow using recipes from rsample.\nFirst, we split the data.\nset.seed(123)\rdat_split \u0026lt;-initial_split(dat)\rdat_train \u0026lt;- training(dat_split)\rdat_test \u0026lt;- testing(dat_split)\rWe specify two recipes for numerical and categorical outcome.\n# Numerical\rrec_num \u0026lt;- recipe(mpg ~., data = dat_train) %\u0026gt;% step_center(all_numeric()) %\u0026gt;% step_dummy(all_nominal_predictors())\r# Categorical\rrec_cat \u0026lt;- recipe(am ~., data = dat_train) %\u0026gt;% step_center(all_numeric()) %\u0026gt;% step_dummy(all_nominal_predictors())\rWe run random forest for numerical outcome recipes.\n# specify control\rrf_ga_ctrl \u0026lt;- gafsControl(functions = rfGA, method = \u0026quot;cv\u0026quot;, number = 5)\r# run random forest\rset.seed(123)\rrf_ga2 \u0026lt;- gafs(rec_num,\rdata = dat_train,\riters = 5, gafsControl = rf_ga_ctrl) rf_ga2\r## ## Genetic Algorithm Feature Selection\r## ## 24 samples\r## 10 predictors\r## ## Maximum generations: 5 ## Population per generation: 50 ## Crossover probability: 0.8 ## Mutation probability: 0.1 ## Elitism: 0 ## ## Internal performance values: RMSE, Rsquared\r## Subset selection driven to minimize internal RMSE ## ## External performance values: RMSE, Rsquared, MAE\r## Best iteration chose by minimizing external RMSE ## External resampling method: Cross-Validated (5 fold) ## ## During resampling:\r## * the top 5 selected variables (out of a possible 10):\r## cyl (80%), disp (80%), hp (80%), wt (80%), carb (60%)\r## * on average, 4.8 variables were selected (min = 2, max = 9)\r## ## In the final search using the entire training set:\r## * 6 features selected at iteration 5 including:\r## cyl, disp, hp, wt, gear ... ## * external performance at this iteration is\r## ## RMSE Rsquared MAE ## 2.830 0.928 2.408\rThe optimal variables.\nrf_ga2$optVariables\r## [1] \u0026quot;cyl\u0026quot; \u0026quot;disp\u0026quot; \u0026quot;hp\u0026quot; \u0026quot;wt\u0026quot; \u0026quot;gear\u0026quot; \u0026quot;vs_X1\u0026quot;\rLet‚Äôs try run SVM for the numerical outcome recipes.\n# specify control\rsvm_ga_ctrl \u0026lt;- gafsControl(functions = caretGA, method = \u0026quot;cv\u0026quot;, number = 5)\r# run SVM\rset.seed(123)\rsvm_ga \u0026lt;- gafs(rec_cat,\rdata = dat_train,\riters = 5, gafsControl = svm_ga_ctrl,\r# below is the options to `train` for caretGA\rmethod = \u0026quot;svmRadial\u0026quot;, #SVM with Radial Basis Function Kernel\rtrControl = trainControl(method = \u0026quot;cv\u0026quot;, allowParallel = T))\rsvm_ga\r## ## Genetic Algorithm Feature Selection\r## ## 24 samples\r## 10 predictors\r## 2 classes: \u0026#39;auto\u0026#39;, \u0026#39;man\u0026#39; ## ## Maximum generations: 5 ## Population per generation: 50 ## Crossover probability: 0.8 ## Mutation probability: 0.1 ## Elitism: 0 ## ## Internal performance values: Accuracy, Kappa\r## Subset selection driven to maximize internal Accuracy ## ## External performance values: Accuracy, Kappa\r## Best iteration chose by maximizing external Accuracy ## External resampling method: Cross-Validated (5 fold) ## ## During resampling:\r## * the top 5 selected variables (out of a possible 10):\r## wt (80%), qsec (60%), vs_X1 (60%), carb (40%), disp (40%)\r## * on average, 4 variables were selected (min = 3, max = 6)\r## ## In the final search using the entire training set:\r## * 9 features selected at iteration 2 including:\r## mpg, cyl, disp, hp, drat ... ## * external performance at this iteration is\r## ## Accuracy Kappa ## 0.9200 0.8571\rThe optimal variables.\nsvm_ga$optVariables\r## [1] \u0026quot;mpg\u0026quot; \u0026quot;cyl\u0026quot; \u0026quot;disp\u0026quot; \u0026quot;hp\u0026quot; \u0026quot;drat\u0026quot; \u0026quot;wt\u0026quot; \u0026quot;qsec\u0026quot; \u0026quot;carb\u0026quot; \u0026quot;vs_X1\u0026quot;\r\rConclusion\rAlthough genetic algorithm seems quite good for variable selection, the main limitation I would say is the computational time. However, if we have a lot of variables or features to reduced, using the genetic algorithm despite the long computational time seems beneficial to me.\nReference:\n\rhttps://topepo.github.io/caret/feature-selection-using-genetic-algorithms.html#ga\n\rhttps://towardsdatascience.com/introduction-to-genetic-algorithms-including-example-code-e396e98d8bf3\n\rhttps://towardsdatascience.com/feature-selection-using-genetic-algorithms-in-r-3d9252f1aa66\r\r\r","date":1641081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641085783,"objectID":"79c13593460838a6cab50b8e98b5ccf6","permalink":"https://tengkuhanis.netlify.app/post/variable-selection-using-genetic-algorithm/","publishdate":"2022-01-02T00:00:00Z","relpermalink":"/post/variable-selection-using-genetic-algorithm/","section":"post","summary":"Background\rGenetic algorithm is inspired by a natural selection process by which the fittest individuals be selected to reproduce. This algorithm has been used in optimization and search problem, and also, can be used for variable selection.","tags":["Machine Learning","variable selection"],"title":"Variable selection using genetic algorithm","type":"post"},{"authors":["Tengku Muhammad Hanis","Wan Nor Arifin","Juhara Haron","Wan Faiziah Wan Abdul Rahman","Nur Intan Raihana Ruhaiyem","Rosni Abdullah","Kamarul Imran Musa"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"8dbb004521f899f4a806ff008dbda4b8","permalink":"https://tengkuhanis.netlify.app/publication/2022-01-01_factors_influencing_/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/2022-01-01_factors_influencing_/","section":"publication","summary":"","tags":[],"title":"Factors Influencing Mammographic Density in Asian Women: A Retrospective Cohort Study in the Northeast Region of Peninsular Malaysia","type":"publication"},{"authors":["Wan Shakira Rodzlan Hasani","Nor Asiah Muhamad","Hasnah Maamor","Tengku Muhammad Hanis","Chen Xin Wee","Kamarul Imran Musa"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"3a324465fe2a512311164d3963d311f7","permalink":"https://tengkuhanis.netlify.app/publication/2022-01-01_premature_cardiovasc/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/2022-01-01_premature_cardiovasc/","section":"publication","summary":"","tags":[],"title":"Premature Cardiovascular Death and its Modifiable Risk Factors: Protocol of a Systematic Review and Meta-Analysis","type":"publication"},{"authors":[],"categories":["R","Map"],"content":"\r\r\r\r\rI have tried creating a map with ggplot2 previously. In this post, I will try to create an interactive map using leaflet package in R.\nThese are the required packages.\nlibrary(tidyverse)\rlibrary(tidygeocoder)\rlibrary(leaflet)\rlibrary(htmltools)\rSo, I‚Äôm going to use a clinics location data in Malaysia. I already uploaded this data tomy GitHub repo. I will skip the explanation for the pre-processing part, but it is the same pre-processing as my previous post.\n# Read the data\rclinic1m \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/tengku-hanis/clinic-data/main/clinic1m.csv\u0026quot;)\rclinicDesa \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/tengku-hanis/clinic-data/main/clinicdesa.csv\u0026quot;)\r\r\rShow code for pre-processing\r\r# Get the missing coordinate based on postal codes\rclinic1m2 \u0026lt;- clinic1m %\u0026gt;%\rmutate(country = \u0026quot;malaysia\u0026quot;) %\u0026gt;% select(name, postcode, country) %\u0026gt;% mutate(postcode = ifelse(nchar(postcode) == 4, paste0(0, postcode), postcode)) %\u0026gt;%\rgeocode(postalcode = postcode, country = country, method = \u0026quot;osm\u0026quot;)\r# Add coordinate from external sources for the still missing coordinates\radd_coord \u0026lt;- read.table(header = T, text = \u0026quot;\rpostal_code latitude longitude\r16070 6.0334 102.3499\r26060 3.6228 102.3926\r90700 5.8456 118.0571\r26060 3.6228 102.3926\u0026quot;)\r# Drop clinics with the still missing coordinate\rclinic1m2 \u0026lt;- clinic1m2 %\u0026gt;% mutate(lat = ifelse(postcode %in% add_coord$postal_code, add_coord$latitude, lat), long = ifelse(postcode %in% add_coord$postal_code, add_coord$longitude, long)) %\u0026gt;% drop_na() #drop 2 clinic1m\r# Bind the 2 data\rall_clinic \u0026lt;- clinic1m2 %\u0026gt;% mutate(Type = \u0026quot;1Malaysia\u0026quot;) %\u0026gt;% select(name, Type, lat, long) %\u0026gt;% bind_rows(clinicDesa %\u0026gt;% mutate(Type = \u0026quot;Desa\u0026quot;, lat = latitude, long = longitude) %\u0026gt;% select(name, Type, lat, long)) %\u0026gt;% mutate(name = str_to_title(name))\r\rFirst, we going to plot the coordinates to see if there is anything strange.\nggplot(all_clinic, aes(long, lat, color = Type)) +\rgeom_point() +\rtheme_minimal()\rSo, we are going to remove the two isolated points as seen from the plot.\nall_clinic2 \u0026lt;- all_clinic %\u0026gt;% filter(long \u0026gt; 25)\rOnce we have our data ready, we can supply to leaflet. We can choose the type of map from addProviderTiles(). Some need an api, but the one we choose here does not. We supply the longitude and latitude of our data to addCircleMarkers(), and clusterOptions to cluster our data.\nleaflet(all_clinic2) %\u0026gt;% addProviderTiles(providers$Stamen.Watercolor) %\u0026gt;%\raddProviderTiles(providers$Stamen.TerrainLabels) %\u0026gt;%\raddCircleMarkers(~long, ~lat, clusterOptions = markerClusterOptions())\r\r{\"x\":{\"url\":\"index.en_files/figure-html//widgets/widget_unnamed-chunk-7.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]}\rNext, we can add a label.\nlabels \u0026lt;- sprintf(\u0026quot;\u0026lt;strong\u0026gt;%s\u0026lt;/strong\u0026gt;\u0026quot;, all_clinic$name) %\u0026gt;% lapply(htmltools::HTML)\rAlso, we can add a mini map to our map. Here, I change the type of map to a more appropriate one.\nleaflet(all_clinic2) %\u0026gt;% addProviderTiles(providers$OpenStreetMap) %\u0026gt;%\raddCircleMarkers(~long, ~lat, popup = ~labels, # popup add the label\rclusterOptions = markerClusterOptions()) %\u0026gt;% # add a mini map\raddMiniMap(tiles = providers$OpenStreetMap, zoomLevelOffset = -3)\r\r{\"x\":{\"url\":\"index.en_files/figure-html//widgets/widget_unnamed-chunk-10.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]}\rNotice that the coordinates look more accurate as compared to the map I created with ggplot2 previously.\nReferences:\n\rhttps://lauriebaker.rbind.io/post/where_work/\rhttps://laurielbaker.github.io/DSCA_leaflet_mapping_in_r/\r\r","date":1638057600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638064619,"objectID":"1ff01340ab05d07aadb58b58d5219214","permalink":"https://tengkuhanis.netlify.app/post/my-first-interactive-map-with-leaflet/","publishdate":"2021-11-28T00:00:00Z","relpermalink":"/post/my-first-interactive-map-with-leaflet/","section":"post","summary":"I have tried creating a map with ggplot2 previously. In this post, I will try to create an interactive map using leaflet package in R.","tags":["spatial","map"],"title":"My first interactive map with {leaflet}","type":"post"},{"authors":[],"categories":["R","applied statistics"],"content":"\r\rSome note\rI have written a short post about missing data and multiple imputation in mice package previously. This post will add to that previous post.\n\rImputation model\rImputation model is the model that we use for our imputation approach. There is another term which is complete-data model. This is a model that we want to fit after we impute the missing values (i.e; the complete-data model is the final model).\nGenerally, we need to include as many relevant variables into the imputation model. However, this general advise may not be very efficient as we may have multicollinearity and computational issue if we include too many predictors. As a rule of thumb, the number of included variables should be no more than 15-20. van Buuren et al. (2011) mentioned that increased in explained variance in linear regression is negligible after 15 variables are included.\nThere are 4 steps suggested by van Buuren et al. (1999) for variable selection in the case of big data:\nInclude all variables that appear in the complete-data model (final model)\n\rThis may include the interaction terms as well (passive imputation can be used to specify the interaction terms in mice package)\r\rInclude variable that have influence on the occurrence of the missing data\n\rThis can be assessed by a correlation matrix between NAs variables and non-NAs variables\r\rInclude variable that explain a considerable amount of variance\n\rThis can be crudely assessed by a correlation matrix between NAs variables and non-NAs variables\r\rRemove variable that have too many missing values within the subgroup of incomplete cases\n\rThis can be assessed by a proportion of usable cases (PUC) - how many cases with missing data in a certain variable have an observed values on the predictor variables\r\r\rAll these steps should be done on the key variables only. There is another more efficient yet laborious approach suggested by Oudshoorn et al. (1999), which take into account important predictor of predictors. We are going to focus on the four steps above, and not cover the latter suggested approach in this post.\n\rR codes\rThese are the required packages.\nlibrary(mice)\rlibrary(corrplot)\rOur data.\nsummary(airquality)\r## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA\u0026#39;s :37 NA\u0026#39;s :7 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 ## \rWe have 2 variables; Ozone and Solar.R with missing values or NAs. We can further explore the pattern of missing variable.\nmd.pattern(airquality)\r## Wind Temp Month Day Solar.R Ozone ## 111 1 1 1 1 1 1 0\r## 35 1 1 1 1 1 0 1\r## 5 1 1 1 1 0 1 1\r## 2 1 1 1 1 0 0 2\r## 0 0 0 0 7 37 44\rThere are 2 rows with NAs in Ozone and Solar.R, 35 rows with NAs only in Ozone, and 5 rows with NAs only in Solar.R. Next, we can check the correlation.\ncor(airquality, use = \u0026quot;pairwise.complete.obs\u0026quot;) |\u0026gt;\rcorrplot(method = \u0026quot;number\u0026quot;, type = \u0026quot;upper\u0026quot;)\rThe correlations of Ozone-Temp and Ozone-Wind are the highest. Now, let‚Äôs do a correlation between the NAs variable and non-NAs variable.\ncor(y = airquality, x = !is.na(airquality), use = \u0026quot;pairwise.complete.obs\u0026quot;) |\u0026gt;\rround(digits = 2)\r## Ozone Solar.R Wind Temp Month Day\r## Ozone NA -0.02 -0.05 0.00 0.26 -0.05\r## Solar.R 0 NA 0.06 0.11 0.11 0.17\r## Wind NA NA NA NA NA NA\r## Temp NA NA NA NA NA NA\r## Month NA NA NA NA NA NA\r## Day NA NA NA NA NA NA\rWe can ignore the warnings and the NAs as only Ozone and Solar.R have a missing values. So, the highest correlation is 0.26 between Month-Ozone - correlation between Month values with Ozone-related NAs and Month values with non-Ozone-related NAs. The column variable in the correlation matrix is the indicators of NAs and the row variables is the variable with observed values.\nLastly we can calculate ‚Äòmanually‚Äô the PUC (proportion of usable cases). md.pairs() here calculate the number of observation per variable pair.\nvar_pair \u0026lt;- md.pairs(airquality)\rround(var_pair$mr / (var_pair$mr + var_pair$mm), digits = 3)\r## Ozone Solar.R Wind Temp Month Day\r## Ozone 0.000 0.946 1 1 1 1\r## Solar.R 0.714 0.000 1 1 1 1\r## Wind NaN NaN NaN NaN NaN NaN\r## Temp NaN NaN NaN NaN NaN NaN\r## Month NaN NaN NaN NaN NaN NaN\r## Day NaN NaN NaN NaN NaN NaN\rLow value of PUC indicate there is a little information on the predictor to impute the target NAs variable. NaN is shown as the variables have no missing values. The row variable are the target variables to be imputed, and the column variables are the predictors in imputation model. We can see that to impute Solar.R (on the row) Ozone has a little less information (0.714) compare to Wind, Temp, and Day. The diagonal elements will always be 0 or NaN. So, from here we can drop predictors with say, 0 PUC as they contain no information to help impute the target NAs variable.\nActually, we have a nice function from mice that can do what we ‚Äòmanually‚Äô did just now.\nquickpred(airquality)\r## Ozone Solar.R Wind Temp Month Day\r## Ozone 0 1 1 1 1 0\r## Solar.R 1 0 0 1 1 1\r## Wind 0 0 0 0 0 0\r## Temp 0 0 0 0 0 0\r## Month 0 0 0 0 0 0\r## Day 0 0 0 0 0 0\rAgain, the column variables are the predictors, and the row variables are the target NAs variables. The above matrix is known as predictor matrix, which going to be used in the imputation model. 1 denote a variable included as predictors and 0 vice versa. The two main arguments in quickpred() are:\n\rmincor - if any of the absolute values in the two correlation matrix that we did earlier above 0.1 (default), the predictors will be included in the predictor matrix\n\rminpuc - the default values for PUC is 0, so the predictors are retained even if they have no information to help imputation model\n\r\rNotice that, variable Day is excluded from the predictors of Ozone. The correlation values are 0 and -0.05 from the first and second correlation matrices, respectively which do not exceed the default setting of 0.1. That‚Äôs why, variable Day is excluded. Also, we can observe a similar situation for variable Wind , which is excluded from the predictors of Solar.R (the correlation coefficients are -0.60 and 0.06). The negative (-) sign does not matter as we actually evaluate the absolute values.\nIntuitively, we can change these two arguments as we see fit to do a variable selection for imputation model. Once we finalise our variable selection, we can do the multiple imputation using mice().\n# Finalised variable selection\rvar_sel \u0026lt;- quickpred(airquality)\rvar_sel\r## Ozone Solar.R Wind Temp Month Day\r## Ozone 0 1 1 1 1 0\r## Solar.R 1 0 0 1 1 1\r## Wind 0 0 0 0 0 0\r## Temp 0 0 0 0 0 0\r## Month 0 0 0 0 0 0\r## Day 0 0 0 0 0 0\r# Impute\rimp \u0026lt;- mice(airquality, m = 5, predictorMatrix = var_sel, printFlag = F)\rimp\r## Class: mids\r## Number of multiple imputations: 5 ## Imputation methods:\r## Ozone Solar.R Wind Temp Month Day ## \u0026quot;pmm\u0026quot; \u0026quot;pmm\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ## PredictorMatrix:\r## Ozone Solar.R Wind Temp Month Day\r## Ozone 0 1 1 1 1 0\r## Solar.R 1 0 0 1 1 1\r## Wind 0 0 0 0 0 0\r## Temp 0 0 0 0 0 0\r## Month 0 0 0 0 0 0\r## Day 0 0 0 0 0 0\rNotice that mice() uses the predictor matrix that we provide.\nReferences:\nhttps://www.jstatsoft.org/article/view/v045i03 - paper written by Staf van Buuren (a bit outdated in terms of codes, but runnable)\n\rhttps://stefvanbuuren.name/fimd/ - online book written by Stef van Buuren (See chapter 6.3.2 and 9.1.6)\n\r\r\r","date":1637539200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637538940,"objectID":"4394c21125b5159de0f4ff26c7e9aeb7","permalink":"https://tengkuhanis.netlify.app/post/variable-selection-for-imputation-model-in-mice/","publishdate":"2021-11-22T00:00:00Z","relpermalink":"/post/variable-selection-for-imputation-model-in-mice/","section":"post","summary":"Some note\rI have written a short post about missing data and multiple imputation in mice package previously. This post will add to that previous post.\n\rImputation model\rImputation model is the model that we use for our imputation approach.","tags":["missing data"],"title":"Variable selection for imputation model in {mice}","type":"post"},{"authors":[],"categories":["R","Epidemiology","Map"],"content":"\r\rAs written in the title of the post, this is my first try ever in making a map with R. I found a great data on the distribution of the clinics in Malaysia. The two types of clinic that we have here:\nKlinik 1Malaysia (1Malaysia clinic)\rKlinik Desa (Desa clinic)\r\rOriginally, these two data are a separated data. Both of the data can be downloaded from here. Also, I have uploaded the data into my GitHub repo for those interested. Klinik Desa data have a latitude and longitude information, but Klinik 1Malaysia data does not.\nThese are the required packages.\nlibrary(rworldmap) #to get a Malaysia map\rlibrary(tidyverse)\rlibrary(tidygeocoder) #to get latitude and logitude\rRead the data.\nclinic1m \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/tengku-hanis/clinic-data/main/clinic1m.csv\u0026quot;)\rclinicDesa \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/tengku-hanis/clinic-data/main/clinicdesa.csv\u0026quot;)\rFirst, we need to get a latitude and longitude information for Klinik 1Malaysia data. So, we going to retrieve the coordinates based on the postal code, though this is not very accurate. We can use tidygeocoder for this.\nclinic1m2 \u0026lt;- clinic1m %\u0026gt;%\rmutate(country = \u0026quot;malaysia\u0026quot;) %\u0026gt;% select(name, postcode, country) %\u0026gt;% mutate(postcode = ifelse(nchar(postcode) == 4, paste0(0, postcode), postcode)) %\u0026gt;%\rgeocode(postalcode = postcode, country = country, method = \u0026quot;osm\u0026quot;)\rFurther checking on the data, we notice that 5 clinics have no coordinate info.\nclinic1m2 %\u0026gt;% filter(is.na(lat) | is.na(long))\r## # A tibble: 5 x 5\r## name postcode country lat long\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Klinik 1 Malaysia Bandar Lela 90700 malaysia NA NA\r## 2 Klinik 1 Malaysia Batu Melintang 17250 malaysia NA NA\r## 3 Klinik 1 Malaysia Cakerapurnama 45010 malaysia NA NA\r## 4 Klinik 1 Malaysia Jelawat 16070 malaysia NA NA\r## 5 Klinik 1 Malaysia Taman Kempadang Makmur 26060 malaysia NA NA\rSome data pre-processing\rSo, I found this data after some googling time, which give coordinate based on the postal code. So, we going to add in the missing coordinate based on this online data.\nadd_coord \u0026lt;- read.table(header = T, text = \u0026quot;\rpostal_code latitude longitude\r16070 6.0334 102.3499\r26060 3.6228 102.3926\r90700 5.8456 118.0571\r26060 3.6228 102.3926\u0026quot;)\rclinic1m2 \u0026lt;- clinic1m2 %\u0026gt;% mutate(lat = ifelse(postcode %in% add_coord$postal_code, add_coord$latitude, lat), long = ifelse(postcode %in% add_coord$postal_code, add_coord$longitude, long)) %\u0026gt;% drop_na() #drop 2 clinic1m\rEven after add in the missing coordinate, we still missing 2 coordinates. So, we going to drop those 2 clinics. Next, we combine both data.\nall_clinic \u0026lt;- clinic1m2 %\u0026gt;% mutate(Type = \u0026quot;1Malaysia\u0026quot;) %\u0026gt;% select(Type, lat, long) %\u0026gt;% bind_rows(clinicDesa %\u0026gt;% mutate(Type = \u0026quot;Desa\u0026quot;, lat = latitude, long = longitude) %\u0026gt;% select(Type, lat, long))\rLet‚Äôs try plotting the data first.\nggplot(all_clinic, aes(long, lat, color = Type)) +\rgeom_point() +\rtheme_minimal() #should remove the isolated two data\rWe have 2 isolated points from Klinik Desa data. We will drop these 2 points as well.\nall_clinic2 \u0026lt;- all_clinic %\u0026gt;% filter(long \u0026gt; 25)\r\rPlotting the map\rThere are 2 ways to plot our data to Malaysia map, that we going to cover in this post.\n1) map from ggplot2\rFirst, we need to get the map.\nglobal \u0026lt;- map_data(\u0026quot;world\u0026quot;) #get map\rOnce, we retrieved the map, we need to filter the region to Malaysia. The rest of the codes are ggplot2 function as we know it.\nggplot() + geom_polygon(data = global %\u0026gt;% filter(region == \u0026quot;Malaysia\u0026quot;), aes(x=long, y = lat, group = group), fill = \u0026quot;gray85\u0026quot;) + coord_fixed(1.3) +\rgeom_point(data = all_clinic2, aes(x = long, y = lat, group = Type, color = Type, shape = Type)) +\rtheme_void() + xlab(\u0026quot;Longitude\u0026quot;) +\rylab(\u0026quot;Latitude\u0026quot;) +\rlabs(title = \u0026quot;Klinik 1Malaysia dan Klinik Desa di Malaysia\u0026quot;, subtitle = \u0026quot;(Data dikemaskini: Klinik 1Malaysia - 16 Mac 2021, Klinik Desa - 9 Mac 2021)\u0026quot;,\rcaption = expression(paste(italic(\u0026quot;Sumber data: https://www.data.gov.my/data/ms_MY/group/pemetaan\u0026quot;))), color = \u0026quot;Jenis klinik:\u0026quot;, shape = \u0026quot;Jenis klinik:\u0026quot;) +\rtheme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = \u0026quot;bottom\u0026quot;) \r\r2) map from rworldmap\rThe flow is similar, we need to get the map first. Then, restrict the map to Malaysia region.\nworld \u0026lt;- getMap(resolution = \u0026quot;low\u0026quot;) #get map\rmsia \u0026lt;- world[world@data$ADMIN == \u0026quot;Malaysia\u0026quot;, ]\rThe rest of the codes are similar to the first approach. But, we going to change the theme a bit.\nggplot() +\rgeom_polygon(data = msia, aes(x = long, y = lat, group = group), fill = NA, colour = \u0026quot;black\u0026quot;) +\rgeom_point(data = all_clinic2, aes(x = long, y = lat, group = Type, color = Type, shape = Type)) +\rcoord_quickmap() + theme_minimal() + xlab(\u0026quot;Longitude\u0026quot;) +\rylab(\u0026quot;Latitude\u0026quot;) +\rlabs(title = \u0026quot;Klinik 1Malaysia dan Klinik Desa di Malaysia\u0026quot;, subtitle = \u0026quot;(Data dikemaskini: Klinik 1Malaysia - 16 Mac 2021, Klinik Desa - 9 Mac 2021)\u0026quot;,\rcaption = expression(paste(italic(\u0026quot;Sumber data: https://www.data.gov.my/data/ms_MY/group/pemetaan\u0026quot;))), color = \u0026quot;Jenis klinik:\u0026quot;, shape = \u0026quot;Jenis klinik:\u0026quot;) +\rtheme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = \u0026quot;bottom\u0026quot;)\r\r\rConclusion\rThe coordinates that we have are not as accurate as it should, or maybe there is something wrong that I miss along the way. As we can see, we have clinics on the ocean. As far as I know, we Malaysian are not that advanced yet. Also, noticed that we severely lacking clinics in Sarawak, given that our data is correct.\n\r","date":1636675200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636682282,"objectID":"a334cb50dfe8e64a6498c37c1ac1c5f3","permalink":"https://tengkuhanis.netlify.app/post/making-maps-with-r-my-first-attempt-ever/","publishdate":"2021-11-12T00:00:00Z","relpermalink":"/post/making-maps-with-r-my-first-attempt-ever/","section":"post","summary":"As written in the title of the post, this is my first try ever in making a map with R. I found a great data on the distribution of the clinics in Malaysia.","tags":["map","spatial"],"title":"Making maps with R (my first attempt ever!)","type":"post"},{"authors":[],"categories":["R","Epidemiology"],"content":"\r\rRecently, I found a GitHub repo containing a global COVID-19 dataset. I thought, why not try to do some plotting for Southeast Asian countries. So, I downloaded the data and limited the data to Southeast Asian countries only (Brunei, Indonesia, Malaysia, Philippines, Singapore, Thailand and Vietnam). I have uploaded this restricted data to my GitHub repo.\nWe are not going to do anything fancy, just some visualisations.\nLet‚Äôs begin by reading the data.\nlibrary(tidyverse)\rcovid_sea \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/tengku-hanis/data-owid-covid/main/covid_sea.csv\u0026quot;)\rWe are going to compare between each Southeast Asian countries in terms of:\nDaily cases\rDaily deaths\rDaily tests\rDaily vaccinations\r\rBefore that, we need to make a function, as all the above items have a generic things to plot with the exception on y axis.\neasy_plot \u0026lt;- function(var1, lab_title, yaxis_lab, span = 0.14){\rcovid_sea %\u0026gt;% select(date, location, {{var1}}) %\u0026gt;% drop_na() %\u0026gt;% ggplot(aes(date, {{var1}}, color = location)) +\rgeom_smooth(se = F, span = 0.14) +\rgeom_point(aes(color = location), alpha = 0.2) +\rgeom_line(aes(color = location), alpha = 0.2, linetype = \u0026quot;dashed\u0026quot;) +\rlabs(title = {{lab_title}}) +\rylab({{yaxis_lab}}) +\rxlab(\u0026quot;Date\u0026quot;) +\rtheme_minimal() }\rvar1 is going to be the item/variable that we want to compare, lab_title is the plot title, yaxis_lab is the label on the y axis, and span is just how smooth our smoothen line should be.\nDaily cases\reasy_plot(new_cases, \u0026quot;Daily cases for southeast Asian countries\u0026quot;, \u0026quot;Daily cases\u0026quot;, span = 0.8)\rWe cannot compare in terms of the frequency as big countries like Indonesia is expected to had a higher number of daily cases. A smoothen line though very basic, may indicate a simple trend. Thailand, Malaysia, Philippines and Indonesia seems to had a decreasing trend of cases. On the other hand, the daily cases in Vietnam seems to start to increase. Singapore had a more stabilised trend of cases, though a higher number of cases was observed in the latest period. Lastly, Brunei had too little cases, for us to see any sort of trend at the scale of the between countries comparison.\n\rDaily deaths\reasy_plot(new_deaths, \u0026quot;Daily deaths for southeast Asian countries\u0026quot;, \u0026quot;Daily deaths\u0026quot;, span = 0.8)\rPhilippines and Indonesia seems started to had a bit of increasing trend. Other countries look okay.\n\rDaily tests\reasy_plot(new_tests, \u0026quot;Daily tests for southeast Asian countries\u0026quot;, \u0026quot;Daily tests\u0026quot;, span = 0.2)\rThe daily tests plot looks a bit weird for Vietnam. Actually, the daily tests below zero are not avaliable (not sure if there is no test done in the period or the values is just missing). Hence, the weird looking plot for Vietnam. Data for Brunei and Thailand are not available. Malaysia seems to be quite aggressive in COVID-19 testing, even on par with Indonesia. Also, Vietnam seems to be very aggressive in the latest period, probably to cover the lack of COVID-19 testing previously.\n\rDaily vaccinations\reasy_plot(new_vaccinations, \u0026quot;Daily vaccinations for southeast Asian countries\u0026quot;, \u0026quot;Daily vaccinations\u0026quot;, span = 0.9)\rMalaysia and Singapore had quite a similar distribution. Vietnam, Philippines, Thailand and Indonesia quite similar in which they had a series of wave in the rate of vaccinations, though the trend of wave for Thailand is less obvious. Again, the number in Brunei was too little for us to see any trend or distribution at this scale.\n\rMalaysia situation\rLet‚Äôs do a plot, specific to Malaysia. We going to scale the numbers, so that we able to see a comparison in term of trend or distribution.\ncovid_sea %\u0026gt;% filter(location == \u0026quot;Malaysia\u0026quot;) %\u0026gt;% mutate(new_cases = scale(new_cases), new_deaths = scale(new_deaths), new_tests = scale(new_tests), new_vaccinations = scale(new_vaccinations)) %\u0026gt;% ggplot(aes(date)) +\rgeom_line(aes(y = new_cases, color = \u0026quot;new_cases\u0026quot;), alpha = 0.3) +\rgeom_line(aes(y = new_deaths, color = \u0026quot;new_deaths\u0026quot;), alpha = 0.3) +\rgeom_line(aes(y = new_tests, color = \u0026quot;new_tests\u0026quot;), alpha = 0.3) +\rgeom_line(aes(y = new_vaccinations, color = \u0026quot;new_vaccinations\u0026quot;), alpha = 0.3) +\rgeom_point(aes(y = new_cases, color = \u0026quot;new_cases\u0026quot;), alpha = 0.3) +\rgeom_point(aes(y = new_deaths, color = \u0026quot;new_deaths\u0026quot;), alpha = 0.3) +\rgeom_point(aes(y = new_tests, color = \u0026quot;new_tests\u0026quot;), alpha = 0.3) +\rgeom_point(aes(y = new_vaccinations, color = \u0026quot;new_vaccinations\u0026quot;), alpha = 0.3) +\rgeom_smooth(aes(y = new_cases, color = \u0026quot;new_cases\u0026quot;), se = F, span = 0.3) +\rgeom_smooth(aes(y = new_deaths, color = \u0026quot;new_deaths\u0026quot;), se = F, span = 0.3) +\rgeom_smooth(aes(y = new_tests, color = \u0026quot;new_tests\u0026quot;), se = F, span = 0.3) +\rgeom_smooth(aes(y = new_vaccinations, color = \u0026quot;new_vaccinations\u0026quot;), se = F, span = 0.6) +\rlabs(title = \u0026quot;Situation in Malaysia\u0026quot;) +\rylab(\u0026quot;Scaled Frequency\u0026quot;) +\rxlab(\u0026quot;Date\u0026quot;) +\rguides(color = guide_legend(\u0026quot;Items\u0026quot;)) +\rscale_color_discrete(labels = c(\u0026quot;Daily cases\u0026quot;, \u0026quot;Daily deaths\u0026quot;, \u0026quot;Daily tests\u0026quot;, \u0026quot;Daily vaccinations\u0026quot;)) +\rtheme_minimal()\rInterestingly, as the number of vaccination increased up to a certain threshold, the number of daily cases and daily deaths started to decreased. Obviously, the daily testing also decreased as in Malaysia, COVID-19 testing is done based on suspected cases and their persons of contact instead of mass testing.\nDisclaimer: Please take anything written here with a massive grain of salt.\nData source:\rhttps://github.com/owid/covid-19-data/tree/master/public/data\n\r","date":1636502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636554811,"objectID":"8db3b92a16116a194461103e6b7b8c6e","permalink":"https://tengkuhanis.netlify.app/post/some-covid-19-plots-for-southeast-asian-countries/","publishdate":"2021-11-10T00:00:00Z","relpermalink":"/post/some-covid-19-plots-for-southeast-asian-countries/","section":"post","summary":"Recently, I found a GitHub repo containing a global COVID-19 dataset. I thought, why not try to do some plotting for Southeast Asian countries. So, I downloaded the data and limited the data to Southeast Asian countries only (Brunei, Indonesia, Malaysia, Philippines, Singapore, Thailand and Vietnam).","tags":["COVID-19","Vaccine"],"title":"Some COVID-19 plots for Southeast Asian countries","type":"post"},{"authors":["Tengku Muhammad Hanis","Md Asiful Islam","Kamarul Imran Musa"],"categories":null,"content":"","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"2c02f1dd66175c4cabe960c5a89d61da","permalink":"https://tengkuhanis.netlify.app/publication/2021-01-01_top_100_most-cited_p/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/publication/2021-01-01_top_100_most-cited_p/","section":"publication","summary":"","tags":[],"title":"Top 100 most-cited publications on breast cancer and machine learning research: a bibliometric analysis","type":"publication"},{"authors":[],"categories":[],"content":"","date":1636207200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636039694,"objectID":"d27a93533355653bcf697c598073a014","permalink":"https://tengkuhanis.netlify.app/talk/pre-conference-workshop-an-introduction-to-meta-analysis-in-r/","publishdate":"2021-11-04T00:00:00Z","relpermalink":"/talk/pre-conference-workshop-an-introduction-to-meta-analysis-in-r/","section":"talk","summary":"This workshop was part of R confeRence 2021 organised by [Malaysia R User Group (MyRUG)](https://www.facebook.com/rusergroupmalaysia/).","tags":[],"title":"Pre-conference Workshop - An introduction to meta-analysis in R","type":"talk"},{"authors":[],"categories":["R"],"content":"\r\rIn a couple of days, I am going to conduct a pre-conference workshop for Malaysian R conference 2021. So, some of the data that I am going to use for this workshop is available in a table in pdf form. Hence, this post is about how I get that particular table from the pdf into R for further analysis.\nSo, this is a table we going to extract.\nExtracting a table from pdf\rWe going to use tabulizer package for this. However, not every pdf works with this package. In our case, it works but need further preprocessing.\nLoad the required packages.\nlibrary(tabulizer)\rlibrary(dplyr)\rlibrary(stringr)\rRead a table from a pdf.\nraw_table \u0026lt;- extract_tables(\u0026quot;https://static-content.springer.com/esm/art%3A10.1038%2Fs41440-021-00720-3/MediaObjects/41440_2021_720_MOESM1_ESM.pdf\u0026quot;, pages = 17, output = \u0026quot;data.frame\u0026quot;)\rSo, this is the extracted table.\nraw_table[[1]] %\u0026gt;% head(10)\r## X X.1 X.2 X.3 X.4 X.5 X.6 X.7 X.8\r## 1 ## 2 ## 3 Ahmed, 2019 Unclear Unclear Unclear High Unclear Low Unclear High\r## 4 ## 5 Badrov, 2013 Unclear High High High Unclear Low Unclear High\r## 6 Baross, 2012 Unclear Unclear High High Unclear Low Unclear High\r## 7 Baross, 2013 Unclear Unclear High High Unclear Low Unclear High\r## 8 Carlson, 2016 Low High High Low Unclear Low Low High\r## 9 Correia, 2020 Low Low Low High Unclear Low Low High\r## 10 ## X.9\r## 1 1- selection bias: random\r## 2 sequence generation\r## 3 2- selection bias: allocation\r## 4 concealment\r## 5 ## 6 3- reporting bias: selective\r## 7 reporting\r## 8 ## 9 4- Performance bias: blinding\r## 10 (participants and personnel)\rSo, a few preprocessing steps needed:\nRemove column X.9 - this column supposed to be a header\rRename a header based on column X.9\rRemove a space between the author name - ‚ÄúAhmed,2019‚Äù instead of ‚ÄúAhmed, 2019‚Äù\rRemove empty rows\r\rirt_rob \u0026lt;- raw_table[[1]] %\u0026gt;% select(-X.9) %\u0026gt;% rename(Study = X, Random.sequence.generation. = X.1, Allocation.concealment. = X.2,\rSelective.reporting. = X.3,\rBlinding.of.participants.and.personnel. = X.4, Blinding.of.outcome.assessment = X.5, Incomplete.outcome.data = X.6, Other.sources.of.bias. = X.7, Overall = X.8) %\u0026gt;% as_tibble() %\u0026gt;% mutate(Study = str_replace_all(Study, \u0026quot; \u0026quot;, \u0026quot;\u0026quot;)) %\u0026gt;% mutate(id_del = str_match(Study, \u0026quot;.\u0026quot;)) %\u0026gt;% filter(!is.na(id_del)) %\u0026gt;% select(-id_del)\rFinally, our data is ready.\nirt_rob\r## Study Random.sequence.generation. Allocation.concealment.\r## 1 Ahmed,2019 Unclear Unclear\r## 2 Badrov,2013 Unclear High\r## 3 Baross,2012 Unclear Unclear\r## 4 Baross,2013 Unclear Unclear\r## 5 Carlson,2016 Low High\r## Selective.reporting. Blinding.of.participants.and.personnel.\r## 1 Unclear High\r## 2 High High\r## 3 High High\r## 4 High High\r## 5 High Low\r## Blinding.of.outcome.assessment Incomplete.outcome.data Other.sources.of.bias.\r## 1 Unclear Low Unclear\r## 2 Unclear Low Unclear\r## 3 Unclear Low Unclear\r## 4 Unclear Low Unclear\r## 5 Unclear Low Low\r## Overall\r## 1 High\r## 2 High\r## 3 High\r## 4 High\r## 5 High\r\r","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635743591,"objectID":"e696c84bb3b1155c4b77269c4bedc703","permalink":"https://tengkuhanis.netlify.app/post/extract-a-table-from-a-pdf/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/post/extract-a-table-from-a-pdf/","section":"post","summary":"In a couple of days, I am going to conduct a pre-conference workshop for Malaysian R conference 2021. So, some of the data that I am going to use for this workshop is available in a table in pdf form.","tags":["Data exploration"],"title":"Extract a table from a pdf","type":"post"},{"authors":[],"categories":["R","applied statistics"],"content":"\r\rBackground\rMissing data is quite challenging to deal with. Deleting it may be the easiest solution, but may not be the best solution. Missing data can be categorised into 3 types (Rubin, 1976):\nMCAR\n\rMissing Completely At Random\rExample; some of the observations are missing due to lost of records during the flood\r\rMAR\n\rMissing At Random\rExample; variable income are missing as some participant refuse to give their salary information which they deems as very personal information\r\rMNAR\n\rMissing Not At Random\rExample; weight variable is missing for morbidly obese participants since the scale is unable to weight them\r\r\rOut of the 3 types above, the most problematic is MNAR, though there exist methods to deal with this type. For example, the miceMNAR package in R.\nThere are several approaches in handling missing data:\nListwise-deletion\n\rBest approach if the amount of missingness is very small\r\rSimple imputation\n\rUsing mean/median/mode imputation\rThis approach is not advisable as it leads to bias due to reduce variance, though the mean is not affected\r\rSingle imputation\n\rSimple imputation above is considered as single imputation as well\rThis approach ignores uncertainty of the imputation and almost always underestimate the variance\r\rMultiple imputation\n\rA bit advanced and it cover the limitation of single imputation approach\r\r\rHowever, the main assumption for any imputation methods is the missingness should be MCAR or MAR.\n\rMultiple imputation\rIn short, there are 2 approaches of multiple imputation implemented by packages in R:\nJoint modeling (JM) or joint multivariate normal distribution multiple imputation\n\rThe main assumption for this method is that the observed data follows a multivariate normal distribution\rA violation of this assumption produces incorrect values, though a slight violation is still okay\rSome packages that implemented this method: Amelia and norm\r\rFully conditional specification (FCS) or conditional multiple imputation\n\rAlso known as multivariate imputation by chained equation (MICE)\rThis approach is a bit flexible as distribution is assumed for each variable rather than the whole dataset\rSome package that implemented this method: mice and mi\r\r\r\rExample\rIn mice package, the general steps are:\nmice() - impute the NAs\rwith() - run the analysis (lm, glm, etc)\rpool() - pool the results\r\r\rFigure 1: Main steps in mice package.\r\rThese are the required packages.\nlibrary(tidyverse)\rlibrary(mice)\rlibrary(VIM)\r#library(missForest) we want to use prodNA() function from this package\rlibrary(naniar)\rlibrary(niceFunction) #install from github (https://github.com/tengku-hanis/niceFunction)\rlibrary(dplyr)\rlibrary(gtsummary)\rWe going to produce some NAs randomly.\nset.seed(123)\rdat \u0026lt;- iris %\u0026gt;% select(-Sepal.Length)%\u0026gt;% missForest::prodNA(0.2) %\u0026gt;% # randomly insert 20% NAs\rmutate(Sepal.Length = iris$Sepal.Length)\rExplore the NAs and the data.\nnaniar::miss_var_summary(dat)\r## # A tibble: 5 x 3\r## variable n_miss pct_miss\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Petal.Length 38 25.3\r## 2 Sepal.Width 33 22 ## 3 Species 28 18.7\r## 4 Petal.Width 21 14 ## 5 Sepal.Length 0 0\rSome references recommend to remove variables with more than 50% NAs. However, we purposely introduce 20% NAs into our data.\nAs a guideline, we can check for MCAR for our NAs.\nnaniar::mcar_test(dat) #p \u0026gt; 0.05, MCAR is indicated\r## # A tibble: 1 x 4\r## statistic df p.value missing.patterns\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r## 1 38.8 40 0.522 14\rNext step is to evaluate the pattern of missingness in our data.\nmd.pattern(dat, rotate.names = T, plot = T) \r## Sepal.Length Petal.Width Species Sepal.Width Petal.Length ## 64 1 1 1 1 1 0\r## 21 1 1 1 1 0 1\r## 15 1 1 1 0 1 1\r## 3 1 1 1 0 0 2\r## 14 1 1 0 1 1 1\r## 4 1 1 0 1 0 2\r## 6 1 1 0 0 1 2\r## 2 1 1 0 0 0 3\r## 7 1 0 1 1 1 1\r## 6 1 0 1 1 0 2\r## 4 1 0 1 0 1 2\r## 2 1 0 1 0 0 3\r## 1 1 0 0 1 1 2\r## 1 1 0 0 0 1 3\r## 0 21 28 33 38 120\raggr(dat, prop = F, numbers = T) \rWe have 13 patterns (numbers on the right) of NAs in our data. These 2 functions work well with small dataset, but with a larger dataset (and with lot more pattern of NAs), it‚Äôs probably quite difficult to assess the pattern.\nmatrixplot() probably more appropriate for a larger dataset.\nmatrixplot(dat)\rIn terms of the missingness pattern, we can also assess the distribution of NAs of Sepal.Width is dependent on the variable Sepal.Length.\nniceFunction::histNA_byVar(dat, Sepal.Width, Sepal.Length)\rAs we can see the distribution and range of the histograms of the NAs (True) and non-NAs (False) is quite similar. Thus, this may indicated that Sepal.Width is at least MAR. However, by right we should do this for each pair of numerical variable before jumping into any conclusion.\nAnother good thing to assess is the correlation.\n# Data with 1 = NAs, 0 = non-NAs\rx \u0026lt;- as.data.frame(abs(is.na(dat))) %\u0026gt;% dplyr::select(-Sepal.Length) #pick variable with NAs only\rFirstly, the correlation between the variables with missing data.\ncor(x) %\u0026gt;% corrplot::corrplot()\rNo high correlation among variable with NAs. Secondly, let‚Äôs see correlation between NAs in a variable and the observed values of other variables.\ncor(dat %\u0026gt;% mutate(Species = as.numeric(Species)), x, use = \u0026quot;pairwise.complete.obs\u0026quot;)\r## Sepal.Width Petal.Length Petal.Width Species\r## Sepal.Width NA 0.049158733 -0.065917718 0.09948263\r## Petal.Length 0.042075695 NA -0.004572405 -0.17265919\r## Petal.Width 0.096195805 -0.003320601 NA -0.11024288\r## Species 0.045849046 -0.104143925 -0.081055707 NA\r## Sepal.Length -0.006435044 -0.052871701 -0.091024799 -0.08527514\rAgain, there is no high correlation. But, if we were to interpret this correlation matrix; the rows are the observed variables and the columns represent the missingness. For example, missing values of Sepal.Width is more likely to be missing for observations with a high value of Petal.Width (r = 0.05 indicates it‚Äôs highly unlikely though).\nNow, we can do multiple imputation. These are the methods in the mice package:\nmethods(mice)\r## [1] mice.impute.2l.bin mice.impute.2l.lmer mice.impute.2l.norm ## [4] mice.impute.2l.pan mice.impute.2lonly.mean mice.impute.2lonly.norm ## [7] mice.impute.2lonly.pmm mice.impute.cart mice.impute.jomoImpute ## [10] mice.impute.lda mice.impute.logreg mice.impute.logreg.boot ## [13] mice.impute.mean mice.impute.midastouch mice.impute.mnar.logreg ## [16] mice.impute.mnar.norm mice.impute.norm mice.impute.norm.boot ## [19] mice.impute.norm.nob mice.impute.norm.predict mice.impute.panImpute ## [22] mice.impute.passive mice.impute.pmm mice.impute.polr ## [25] mice.impute.polyreg mice.impute.quadratic mice.impute.rf ## [28] mice.impute.ri mice.impute.sample mice.mids ## [31] mice.theme ## see \u0026#39;?methods\u0026#39; for accessing help and source code\rBy default, mice uses:\n\rpmm (predictive mean matching) for numeric data\rlogreg (logistic regression imputation) for binary data, factor with 2 levels\rpolyreg (polytomous regression imputation) for unordered categorical data (factor \u0026gt; 2 levels)\rpolr (proportional odds model) for ordered, \u0026gt; 2 levels\r\rlet‚Äôs run the mice function to our data:\nimp \u0026lt;- mice(dat, m = 5, seed=1234, maxit = 5, printFlag = F) imp\r## Class: mids\r## Number of multiple imputations: 5 ## Imputation methods:\r## Sepal.Width Petal.Length Petal.Width Species Sepal.Length ## \u0026quot;pmm\u0026quot; \u0026quot;pmm\u0026quot; \u0026quot;pmm\u0026quot; \u0026quot;polyreg\u0026quot; \u0026quot;\u0026quot; ## PredictorMatrix:\r## Sepal.Width Petal.Length Petal.Width Species Sepal.Length\r## Sepal.Width 0 1 1 1 1\r## Petal.Length 1 0 1 1 1\r## Petal.Width 1 1 0 1 1\r## Species 1 1 1 0 1\r## Sepal.Length 1 1 1 1 0\rNext, we can do some diagnostic assessment on the imputed data. This is our imputed data.\nimp$imp$Sepal.Width %\u0026gt;% head()\r## 1 2 3 4 5\r## 5 3.4 3.4 4.1 3.1 3.5\r## 13 3.2 3.1 3.2 3.6 3.1\r## 14 3.1 3.2 2.9 3.4 3.0\r## 23 3.6 3.2 3.0 3.8 3.1\r## 26 4.1 3.0 3.1 3.5 3.0\r## 34 3.4 3.7 3.7 3.4 4.4\rOne important thing to check is the convergence. We are going increase the number of iteration for this.\nimp_conv \u0026lt;- mice.mids(imp, maxit = 30, printFlag = F)\rplot(imp_conv)\rThe line in the plot should be intermingled and no obvious trend should be observed. Our plot above indicates a convergence.\nWe can also assess density plot of imputed data and the observed data. Blue color is the observed data and red color is the imputed data.\ndensityplot(imp)\rWe can further assess variable Sepal.Width.\ndensityplot(imp, ~ Sepal.Width | .imp)\rLastly, we can assess the strip plot. The imputed observations (red color) should not distributed too far from the observed data (blue color).\nstripplot(imp)\rSo, once we finish the diagnostic checking, we can actually go back and change the imputation method for Sepal.Width, since the its distribution changes quite differently at each iteration. But, we are not going to do that, instead we are going to do the analysis.\n# run regression\rfit \u0026lt;- with(imp, lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width + Species))\r# pool all imputed set\rpooled \u0026lt;- pool(fit) summary(pooled)\r## term estimate std.error statistic df p.value\r## 1 (Intercept) 2.2008307 0.34577321 6.364954 29.02484 5.859560e-07\r## 2 Sepal.Width 0.5233500 0.09717217 5.385801 50.89918 1.854832e-06\r## 3 Petal.Length 0.7409159 0.09020153 8.214006 12.73722 1.921415e-06\r## 4 Petal.Width -0.3623895 0.18562168 -1.952301 22.34517 6.354332e-02\r## 5 Speciesversicolor -0.3891112 0.28166528 -1.381467 15.07547 1.872683e-01\r## 6 Speciesvirginica -0.5237106 0.42629920 -1.228505 10.82804 2.452897e-01\rSince we have the original dataset without the NAs, we going to compare them.\nmimpute \u0026lt;- fit %\u0026gt;% tbl_regression() #with mice\rnoimpute \u0026lt;- dat %\u0026gt;% lm(Sepal.Length ~ ., data = .) %\u0026gt;% tbl_regression() #w/o mice\roriginal \u0026lt;- iris %\u0026gt;% lm(Sepal.Length ~ ., data = .) %\u0026gt;% tbl_regression() #original data\rtbl_merge(\rtbls = list(mimpute, noimpute, original), tab_spanner = c(\u0026quot;With MICE\u0026quot;, \u0026quot;Without MICE\u0026quot;, \u0026quot;Original data\u0026quot;)\r)\rhtml {\rfont-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r}\r#kofvwjwgme .gt_table {\rdisplay: table;\rborder-collapse: collapse;\rmargin-left: auto;\rmargin-right: auto;\rcolor: #333333;\rfont-size: 16px;\rfont-weight: normal;\rfont-style: normal;\rbackground-color: #FFFFFF;\rwidth: auto;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #A8A8A8;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #A8A8A8;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\r}\r#kofvwjwgme .gt_heading {\rbackground-color: #FFFFFF;\rtext-align: center;\rborder-bottom-color: #FFFFFF;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#kofvwjwgme .gt_title {\rcolor: #333333;\rfont-size: 125%;\rfont-weight: initial;\rpadding-top: 4px;\rpadding-bottom: 4px;\rborder-bottom-color: #FFFFFF;\rborder-bottom-width: 0;\r}\r#kofvwjwgme .gt_subtitle {\rcolor: #333333;\rfont-size: 85%;\rfont-weight: initial;\rpadding-top: 0;\rpadding-bottom: 6px;\rborder-top-color: #FFFFFF;\rborder-top-width: 0;\r}\r#kofvwjwgme .gt_bottom_border {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#kofvwjwgme .gt_col_headings {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#kofvwjwgme .gt_col_heading {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\rpadding-left: 5px;\rpadding-right: 5px;\roverflow-x: hidden;\r}\r#kofvwjwgme .gt_column_spanner_outer {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rpadding-top: 0;\rpadding-bottom: 0;\rpadding-left: 4px;\rpadding-right: 4px;\r}\r#kofvwjwgme .gt_column_spanner_outer:first-child {\rpadding-left: 0;\r}\r#kofvwjwgme .gt_column_spanner_outer:last-child {\rpadding-right: 0;\r}\r#kofvwjwgme .gt_column_spanner {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 5px;\roverflow-x: hidden;\rdisplay: inline-block;\rwidth: 100%;\r}\r#kofvwjwgme .gt_group_heading {\rpadding: 8px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\r}\r#kofvwjwgme .gt_empty_group_heading {\rpadding: 0.5px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: middle;\r}\r#kofvwjwgme .gt_from_md  :first-child {\rmargin-top: 0;\r}\r#kofvwjwgme .gt_from_md  :last-child {\rmargin-bottom: 0;\r}\r#kofvwjwgme .gt_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rmargin: 10px;\rborder-top-style: solid;\rborder-top-width: 1px;\rborder-top-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\roverflow-x: hidden;\r}\r#kofvwjwgme .gt_stub {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-right-style: solid;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rpadding-left: 12px;\r}\r#kofvwjwgme .gt_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#kofvwjwgme .gt_first_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\r}\r#kofvwjwgme .gt_grand_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#kofvwjwgme .gt_first_grand_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: double;\rborder-top-width: 6px;\rborder-top-color: #D3D3D3;\r}\r#kofvwjwgme .gt_striped {\rbackground-color: rgba(128, 128, 128, 0.05);\r}\r#kofvwjwgme .gt_table_body {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#kofvwjwgme .gt_footnotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#kofvwjwgme .gt_footnote {\rmargin: 0px;\rfont-size: 90%;\rpadding: 4px;\r}\r#kofvwjwgme .gt_sourcenotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#kofvwjwgme .gt_sourcenote {\rfont-size: 90%;\rpadding: 4px;\r}\r#kofvwjwgme .gt_left {\rtext-align: left;\r}\r#kofvwjwgme .gt_center {\rtext-align: center;\r}\r#kofvwjwgme .gt_right {\rtext-align: right;\rfont-variant-numeric: tabular-nums;\r}\r#kofvwjwgme .gt_font_normal {\rfont-weight: normal;\r}\r#kofvwjwgme .gt_font_bold {\rfont-weight: bold;\r}\r#kofvwjwgme .gt_font_italic {\rfont-style: italic;\r}\r#kofvwjwgme .gt_super {\rfont-size: 65%;\r}\r#kofvwjwgme .gt_footnote_marks {\rfont-style: italic;\rfont-weight: normal;\rfont-size: 65%;\r}\r\r\rCharacteristic\rWith MICE\r\rWithout MICE\r\rOriginal data\r\r\r\rBeta\r95% CI1\rp-value\rBeta\r95% CI1\rp-value\rBeta\r95% CI1\rp-value\r\r\rSepal.Width\r0.52\r0.33, 0.72\r\r0.48\r0.17, 0.79\r0.003\r0.50\r0.33, 0.67\r\rPetal.Length\r0.74\r0.55, 0.94\r\r0.71\r0.51, 0.90\r\r0.83\r0.69, 1.0\r\rPetal.Width\r-0.36\r-0.75, 0.02\r0.064\r-0.35\r-0.85, 0.14\r0.2\r-0.32\r-0.61, -0.02\r0.039\rSpecies\r\r\r\r\r\r\r\r\r\rsetosa\r‚Äî\r‚Äî\r\r‚Äî\r‚Äî\r\r‚Äî\r‚Äî\r\rversicolor\r-0.39\r-1.0, 0.21\r0.2\r-0.42\r-1.1, 0.30\r0.3\r-0.72\r-1.2, -0.25\r0.003\rvirginica\r-0.52\r-1.5, 0.42\r0.2\r-0.42\r-1.5, 0.63\r0.4\r-1.0\r-1.7, -0.36\r0.003\r\r\r1\r\rCI = Confidence Interval\r\r\r\r\r\rThere is a different in the result between the original dataset (no NAs) and with mice imputation. Probably, exploring other imputation methods will produce a better result.\nThere are a lot more that are not cover in this post. For example passive imputation and post-processing. In fact, there are a series of vignettes written by Gerko Vink and Stef van Buuren (both are the authors of mice) which provides a good tutorial on using mice though quite advanced.\nSuggested online books (though, I have not really studied both of the books yet):\nFlexible imputation of missing data by Stef van Buuren\rApplied missing data analysis with SPSS and (R)Studio\r\rReferences for this post:\nR in Action, Data analysis and graphics with R (Chapter 15)\rhttps://data.library.virginia.edu/getting-started-with-multiple-imputation-in-r/\rhttps://stats.idre.ucla.edu/r/faq/how-do-i-perform-multiple-imputation-using-predictive-mean-matching-in-r/\rmice: Multivariate Imputation by Chained Equations in R\r\r\r","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635510904,"objectID":"8f34c76190b1f2ce6a1652a2fb93445a","permalink":"https://tengkuhanis.netlify.app/post/a-short-note-on-multiple-imputation/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/post/a-short-note-on-multiple-imputation/","section":"post","summary":"Background\rMissing data is quite challenging to deal with. Deleting it may be the easiest solution, but may not be the best solution. Missing data can be categorised into 3 types (Rubin, 1976):","tags":["missing data"],"title":"A short note on multiple imputation","type":"post"},{"authors":[],"categories":[],"content":"","date":1634565600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636038816,"objectID":"07e132b33e14d86485b672f51fb163c7","permalink":"https://tengkuhanis.netlify.app/talk/workshop-sytematic-review-and-meta-analysis-using-rstudio/","publishdate":"2021-10-18T14:00:00Z","relpermalink":"/talk/workshop-sytematic-review-and-meta-analysis-using-rstudio/","section":"talk","summary":"This workshop was part of series of workshop organised by [Department of Medical Microbiology and Parasitology](https://medic.usm.my/microbiology/en/), School of Medical Sciences, USM.","tags":[],"title":"Workshop - Sytematic review and meta-analysis using RStudio","type":"talk"},{"authors":[],"categories":["R","text analysis"],"content":"\r\rWe are going to do a basic google trends search using gtrendsR package and do some plotting with ggplot2.\nThese are the required packages.\nlibrary(gtrendsR)\rlibrary(tidyverse)\rRun gtrends() function to search our keywords of interest (i.e; type of vaccine). So far, we only used 4 type of vaccines in Malaysia.\nvaccine \u0026lt;- gtrends(c(\u0026quot;pfizer\u0026quot;, \u0026quot;astrazeneca\u0026quot;, \u0026quot;sinovac\u0026quot;, \u0026quot;cansino\u0026quot;), geo = \u0026quot;MY\u0026quot;)\rThen, plot our keywords.\nplot(vaccine)\rProbably, it‚Äôs better if we filter our date to when the COVID-19 pandemic started, which is around March 2020.\nvaccine$interest_over_time %\u0026gt;% group_by(keyword) %\u0026gt;% filter(hits != \u0026quot;\u0026lt;1\u0026quot; \u0026amp; date \u0026gt; as.Date(\u0026quot;2020-03-01\u0026quot;)) %\u0026gt;% mutate(hits = as.numeric(hits), date = as.Date(date)) %\u0026gt;% ggplot() + geom_line(aes(x = date, y = hits, color = keyword), size = 0.8) +\rtheme_minimal() +\rlabs(title = \u0026quot;COVID-19 vaccine interest in Malaysia\u0026quot;, y = \u0026quot;Search hits\u0026quot;, x = \u0026quot;Date\u0026quot;) +\rscale_x_date(date_breaks = \u0026quot;4 month\u0026quot;)\rSo, AstraZeneca vaccine is of high interest, probably due to infamous blood clotting issue. Next, we can also get the search keywords based on the states.\nvaccine$interest_by_region %\u0026gt;% group_by(location) %\u0026gt;% ggplot(aes(location, hits, fill = keyword)) +\rgeom_col(alpha = 0.8) +\rcoord_flip() +\rtheme_minimal() +\rscale_fill_viridis_d() +\rlabs(title = \u0026quot;COVID-19 vaccine interest in Malaysia by states\u0026quot;, y = \u0026quot;Search hits\u0026quot;, x = \u0026quot;\u0026quot;)\rLastly, we can plot the search keywords based on the city.\nvaccine$interest_by_city %\u0026gt;% group_by(location) %\u0026gt;% drop_na() %\u0026gt;% ggplot(aes(location, hits, fill = keyword)) +\rgeom_col(alpha = 0.8) +\rcoord_flip() +\rtheme_minimal() +\rscale_fill_viridis_d() +\rlabs(title = \u0026quot;COVID-19 vaccine interest in Malaysia by cities\u0026quot;, y = \u0026quot;Search hits\u0026quot;, x = \u0026quot;\u0026quot;)\rgtrendsR with just a bit of plots certainly very useful if we want to gauge certain issues in the community.\n","date":1634428800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634445360,"objectID":"0af67d1b5682d47328982ef0e125722d","permalink":"https://tengkuhanis.netlify.app/post/covid-19-vaccine-interest-in-malaysia/","publishdate":"2021-10-17T00:00:00Z","relpermalink":"/post/covid-19-vaccine-interest-in-malaysia/","section":"post","summary":"We are going to do a basic google trends search using gtrendsR package and do some plotting with ggplot2.\nThese are the required packages.\nlibrary(gtrendsR)\rlibrary(tidyverse)\rRun gtrends() function to search our keywords of interest (i.","tags":["Vaccine","text analysis"],"title":"COVID-19 vaccine interest in Malaysia","type":"post"},{"authors":["Wan Nor Arifin","Kamarul Imran Musa","Tengku Muhammad Hanis","Wan Shakira Rodzlan Hasani","Che Muhammad Nur Hidayat Che Nawi","Erwan Ershad Ahmad Khan","Mohd Azmi Suliman","Sahrol Azmi Termizi","Wira Alfatah Ab Aziz"],"categories":null,"content":"","date":1632873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632873600,"objectID":"16d757c712baa8c59373934f61d6d34b","permalink":"https://tengkuhanis.netlify.app/publication/2021-01-01_a_brief_analysis_of_/","publishdate":"2021-09-29T00:00:00Z","relpermalink":"/publication/2021-01-01_a_brief_analysis_of_/","section":"publication","summary":"","tags":[],"title":"A brief analysis of the COVID-19 death data in Malaysia","type":"publication"},{"authors":[],"categories":["R","text analysis"],"content":"\r\r\r\r\r\rLet‚Äôs see how much research has been done in term of COVID-19 in Malaysia. In this analysis, we are going to use Scopus database to access the relevant research or papers. In this analysis we are going to use 4 specific parts of the scientific paper:\nTitle\rAbstract\rAuthor‚Äôs keywords\rScopus‚Äôs keywords\r\rAbove is a sample of paper that shows the section of scientific paper that we are going to use in our analysis. The Scopus‚Äôs keywords are generated by the Scopus database, thus, it does not available on the paper.\nSo, the analysis will be applied separately on these 4 parts of the papers. Also, we are going to use map (equivalent to loop) since the flow of the analysis is similar.\nLoad the related packages. The main package is quanteda.\nlibrary(tidyverse)\rlibrary(quanteda)\rlibrary(quanteda.textstats)\rlibrary(quanteda.textplots)\rlibrary(patchwork)\rlibrary(wordcloud2)\rI have uploaded the data that I downloaded from the Scopus database into my GitHub.\n# Read data from GitHub repo\rdf \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/tengku-hanis/scopus-data/main/covid-malaysia.csv\u0026quot;) %\u0026gt;% janitor::clean_names() %\u0026gt;% rename(title =i_title)\rFirst, we need to tokenize the sentence. In other words, we break down the sentences into words.\n# Tokenize\rtok_list \u0026lt;- df %\u0026gt;% select(title, abstract, author_keywords, index_keywords) %\u0026gt;% map(tokens, remove_punct = T, remove_numbers = T, remove_symbols = T)\rNext, we remove words that are not meaningful such ‚Äòa‚Äô, ‚Äòthe‚Äô, etc. These words are known as stop words.\n# Remove stop words\rnostop_toks \u0026lt;- tok_list %\u0026gt;% map(tokens_select, c(tidytext::stop_words$word, stopwords(\u0026quot;en\u0026quot;)), selection = \u0026quot;remove\u0026quot;)\rThen, we create a document feature matrix (DFM). Basically DFM is a matrix that represent the frequency of each word (feature) in each document (in our case, paper or manuscript). Another name for DFM is document term matrix (DTM). quanteda uses the term DFM, some other packages use the term DTM.\nAdditionally, we also apply term frequency-inverse document frequency (TF-IDF) metrics. In scientific papers, the words such as ‚Äòdetermine‚Äô, ‚Äòconclusion‚Äô, ‚Äòintroduction‚Äô, etc are very frequent, and these words are not meaningful as well. Instead of removing manually one by one, we use TF-IDF. So, TF-IDF basically remove the words that are too common, thus we get only the relevant or important words.\n# Create DFM and apply tf_idf\rcovid_dfm_list \u0026lt;- nostop_toks %\u0026gt;% map(dfm) %\u0026gt;% map(dfm_tfidf)\rOnce, we have our words (tokens), we can create a plot of most relevant terms based on TF-IDF.\r\r\rShow code\r\r# Plot top features\rA \u0026lt;- covid_dfm_list$title %\u0026gt;% textstat_frequency(n = 15, force = T) %\u0026gt;% ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\rgeom_point(size = 4, colour = \u0026quot;blueviolet\u0026quot;) +\rcoord_flip() +\rlabs(x = NULL, y = \u0026quot;Frequency (tf-idf)\u0026quot;) +\rtheme_minimal() +\rlabs(title = \u0026quot;Top relevant terms for covid research based on the title\u0026quot;)\rB \u0026lt;- covid_dfm_list$abstract %\u0026gt;% textstat_frequency(n = 15, force = T) %\u0026gt;% ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\rgeom_point(size = 4, colour = \u0026quot;darkolivegreen3\u0026quot;) +\rcoord_flip() +\rlabs(x = NULL, y = \u0026quot;Frequency (tf-idf)\u0026quot;) +\rtheme_minimal() +\rlabs(title = \u0026quot;Top relevant terms for covid research based on the abstract\u0026quot;)\rC \u0026lt;- covid_dfm_list$author_keywords %\u0026gt;% textstat_frequency(n = 15, force = T) %\u0026gt;% ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\rgeom_point(size = 4, colour = \u0026quot;deepskyblue2\u0026quot;) +\rcoord_flip() +\rlabs(x = NULL, y = \u0026quot;Frequency (tf-idf)\u0026quot;) +\rtheme_minimal() +\rlabs(title = \u0026quot;Top relevant terms for covid research based on the author\u0026#39;s keywords\u0026quot;)\rD \u0026lt;- covid_dfm_list$index_keywords %\u0026gt;% textstat_frequency(n = 15, force = T) %\u0026gt;% ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\rgeom_point(size = 4, colour = \u0026quot;aquamarine2\u0026quot;) +\rcoord_flip() +\rlabs(x = NULL, y = \u0026quot;Frequency (tf-idf)\u0026quot;) +\rtheme_minimal() +\rlabs(title = \u0026quot;Top relevant terms for covid research based on the Scopus\u0026#39;s keywords\u0026quot;)\r\rThese are the plots of the most relevant terms in COVID-19 research in Malaysia.\rWordcloud\rFinally, we can make our wordcloud, but we need to convert our DFM to data frame first. Also, we are going to round the value of TF-IDF and limit to top 1000 terms only.\ncovid_wc \u0026lt;- covid_dfm_list %\u0026gt;% map(textstat_frequency, force = T)\rActually, quanteda itself is able to produce a wordcloud. However, the wordcloud from wordcloud2 is more interactive and we can see the value of TF-IDF if we click the words.\nwordcloud2(covid_wc$title%\u0026gt;% slice(1:1000) %\u0026gt;% mutate(frequency = round(frequency)))\r\r\r{\"x\":{\"word\":[\"pandemic\",\"malaysia\",\"√∞\",\"impact\",\"health\",\"study\",\"patients\",\"sars-cov-2\",\"learning\",\"review\",\"analysis\",\"students\",\"malaysian\",\"coronavirus\",\"control\",\"online\",\"global\",\"outbreak\",\"social\",\"risk\",\"healthcare\",\"model\",\"covid-19\",\"challenges\",\"education\",\"disease\",\"workers\",\"lockdown\",\"factors\",\"countries\",\"survey\",\"mental\",\"response\",\"potential\",\"psychological\",\"management\",\"system\",\"perspective\",\"movement\",\"public\",\"clinical\",\"role\",\"medical\",\"infection\",\"√¢\",\"detection\",\"cross-sectional\",\"university\",\"implications\",\"impacts\",\"systematic\",\"transmission\",\"covid\",\"care\",\"development\",\"evidence\",\"effect\",\"meta-analysis\",\"experience\",\"teaching\",\"based\",\"treatment\",\"knowledge\",\"practice\",\"approach\",\"economic\",\"strategies\",\"effects\",\"√±\",\"quality\",\"media\",\"measures\",\"indonesia\",\"asia\",\"amid\",\"vaccine\",\"pakistan\",\"mortality\",\"application\",\"tourism\",\"images\",\"intention\",\"digital\",\"coping\",\"anxiety\",\"spread\",\"islamic\",\"epidemic\",\"data\",\"people\",\"stress\",\"screening\",\"future\",\"performance\",\"post-covid-19\",\"perception\",\"crisis\",\"machine\",\"industry\",\"era\",\"hospital\",\"adults\",\"assessment\",\"critical\",\"services\",\"international\",\"time\",\"financial\",\"rapid\",\"depression\",\"food\",\"bangladesh\",\"sustainable\",\"stock\",\"understanding\",\"deep\",\"virtual\",\"molecular\",\"x-ray\",\"patient\",\"prevention\",\"asian\",\"chest\",\"responses\",\"comparison\",\"acute\",\"effectiveness\",\"perspectives\",\"population\",\"dynamics\",\"e-learning\",\"policy\",\"awareness\",\"security\",\"inhibitors\",\"framework\",\"cancer\",\"severe\",\"network\",\"perceptions\",\"prediction\",\"relationship\",\"association\",\"technology\",\"china\",\"recovery\",\"mco\",\"drugs\",\"lessons\",\"women\",\"air\",\"perceived\",\"research\",\"neural\",\"behavior\",\"level\",\"life\",\"ct\",\"exploring\",\"information\",\"therapy\",\"overview\",\"distress\",\"environmental\",\"monitoring\",\"emergency\",\"southeast\",\"strategy\",\"market\",\"environment\",\"syndrome\",\"status\",\"community\",\"children\",\"preventive\",\"prevalence\",\"diagnosis\",\"adoption\",\"sector\",\"approaches\",\"threat\",\"modelling\",\"preliminary\",\"managing\",\"characteristics\",\"region\",\"post\",\"practices\",\"due\",\"recommendations\",\"waste\",\"respiratory\",\"preparedness\",\"human\",\"support\",\"fear\",\"amidst\",\"therapeutic\",\"drug\",\"resilience\",\"current\",\"travel\",\"severity\",\"attitude\",\"vaccines\",\"effective\",\"methods\",\"insights\",\"news\",\"economy\",\"acceptance\",\"systems\",\"vaccination\",\"dental\",\"infections\",\"image\",\"academic\",\"physical\",\"activity\",\"report\",\"influence\",\"implementation\",\"diagnostic\",\"protease\",\"diseases\",\"immune\",\"outcomes\",\"well-being\",\"surgery\",\"period\",\"home\",\"evaluation\",\"experiences\",\"developing\",\"students√¢\",\"energy\",\"professionals\",\"√∞¬µ√∞\",\"sharing\",\"opportunities\",\"behaviour\",\"method\",\"sars\",\"virus\",\"design\",\"qualitative\",\"predictors\",\"government\",\"related\",\"student\",\"engagement\",\"national\",\"infected\",\"pneumonia\",\"editor\",\"pharmacy\",\"handling\",\"models\",\"safe\",\"india\",\"consequences\",\"society\",\"emerging\",\"nexus\",\"hydroxychloroquine\",\"adverse\",\"classification\",\"testing\",\"safety\",\"moderating\",\"psychosocial\",\"findings\",\"symptoms\",\"trials\",\"issues\",\"readiness\",\"distance\",\"influencing\",\"protection\",\"mechanisms\",\"training\",\"surgical\",\"techniques\",\"letter\",\"de\",\"fake\",\"en\",\"reality\",\"di\",\"hospitals\",\"investigation\",\"price\",\"oral\",\"communication\",\"country\",\"context\",\"markets\",\"interventions\",\"spike\",\"diabetes\",\"asia-pacific\",\"identification\",\"assessing\",\"science\",\"technologies\",\"iot\",\"educational\",\"malaysia√¢\",\"trends\",\"combating\",\"satisfaction\",\"sustainability\",\"pandemik\",\"service\",\"negative\",\"considerations\",\"action\",\"integrated\",\"pattern\",\"literature\",\"la\",\"position\",\"sleep\",\"protein\",\"hybrid\",\"covid√¢\",\"theory\",\"responsibility\",\"call\",\"middle-income\",\"tertiary\",\"comparative\",\"pacific\",\"sabah\",\"mitigating\",\"events\",\"nationwide\",\"protective\",\"secondary\",\"delivery\",\"age\",\"phase\",\"confirmed\",\"hospitalized\",\"saudi\",\"main\",\"mathematical\",\"business\",\"uk\",\"concerns\",\"natural\",\"mass\",\"telemedicine\",\"scenario\",\"wave\",\"daily\",\"frontline\",\"normal\",\"medicine\",\"applications\",\"learned\",\"quarantine\",\"forecasting\",\"roles\",\"predicting\",\"modeling\",\"building\",\"ace2\",\"attitudes\",\"antiviral\",\"mitigation\",\"simulation\",\"world\",\"reduce\",\"dynamic\",\"manifestations\",\"rate\",\"√∞¬µ\",\"mobile\",\"endoscopy\",\"genome\",\"type\",\"spatial\",\"marketing\",\"major\",\"structural\",\"fuzzy\",\"empirical\",\"aftermath\",\"combat\",\"behavioural\",\"outpatient\",\"affect\",\"suspected\",\"cardiovascular\",\"products\",\"policies\",\"destination\",\"examining\",\"smart\",\"properties\",\"bangladeshi\",\"arabia\",\"artificial\",\"randomized\",\"storm\",\"providers\",\"key\",\"death\",\"platform\",\"barriers\",\"school\",\"dan\",\"pulmonary\",\"province\",\"protocol\",\"activities\",\"supply\",\"index\",\"scale\",\"systemic\",\"times\",\"consensus\",\"contact\",\"medicines\",\"computational\",\"hiv\",\"laboratory\",\"corporate\",\"narrative\",\"link\",\"convolutional\",\"confinement\",\"usage\",\"statements\",\"body\",\"cloud\",\"influenza\",\"continuity\",\"disaster\",\"nursing\",\"aerosol\",\"levels\",\"religious\",\"silent\",\"proteins\",\"scoping\",\"united\",\"kingdom\",\"staff\",\"correction\",\"solidarity\",\"web-based\",\"versus\",\"chinese\",\"covid-19√¢\",\"success\",\"mutation\",\"individuals\",\"addressing\",\"detect\",\"uncertainty\",\"silico\",\"residents\",\"compliance\",\"test\",\"construction\",\"docking\",\"database\",\"climate\",\"animal\",\"automatic\",\"features\",\"conditions\",\"efficient\",\"networks\",\"cerebral\",\"venous\",\"cytokine\",\"teachers\",\"undergoing\",\"elective\",\"district\",\"income\",\"results\",\"plant\",\"surveillance\",\"pandemics\",\"detected\",\"asymptomatic\",\"male\",\"goals\",\"innovation\",\"receptor\",\"intervention\",\"employee\",\"wellbeing\",\"nigeria\",\"resources\",\"studies\",\"mediating\",\"aspects\",\"innovative\",\"green\",\"inflammatory\",\"practical\",\"setting\",\"immunity\",\"local\",\"injury\",\"transformation\",\"participation\",\"collaboration\",\"positive\",\"banking\",\"cluster\",\"remdesivir\",\"motivation\",\"statement\",\"pakistani\",\"neurosurgical\",\"sensing\",\"stability\",\"equipment\",\"estimation\",\"distancing\",\"host\",\"remote\",\"alternative\",\"guidelines\",\"improving\",\"asean\",\"war\",\"migrant\",\"patterns\",\"google\",\"comprehensive\",\"share\",\"stroke\",\"isolation\",\"collective\",\"hand\",\"promote\",\"reproductive\",\"longitudinal\",\"concurrent\",\"sectors\",\"detecting\",\"matter\",\"pm2.5\",\"change\",\"interactive\",\"antibody\",\"cells\",\"option\",\"reported\",\"plastic\",\"opportunity\",\"middle\",\"algorithms\",\"fresh\",\"saliva\",\"influences\",\"distribution\",\"gender\",\"project\",\"estimating\",\"descriptive\",\"mining\",\"emotion\",\"opinion\",\"content\",\"humans\",\"traditional\",\"copd\",\"thrombosis\",\"industrial\",\"angiotensin\",\"convalescent\",\"plasma\",\"fractal-fractional\",\"cohort\",\"mask\",\"emotional\",\"midst\",\"hesitancy\",\"density\",\"urban\",\"obese\",\"iran\",\"smes\",\"initiatives\",\"guide\",\"small-scale\",\"thailand\",\"italy\",\"essential\",\"trend\",\"treat\",\"scenarios\",\"efficacy\",\"measure\",\"scan\",\"nurses\",\"geriatric\",\"cell\",\"city\",\"preventing\",\"algorithm\",\"renin-angiotensin\",\"optimal\",\"facilities\",\"self-efficacy\",\"proposed\",\"return\",\"eating\",\"growth\",\"short-term\",\"lung\",\"situation\",\"tool\",\"bibliometric\",\"wake\",\"binding\",\"analisis\",\"disorder\",\"rheumatic\",\"restriction\",\"curve\",\"tracing\",\"institution\",\"update\",\"actions\",\"conspiracy\",\"theories\",\"hypertension\",\"intelligence\",\"semasa\",\"dalam\",\"direct\",\"paediatric\",\"gastroenterology\",\"platforms\",\"induced\",\"yemen\",\"targeted\",\"topsis\",\"risks\",\"predictive\",\"stressors\",\"therapeutics\",\"resistance\",\"aid\",\"leadership\",\"spectrum\",\"variations\",\"architecture\",\"pollution\",\"box\",\"responding\",\"efforts\",\"volatility\",\"silver\",\"poor\",\"conceptual\",\"healthy\",\"violence\",\"viral\",\"identify\",\"chain\",\"frailty\",\"thromboembolism\",\"klang\",\"valley\",\"persistent\",\"penang\",\"reactions\",\"asthma\",\"unprecedented\",\"leading\",\"trajectory\",\"engineering\",\"airway\",\"kuala\",\"lumpur\",\"selangor\",\"feature\",\"fight\",\"africa\",\"pandemic√¢\",\"belief\",\"thinking\",\"targets\",\"exposure\",\"meteorological\",\"robust\",\"reducing\",\"willingness\",\"classroom\",\"iraq\",\"√£\",\"solutions\",\"malay\",\"emergence\",\"common\",\"clustering\",\"√±∆í√±\",\"counselling\",\"inhaler\",\"lower\",\"radiotherapy\",\"cruise\",\"gynecological\",\"lupus\",\"brand\",\"fasting\",\"music\",\"pandemije\",\"gen\",\"shocks\",\"vitamin\",\"liver\",\"proposal\",\"upper\",\"universal\",\"coverage\",\"multiple\",\"populations\",\"covid-19-related\",\"function\",\"search\",\"types\",\"universities\",\"communications\",\"sri\",\"sexual\",\"sedentary\",\"ventilation\",\"private\",\"lives\",\"scans\",\"dexamethasone\",\"aged\",\"dataset\",\"nanomaterials\",\"cities\",\"wavelet-based\",\"cov-2\",\"reduction\",\"consumers\",\"buying\",\"survivors\",\"factor\",\"personal\",\"limited\",\"lifestyle\",\"admitted\",\"masks\",\"ongoing\",\"past\",\"panic\",\"rising\",\"infrastructure\",\"anti-sars-cov-2\",\"peptides\",\"mrna\",\"affected\",\"administration\",\"kits\",\"projects\",\"phytochemicals\",\"large-scale\",\"restrictions\",\"sentiment\",\"strains\",\"co2\",\"options\",\"provide\",\"solution\",\"supporting\",\"circular\",\"prophylaxis\",\"low\",\"outbreaks\",\"inhibitor\",\"controlled\",\"combined\",\"modulation\",\"sir\",\"derivative\",\"wastewater\",\"tocilizumab\",\"commentary\",\"planning\",\"perioperative\",\"illness\",\"college\",\"importance\",\"agricultural\",\"costs\",\"enhancing\",\"socioeconomic\",\"entrepreneurs\",\"peninsular\",\"australian\",\"norms\",\"paradigm\",\"entrepreneurial\",\"contagion\",\"integration\",\"disinfectant\",\"terhadap\",\"pengalaman\",\"improved\",\"road\",\"fluid\",\"sociodemographic\",\"process\",\"borneo\",\"lineage\",\"epidemiological\",\"weight\",\"controlling\",\"reproduction\",\"nurses√¢\",\"aquatic\",\"chains\",\"palliative\",\"technique\",\"europe\",\"burnout\",\"cross\",\"sectional\",\"emergencies\",\"preadmission\",\"disorders\",\"repurposing\",\"revolution\",\"vulnerable\",\"affecting\",\"examination\",\"implementing\",\"mixed-method\",\"statistical\",\"dentists\",\"pregnancy\",\"progression\",\"users\",\"focused\",\"qt\",\"caring\",\"precautionary\",\"nigerian\",\"tools\",\"drives\",\"free\",\"agenda\",\"prevent\",\"female\",\"metabolism\",\"nasopharyngeal\",\"converting\",\"enzyme\",\"manage\",\"institutional\",\"synthetic\",\"routine\",\"myths\",\"polymerase\",\"genetic\",\"herd\",\"wuhan\",\"special\",\"kidney\",\"sample\",\"possibly\",\"isolated\",\"quarantined\",\"augmented\",\"e-commerce\",\"antibiotics\",\"recurrent\",\"mini-review\",\"battling\",\"internet\",\"programme\",\"fever\",\"adult\",\"advance\",\"schools\",\"immunomodulatory\",\"dysfunction\",\"infectious\",\"english\",\"inquiry\",\"singapore\",\"firms\",\"postgraduate\",\"instant\",\"burden\",\"australia\",\"indicators\",\"intentions\",\"homes\",\"interactions\",\"indonesian\",\"aquaculture\",\"heparin\",\"concentration\",\"reporting\",\"insecurity\",\"happiness\",\"massive\",\"nutrition\",\"automated\",\"america\",\"journal\",\"urology\",\"observational\",\"dominant\",\"methodology\",\"determinants\",\"domestic\",\"blood\",\"referral\",\"temporal\",\"communities\",\"waves\",\"availability\",\"selected\",\"regions\",\"biological\",\"benefits\",\"finance\",\"health-care\",\"logistics\",\"deployment\",\"geographical\",\"critically\",\"ill\",\"south-east\",\"mini\",\"requirements\",\"exploration\",\"statins\",\"regulatory\",\"faculty\",\"correlation\",\"rights\",\"weather\",\"visual\",\"battle\",\"extraction\",\"respond\",\"lining\",\"robotic\",\"generated\",\"esl\",\"electronic\",\"compounds\",\"orientation\",\"pasaran\",\"faced\",\"na\",\"zinc\",\"putative\",\"ethical\",\"disinfection\",\"apps\",\"mindfulness\",\"malaysia's\",\"cascade\",\"orthopaedic\",\"contemporary\",\"sarawak\",\"manifestation\",\"assay\",\"worldwide\",\"target\",\"chloroquine\",\"pharmacologic\",\"agents\",\"cycle\",\"south\",\"corticosteroids\",\"corona\",\"mediated\",\"neurological\",\"reverse\",\"transcription\",\"amplification\",\"prophylactic\",\"reference\",\"multicenter\",\"azithromycin\",\"pharmacotherapeutic\",\"receiving\",\"al-quran\",\"expert\",\"plan\"],\"freq\":[259,198,193,144,143,135,134,132,128,122,113,100,99,93,90,90,88,84,80,80,79,76,76,75,74,74,71,70,69,68,68,68,68,68,68,68,67,67,66,65,63,63,63,62,61,61,61,58,58,57,57,56,56,55,53,53,53,53,52,51,51,49,49,49,49,49,49,49,48,48,48,47,47,45,45,44,44,44,44,44,43,43,42,42,41,41,40,40,40,40,40,40,40,40,38,38,38,38,38,37,37,37,37,37,37,37,37,36,36,36,36,35,35,35,35,35,34,34,34,34,34,34,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,31,31,31,31,31,31,31,31,31,31,31,31,30,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,28,28,28,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,25,25,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,23,23,23,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,19,19,19,19,19,19,19,19,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,16,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,14,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,11,11,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8],\"fontFamily\":\"Segoe UI\",\"fontWeight\":\"bold\",\"color\":\"random-dark\",\"minSize\":0,\"weightFactor\":0.694980694980695,\"backgroundColor\":\"white\",\"gridSize\":0,\"minRotation\":-0.785398163397448,\"maxRotation\":0.785398163397448,\"shuffle\":true,\"rotateRatio\":0.4,\"shape\":\"circle\",\"ellipticity\":0.65,\"figBase64\":null,\"hover\":null},\"evals\":[],\"jsHooks\":[]}\rFigure 1: Top 1000 terms extracted from the title\r\rwordcloud2(covid_wc$abstract %\u0026gt;% slice(1:1000) %\u0026gt;% mutate(frequency = round(frequency)))\r\r\r{\"x\":{\"word\":[\"patients\",\"students\",\"learning\",\"study\",\"health\",\"covid-19\",\"sars-cov-2\",\"pandemic\",\"anxiety\",\"online\",\"malaysia\",\"social\",\"data\",\"disease\",\"model\",\"countries\",\"mco\",\"research\",\"coronavirus\",\"impact\",\"healthcare\",\"control\",\"risk\",\"results\",\"analysis\",\"virus\",\"ci\",\"infection\",\"clinical\",\"care\",\"education\",\"knowledge\",\"stress\",\"measures\",\"public\",\"depression\",\"spread\",\"significant\",\"system\",\"outbreak\",\"information\",\"psychological\",\"factors\",\"world\",\"studies\",\"findings\",\"reported\",\"respondents\",\"media\",\"global\",\"medical\",\"vaccine\",\"lockdown\",\"paper\",\"participants\",\"transmission\",\"review\",\"positive\",\"mental\",\"people\",\"based\",\"economic\",\"severe\",\"symptoms\",\"management\",\"methods\",\"survey\",\"level\",\"current\",\"treatment\",\"food\",\"respiratory\",\"perceived\",\"due\",\"development\",\"performance\",\"total\",\"mortality\",\"quality\",\"challenges\",\"rights\",\"government\",\"including\",\"teaching\",\"strategies\",\"conducted\",\"services\",\"future\",\"time\",\"human\",\"approach\",\"workers\",\"method\",\"malaysian\",\"reserved\",\"crisis\",\"significantly\",\"effects\",\"support\",\"tourism\",\"period\",\"proposed\",\"movement\",\"design\",\"effect\",\"university\",\"potential\",\"compared\",\"rate\",\"abstract\",\"affected\",\"found\",\"score\",\"effective\",\"financial\",\"limited\",\"march\",\"author\",\"provide\",\"population\",\"acute\",\"technology\",\"china\",\"levels\",\"intention\",\"air\",\"activities\",\"detection\",\"syndrome\",\"caused\",\"aims\",\"related\",\"negative\",\"impacts\",\"literature\",\"physical\",\"questionnaire\",\"waste\",\"practice\",\"response\",\"market\",\"viral\",\"infected\",\"article\",\"relationship\",\"authors\",\"vaccines\",\"increased\",\"conclusion\",\"international\",\"e-learning\",\"worldwide\",\"practices\",\"patient\",\"role\",\"images\",\"test\",\"age\",\"industry\",\"islamic\",\"responses\",\"prevalence\",\"community\",\"implications\",\"perception\",\"process\",\"background\",\"cancer\",\"published\",\"coping\",\"digital\",\"distancing\",\"drugs\",\"developed\",\"systems\",\"country\",\"evidence\",\"sector\",\"business\",\"science\",\"individuals\",\"experience\",\"identify\",\"association\",\"policy\",\"terms\",\"hospital\",\"included\",\"reduce\",\"daily\",\"purpose\",\"increase\",\"identified\",\"understanding\",\"children\",\"life\",\"implementation\",\"society\",\"√¢\",\"situation\",\"stock\",\"models\",\"collected\",\"home\",\"access\",\"trials\",\"accuracy\",\"prevention\",\"april\",\"main\",\"lower\",\"investigate\",\"epidemic\",\"travel\",\"observed\",\"diseases\",\"attitude\",\"technologies\",\"revealed\",\"image\",\"personal\",\"aimed\",\"distress\",\"environment\",\"infections\",\"drug\",\"diagnosis\",\"result\",\"nature\",\"scale\",\"confirmed\",\"preventive\",\"issues\",\"acceptance\",\"communication\",\"objective\",\"safety\",\"resources\",\"switzerland\",\"virtual\",\"major\",\"testing\",\"outcomes\",\"performed\",\"variables\",\"articles\",\"phase\",\"developing\",\"framework\",\"provided\",\"key\",\"techniques\",\"deaths\",\"considered\",\"critical\",\"features\",\"rapid\",\"low\",\"screening\",\"women\",\"universities\",\"cross-sectional\",\"springer\",\"status\",\"asia\",\"environmental\",\"emergency\",\"majority\",\"activity\",\"hcws\",\"indonesia\",\"institutions\",\"severity\",\"policies\",\"protein\",\"assess\",\"fear\",\"days\",\"death\",\"licensee\",\"security\",\"news\",\"protective\",\"change\",\"examine\",\"mdpi\",\"basel\",\"national\",\"contact\",\"multiple\",\"obtained\",\"conditions\",\"aim\",\"distribution\",\"selected\",\"local\",\"conclusions\",\"prevent\",\"behaviour\",\"elsevier\",\"sample\",\"dental\",\"staff\",\"quarantine\",\"regression\",\"ensure\",\"hospitals\",\"engagement\",\"addition\",\"income\",\"reduction\",\"specific\",\"training\",\"interventions\",\"lack\",\"restrictions\",\"effectiveness\",\"characteristics\",\"satisfaction\",\"student\",\"improve\",\"application\",\"methodology\",\"stroke\",\"moderate\",\"december\",\"al\",\"ppe\",\"criteria\",\"network\",\"region\",\"organization\",\"importance\",\"growth\",\"energy\",\"pakistan\",\"teachers\",\"sustainable\",\"systematic\",\"scores\",\"recommendations\",\"academic\",\"economy\",\"million\",\"guidelines\",\"approaches\",\"strategy\",\"implemented\",\"vaccination\",\"recent\",\"evaluate\",\"adults\",\"family\",\"similar\",\"distributed\",\"equipment\",\"recovery\",\"correlation\",\"reduced\",\"binding\",\"body\",\"sharing\",\"researchers\",\"wuhan\",\"machine\",\"analyzed\",\"immune\",\"assessment\",\"awareness\",\"x-ray\",\"religious\",\"sensitivity\",\"globally\",\"explore\",\"ongoing\",\"led\",\"behavior\",\"publishing\",\"direct\",\"procedures\",\"theory\",\"rates\",\"determine\",\"develop\",\"resilience\",\"bangladesh\",\"exposure\",\"context\",\"factor\",\"focus\",\"understand\",\"pneumonia\",\"questions\",\"readiness\",\"well-being\",\"√Æ\",\"deep\",\"relevant\",\"required\",\"asian\",\"usage\",\"compounds\",\"differences\",\"odds\",\"january\",\"copyright\",\"threat\",\"covid\",\"aspects\",\"chest\",\"essential\",\"google\",\"delivery\",\"sustainability\",\"sectors\",\"affect\",\"monitoring\",\"primary\",\"qualitative\",\"ratio\",\"discussed\",\"ct\",\"hand\",\"antiviral\",\"common\",\"infectious\",\"perceptions\",\"existing\",\"suggest\",\"normal\",\"sampling\",\"professionals\",\"universiti\",\"concern\",\"solution\",\"search\",\"individual\",\"carried\",\"standard\",\"concerns\",\"assessed\",\"tested\",\"lives\",\"attitudes\",\"facilities\",\"rapidly\",\"quantitative\",\"service\",\"preparedness\",\"influence\",\"limitations\",\"presence\",\"confidence\",\"internet\",\"parameters\",\"applications\",\"index\",\"report\",\"databases\",\"ace2\",\"analyses\",\"efforts\",\"increasing\",\"type\",\"previous\",\"markets\",\"inhibitors\",\"unprecedented\",\"applied\",\"highly\",\"reports\",\"products\",\"wave\",\"structural\",\"compliance\",\"surgery\",\"crucial\",\"received\",\"faced\",\"source\",\"√¢ÀÜ\",\"production\",\"adequate\",\"providing\",\"impacted\",\"enhance\",\"illness\",\"therapy\",\"intervention\",\"educational\",\"governments\",\"dataset\",\"technique\",\"vulnerable\",\"spreading\",\"hydroxychloroquine\",\"platform\",\"molecular\",\"experiences\",\"day\",\"gender\",\"practical\",\"demonstrated\",\"laboratory\",\"faculty\",\"traditional\",\"attention\",\"address\",\"lead\",\"analyze\",\"surveillance\",\"students√¢\",\"recommended\",\"fake\",\"prediction\",\"set\",\"range\",\"dynamics\",\"mild\",\"examined\",\"measure\",\"investigated\",\"platforms\",\"poor\",\"increases\",\"pm2.5\",\"classification\",\"chain\",\"diagnostic\",\"pooled\",\"trend\",\"items\",\"involved\",\"emotional\",\"mobile\",\"samples\",\"objectives\",\"nurses\",\"tools\",\"mass\",\"size\",\"saliva\",\"qtc\",\"predict\",\"form\",\"uk\",\"play\",\"average\",\"male\",\"adverse\",\"tests\",\"statistical\",\"classes\",\"therapeutic\",\"licence\",\"taylor\",\"francis\",\"telemedicine\",\"wellbeing\",\"predicted\",\"evaluation\",\"types\",\"june\",\"shown\",\"active\",\"employees\",\"challenge\",\"employed\",\"trading\",\"india\",\"mitigate\",\"past\",\"patterns\",\"analysed\",\"affecting\",\"motivation\",\"consequences\",\"reality\",\"private\",\"providers\",\"opportunities\",\"issue\",\"managing\",\"medicine\",\"experienced\",\"confinement\",\"meta-analysis\",\"include\",\"causing\",\"supply\",\"scientific\",\"outcome\",\"events\",\"descriptive\",\"authorities\",\"consumers\",\"potentially\",\"introduction\",\"organizations\",\"risks\",\"interviews\",\"secondary\",\"demand\",\"sd\",\"times\",\"declared\",\"rna\",\"tool\",\"construction\",\"users\",\"apps\",\"resulted\",\"sleep\",\"stability\",\"completed\",\"safe\",\"burden\",\"action\",\"fever\",\"leading\",\"hygiene\",\"networks\",\"alternative\",\"sources\",\"outbreaks\",\"female\",\"suggested\",\"adjusted\",\"estimated\",\"llc\",\"finally\",\"specifically\",\"i.e\",\"months\",\"marketing\",\"emerged\",\"original\",\"disorders\",\"mechanisms\",\"ieee\",\"long-term\",\"solutions\",\"pollution\",\"distance\",\"decision\",\"globe\",\"benefits\",\"pattern\",\"temperature\",\"lung\",\"ict\",\"iot\",\"policymakers\",\"chronic\",\"reproduction\",\"central\",\"recently\",\"insights\",\"aor\",\"algorithm\",\"southeast\",\"difference\",\"isolation\",\"sars\",\"assay\",\"describe\",\"emergence\",\"content\",\"protection\",\"skills\",\"manage\",\"healthy\",\"originality\",\"adopted\",\"barriers\",\"designed\",\"february\",\"mitigation\",\"communities\",\"asymptomatic\",\"comprehensive\",\"oral\",\"highlights\",\"trust\",\"males\",\"continue\",\"affects\",\"humans\",\"engineering\",\"person\",\"prices\",\"fast\",\"pubmed\",\"english\",\"medium\",\"spike\",\"close\",\"prior\",\"reporting\",\"target\",\"regions\",\"short\",\"discuss\",\"penerbit\",\"africa\",\"host\",\"neural\",\"predictors\",\"basis\",\"females\",\"school\",\"natural\",\"institute\",\"week\",\"genome\",\"emerging\",\"pandemics\",\"east\",\"aerosol\",\"suspected\",\"conventional\",\"specificity\",\"achieve\",\"odl\",\"unique\",\"version\",\"detected\",\"investment\",\"guide\",\"disruption\",\"press\",\"median\",\"burnout\",\"availability\",\"collection\",\"emerald\",\"informa\",\"e.g\",\"basic\",\"city\",\"ability\",\"require\",\"planning\",\"creative\",\"capacity\",\"cells\",\"cell\",\"independent\",\"wiley\",\"dentists\",\"epidemiological\",\"self-efficacy\",\"effectively\",\"price\",\"values\",\"proteins\",\"behavioural\",\"surgical\",\"journal\",\"facing\",\"protease\",\"medicines\",\"coverage\",\"field\",\"reducing\",\"oil\",\"condition\",\"evaluated\",\"real-time\",\"contribute\",\"highlight\",\"receiving\",\"partial\",\"efficacy\",\"living\",\"proper\",\"remdesivir\",\"remains\",\"studied\",\"statistics\",\"reviewed\",\"cognitive\",\"hypertension\",\"sites\",\"aged\",\"fight\",\"exclusive\",\"inclusion\",\"immunity\",\"complications\",\"banking\",\"additional\",\"detect\",\"building\",\"consumption\",\"assist\",\"efficient\",\"improvement\",\"coronaviruses\",\"incidence\",\"united\",\"job\",\"estimate\",\"properties\",\"no2\",\"behavioral\",\"expected\",\"improved\",\"doctors\",\"destination\",\"modelling\",\"electronic\",\"involving\",\"simulation\",\"morbidity\",\"chinese\",\"database\",\"initial\",\"widely\",\"face-to-face\",\"integrated\",\"diabetes\",\"logistic\",\"decrease\",\"versus\",\"interval\",\"web\",\"demographic\",\"phases\",\"date\",\"influenza\",\"achieved\",\"actions\",\"additionally\",\"tracing\",\"smart\",\"europe\",\"contagious\",\"middle\",\"perspective\",\"climate\",\"swab\",\"language\",\"linear\",\"treat\",\"interaction\",\"interactions\",\"zakat\",\"health-care\",\"eating\",\"resulting\",\"relationships\",\"questionnaires\",\"discussion\",\"license\",\"spatial\",\"ministry\",\"vital\",\"discusses\",\"recorded\",\"usefulness\",\"programs\",\"american\",\"depressive\",\"materials\",\"strong\",\"modified\",\"cost\",\"explored\",\"random\",\"weeks\",\"adoption\",\"comorbidities\",\"ml\",\"algorithms\",\"comparison\",\"improving\",\"established\",\"structure\",\"function\",\"physicians\",\"degree\",\"entry\",\"highlighted\",\"reaction\",\"singapore\",\"infrastructure\",\"complex\",\"suitable\",\"section\",\"demonstrate\",\"sciences\",\"challenging\",\"generated\",\"decreased\",\"south\",\"masks\",\"selection\",\"hajj\",\"rt-pcr\",\"single\",\"proportion\",\"b.v\",\"equation\",\"ai\",\"series\",\"feature\",\"mechanism\",\"believed\",\"accurate\",\"scopus\",\"reliability\",\"p√¢\",\"lt\",\"measured\",\"history\",\"finding\",\"liver\",\"companies\",\"sars-cov\",\"nigeria\",\"receptor\",\"frontline\",\"combination\",\"software\",\"determined\",\"urgent\",\"returns\",\"participated\",\"acid\",\"post-covid-19\",\"psychosocial\",\"successful\",\"empirical\",\"directly\",\"real\",\"spss\",\"duration\",\"plan\",\"beginning\",\"covid-19-related\",\"success\",\"blood\",\"personnel\",\"remain\",\"imposed\",\"created\",\"examines\",\"requires\",\"called\",\"commons\",\"focused\",\"final\",\"water\",\"periods\",\"advanced\",\"utilized\",\"addressing\",\"damage\",\"citizens\",\"plasma\",\"amount\",\"influenced\",\"curve\",\"curb\",\"discovered\",\"icu\",\"treatments\",\"nations\",\"optimal\",\"urological\",\"neurosurgical\",\"rf-ssa\",\"trends\",\"frequency\",\"italy\",\"stakeholders\",\"reliable\",\"handling\",\"enhanced\",\"august\",\"agents\",\"viruses\",\"perspectives\",\"emotion\",\"negatively\",\"introduced\",\"analyse\",\"residents\",\"adult\",\"corona\",\"approved\",\"nursing\",\"thinking\",\"forced\",\"complete\",\"suggests\",\"populations\",\"emissions\",\"economies\"],\"freq\":[634,569,561,539,526,483,481,447,443,441,433,417,396,394,388,374,367,355,353,352,343,336,335,334,332,331,330,329,322,321,319,314,311,310,302,300,299,299,296,295,294,293,292,290,290,288,288,285,284,282,280,279,279,276,273,272,271,271,269,268,268,265,264,261,260,258,257,257,252,252,252,250,248,248,248,248,242,241,241,241,237,232,232,231,230,230,230,228,228,227,226,226,225,225,224,224,223,221,221,219,215,215,214,214,214,213,211,211,211,210,209,208,206,204,204,202,199,199,198,197,196,196,196,196,195,195,194,194,194,192,192,192,191,190,190,190,189,189,187,186,186,185,185,185,183,182,179,179,178,178,177,176,176,176,175,175,172,172,172,171,169,169,169,168,168,167,167,165,165,165,164,164,164,162,162,161,161,161,160,160,159,159,159,159,158,158,157,157,157,156,155,155,154,154,154,154,154,153,153,153,152,152,152,152,152,151,151,151,151,150,150,149,149,149,149,148,148,148,148,148,147,147,147,146,146,145,144,144,144,143,142,142,142,142,142,141,141,141,141,140,140,140,140,140,139,139,139,139,138,138,138,138,138,137,137,137,137,137,136,136,136,135,135,134,134,133,133,133,133,133,132,132,131,131,131,131,130,130,130,129,128,128,128,128,127,127,127,127,126,126,126,126,125,125,125,125,125,124,124,124,123,123,123,123,122,122,122,121,121,121,120,120,120,120,120,119,119,119,119,119,118,118,118,118,118,118,118,117,116,116,116,116,116,115,115,115,115,115,115,115,114,114,114,114,114,114,113,113,113,113,112,112,112,112,112,112,111,111,111,111,111,111,111,111,111,110,110,110,110,110,110,109,109,109,109,109,109,108,108,108,108,107,107,107,107,107,107,107,107,106,106,106,105,105,105,105,105,105,105,105,105,105,105,105,105,104,104,104,104,104,104,103,103,103,102,102,102,102,102,102,102,102,101,101,101,101,101,101,101,101,101,101,101,101,101,100,99,99,99,99,99,99,99,98,98,98,98,98,98,98,98,97,97,97,97,97,97,97,97,97,96,96,96,95,95,95,95,95,95,95,95,95,95,95,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,93,93,93,93,93,93,93,92,92,92,92,92,92,92,92,91,91,91,91,91,91,91,91,91,91,90,90,90,90,90,90,90,90,90,90,90,90,89,89,89,89,89,89,88,88,88,88,88,88,88,88,88,88,88,88,87,87,87,87,87,87,87,87,86,86,86,86,86,86,86,86,86,86,86,86,86,85,85,85,85,85,85,85,84,84,84,84,84,84,84,84,84,84,84,84,84,83,83,83,83,83,83,83,83,82,82,82,82,82,82,82,82,82,81,81,81,81,81,81,81,81,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,79,79,79,79,79,79,79,79,78,78,78,78,78,78,78,78,77,77,77,77,77,77,77,77,77,77,77,76,76,76,76,76,76,76,76,76,76,76,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,74,74,74,74,74,74,74,74,74,74,74,74,73,73,73,73,73,73,73,73,73,73,73,72,72,72,72,72,72,72,72,72,72,72,72,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,69,69,69,69,69,69,69,69,69,69,69,69,69,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,63,63,63,63,63,63,63,63,63,63,63,63,63,63,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,56,56,56,56,56,56],\"fontFamily\":\"Segoe UI\",\"fontWeight\":\"bold\",\"color\":\"random-dark\",\"minSize\":0,\"weightFactor\":0.28391167192429,\"backgroundColor\":\"white\",\"gridSize\":0,\"minRotation\":-0.785398163397448,\"maxRotation\":0.785398163397448,\"shuffle\":true,\"rotateRatio\":0.4,\"shape\":\"circle\",\"ellipticity\":0.65,\"figBase64\":null,\"hover\":null},\"evals\":[],\"jsHooks\":[]}\rFigure 2: Top 1000 terms extracted from the abstract\r\rwordcloud2(covid_wc$author_keywords%\u0026gt;% slice(1:1000) %\u0026gt;% mutate(frequency = round(frequency)))\r\r\r{\"x\":{\"word\":[\"covid-19\",\"pandemic\",\"learning\",\"coronavirus\",\"health\",\"sars-cov-2\",\"malaysia\",\"social\",\"online\",\"education\",\"disease\",\"analysis\",\"control\",\"anxiety\",\"technology\",\"students\",\"teaching\",\"mental\",\"movement\",\"model\",\"public\",\"management\",\"media\",\"stress\",\"healthcare\",\"lockdown\",\"machine\",\"psychological\",\"quality\",\"risk\",\"medical\",\"food\",\"policy\",\"system\",\"depression\",\"vaccine\",\"respiratory\",\"care\",\"university\",\"impact\",\"clinical\",\"deep\",\"knowledge\",\"economic\",\"virus\",\"diseases\",\"tourism\",\"digital\",\"neural\",\"network\",\"theory\",\"waste\",\"development\",\"islamic\",\"image\",\"epidemic\",\"e-learning\",\"mortality\",\"performance\",\"infectious\",\"sustainable\",\"workers\",\"covid\",\"syndrome\",\"artificial\",\"intention\",\"antiviral\",\"drug\",\"asia\",\"transmission\",\"practice\",\"infection\",\"global\",\"index\",\"perception\",\"air\",\"security\",\"acceptance\",\"medicine\",\"stock\",\"distance\",\"distancing\",\"information\",\"pneumonia\",\"communication\",\"resilience\",\"screening\",\"acute\",\"perceived\",\"attitude\",\"virtual\",\"outbreak\",\"sars\",\"sustainability\",\"data\",\"crisis\",\"pollution\",\"community\",\"measures\",\"fear\",\"review\",\"protective\",\"support\",\"behavior\",\"emergency\",\"response\",\"financial\",\"student\",\"systems\",\"epidemiology\",\"life\",\"quarantine\",\"prevention\",\"industry\",\"intelligence\",\"forecasting\",\"smart\",\"coping\",\"pandemics\",\"drugs\",\"x-ray\",\"travel\",\"detection\",\"survey\",\"cancer\",\"monitoring\",\"bangladesh\",\"hydroxychloroquine\",\"physical\",\"pakistan\",\"immunity\",\"molecular\",\"decision\",\"equipment\",\"services\",\"news\",\"distress\",\"price\",\"service\",\"factors\",\"chest\",\"destination\",\"human\",\"satisfaction\",\"research\",\"hospital\",\"regression\",\"diagnosis\",\"vaccines\",\"modelling\",\"simulation\",\"behaviour\",\"covid19\",\"mco\",\"networks\",\"transfer\",\"supply\",\"chain\",\"environmental\",\"therapy\",\"corona\",\"rapid\",\"government\",\"personal\",\"stability\",\"southeast\",\"home\",\"market\",\"internet\",\"motivation\",\"fake\",\"optimization\",\"neurosurgery\",\"events\",\"energy\",\"surgery\",\"children\",\"change\",\"hiv\",\"assessment\",\"safety\",\"population\",\"strategy\",\"protection\",\"treatment\",\"international\",\"immune\",\"activity\",\"environment\",\"vaccination\",\"docking\",\"2019-ncov\",\"women\",\"engagement\",\"severe\",\"challenges\",\"diabetes\",\"mobile\",\"project\",\"spike\",\"markets\",\"adverse\",\"convolutional\",\"perceptions\",\"protein\",\"protease\",\"sharing\",\"plasma\",\"storm\",\"disorders\",\"educational\",\"volatility\",\"oil\",\"countries\",\"ct\",\"reproduction\",\"climate\",\"impacts\",\"well-being\",\"mass\",\"preventive\",\"mathematical\",\"readiness\",\"worker\",\"people\",\"policies\",\"school\",\"indonesia\",\"diagnostic\",\"resources\",\"nigeria\",\"images\",\"ace2\",\"approach\",\"systematic\",\"infections\",\"nutrition\",\"rna\",\"cytokine\",\"business\",\"fuzzy\",\"pharmacy\",\"prediction\",\"pharmacists\",\"classification\",\"academic\",\"adults\",\"china\",\"wavelet\",\"economy\",\"test\",\"intervention\",\"saudi\",\"arabia\",\"herd\",\"critical\",\"method\",\"computed\",\"tomography\",\"viral\",\"dental\",\"awareness\",\"preparedness\",\"status\",\"application\",\"trend\",\"forest\",\"finance\",\"rate\",\"testing\",\"strategies\",\"africa\",\"google\",\"brand\",\"iran\",\"literacy\",\"loss\",\"nervous\",\"hand\",\"hygiene\",\"cytokines\",\"training\",\"therapeutics\",\"body\",\"indices\",\"construction\",\"stroke\",\"convalescent\",\"ict\",\"modeling\",\"patients\",\"reality\",\"responsibility\",\"equity\",\"gold\",\"misinformation\",\"pedagogy\",\"meta-analysis\",\"thinking\",\"recovery\",\"experience\",\"hesitancy\",\"sir\",\"malaysian\",\"multi-criteria\",\"industrial\",\"delivery\",\"derivative\",\"numerical\",\"national\",\"cov\",\"interaction\",\"world\",\"repurposing\",\"innovation\",\"biomarkers\",\"vector\",\"wellbeing\",\"angiotensin-converting\",\"enzyme\",\"contact\",\"growth\",\"pathology\",\"mining\",\"chloroquine\",\"design\",\"smes\",\"telemedicine\",\"characteristics\",\"oral\",\"influenza\",\"employee\",\"fractional\",\"algorithm\",\"revolution\",\"otolaryngology\",\"angiotensin\",\"leadership\",\"privacy\",\"iomt\",\"country\",\"convolution\",\"staff\",\"isolation\",\"consumption\",\"cardiovascular\",\"sars-cov2\",\"aids\",\"sensitivity\",\"generation\",\"laboratory\",\"imaging\",\"basic\",\"sentiment\",\"psychosocial\",\"feature\",\"cnn\",\"pulmonary\",\"stimulus\",\"inflammatory\",\"anaesthesia\",\"religious\",\"secondary\",\"evolution\",\"inhibitors\",\"factor\",\"aid\",\"normal\",\"remdesivir\",\"spectrum\",\"random\",\"study\",\"receptor\",\"sciences\",\"multiple\",\"symptoms\",\"banking\",\"nanoparticles\",\"curve\",\"practices\",\"action\",\"topsis\",\"english\",\"asean\",\"corporate\",\"conservation\",\"outcome\",\"urology\",\"carbon\",\"remote\",\"cross-sectional\",\"seir\",\"entropy\",\"opportunity\",\"hearing\",\"visual\",\"spillover\",\"type\",\"mask\",\"airline\",\"wastewater\",\"covid√¢\",\"trade\",\"science\",\"perspective\",\"frailty\",\"app\",\"psychology\",\"discourse\",\"customer\",\"organizational\",\"text\",\"music\",\"belief\",\"sequence\",\"middle-income\",\"bitcoin\",\"investment\",\"male\",\"panel\",\"circular\",\"cell\",\"wave\",\"gender\",\"plastic\",\"cloud\",\"severity\",\"uncertainty\",\"real-time\",\"infrastructure\",\"sensors\",\"electronic\",\"content\",\"co2\",\"collaboration\",\"chronic\",\"venous\",\"immunotherapy\",\"fractal-fractional\",\"fisheries\",\"aerosols\",\"trial\",\"occupational\",\"density\",\"income\",\"entrepreneurial\",\"surveillance\",\"institutions\",\"natural\",\"vulnerability\",\"linear\",\"structural\",\"engineering\",\"mindfulness\",\"features\",\"mitigation\",\"reproductive\",\"aquaculture\",\"biosensor\",\"asia-pacific\",\"adaptive\",\"disinfection\",\"eating\",\"methods\",\"iot\",\"favipiravir\",\"recurrent\",\"singular\",\"vulnerable\",\"integration\",\"interventions\",\"k-nearest\",\"vision\",\"economics\",\"pregnancy\",\"fatality\",\"tracing\",\"guidelines\",\"models\",\"conspiracy\",\"nasopharyngeal\",\"augmented\",\"antibody\",\"results\",\"mutation\",\"health-care\",\"shopping\",\"lifestyle\",\"resistance\",\"attitudes\",\"availability\",\"emissions\",\"arima\",\"medicines\",\"family\",\"returns\",\"behavioural\",\"governance\",\"tool\",\"language\",\"clinic\",\"qualitative\",\"orientation\",\"insecurity\",\"emerging\",\"zoonotic\",\"logistic\",\"sars-cov\",\"zinc\",\"rises\",\"cov-2\",\"cognitive\",\"experiences\",\"teachers\",\"classroom\",\"correlation\",\"dynamic\",\"ann\",\"professionals\",\"geographical\",\"thromboembolism\",\"coronaviruses\",\"dentistry\",\"endoscopy\",\"clustering\",\"proteomics\",\"sources\",\"computing\",\"tree\",\"planning\",\"poverty\",\"sales\",\"jakarta\",\"efficiency\",\"operator\",\"systemic\",\"gut\",\"chart\",\"water\",\"exercise\",\"gastrointestinal\",\"ecological\",\"binary\",\"selection\",\"form\",\"rights\",\"wildlife\",\"continuity\",\"green\",\"upper\",\"pacific\",\"function\",\"infertility\",\"semen\",\"concern\",\"collaborative\",\"azithromycin\",\"buying\",\"usage\",\"surface\",\"product\",\"scarcity\",\"adoption\",\"sequencing\",\"accessibility\",\"mers-cov\",\"biomarker\",\"processing\",\"aviation\",\"rt-pcr\",\"polymerase\",\"variants\",\"adolescents\",\"cycle\",\"panic\",\"module\",\"domestic\",\"emission\",\"pm2.5\",\"lightweight\",\"non-pharmaceutical\",\"dynamics\",\"twitter\",\"opinion\",\"animal\",\"cerebral\",\"thrombosis\",\"tools\",\"enterprises\",\"sector\",\"existence\",\"adams-bashforth\",\"ab\",\"package\",\"bias\",\"emotional\",\"city\",\"illness\",\"law\",\"window\",\"decision-making\",\"process\",\"droplets\",\"scan\",\"cells\",\"inflammation\",\"physics\",\"biosensors\",\"segmentation\",\"airway\",\"sedentary\",\"weight\",\"active\",\"matrix\",\"optimal\",\"nurses\",\"chains\",\"palliative\",\"capacity\",\"techniques\",\"coherence\",\"burnout\",\"evaluation\",\"devices\",\"surveys\",\"questionnaires\",\"psychiatry\",\"organization\",\"ivermectin\",\"robot\",\"goals\",\"sdg\",\"behavioral\",\"rational\",\"phytochemicals\",\"neighbor\",\"matter\",\"blood\",\"nucleocapsid\",\"workplace\",\"inhibitor\",\"consensus\",\"particulate\",\"limited\",\"migrant\",\"silver\",\"job\",\"pay\",\"marketing\",\"technologies\",\"lstm\",\"institution\",\"chinese\",\"facilities\",\"saliva\",\"dysfunction\",\"hypertension\",\"thailand\",\"coverage\",\"immunoassay\",\"targeted\",\"agents\",\"lung\",\"genetic\",\"yemen\",\"parameters\",\"integrated\",\"addiction\",\"opioid\",\"substance\",\"stressors\",\"apps\",\"behaviors\",\"effectiveness\",\"trust\",\"sarawak\",\"tb\",\"stigma\",\"taiwan\",\"mhealth\",\"utaut2\",\"emotion\",\"spatial\",\"pollutants\",\"rural\",\"singapore\",\"exchange\",\"utaut\",\"principles\",\"humanitarian\",\"disaster\",\"fiscal\",\"barriers\",\"self-efficacy\",\"pattern\",\"relationship\",\"trials\",\"employment\",\"inclusion\",\"contagion\",\"asthma\",\"happiness\",\"alternative\",\"death\",\"ppe\",\"condition\",\"mcdm\",\"violence\",\"simulations\",\"temporal\",\"users\",\"coronavirus-2\",\"shortages\",\"india\",\"literature\",\"logistics\",\"d-dimer\",\"fintech\",\"sabah\",\"unemployment\",\"issues\",\"ground-glass\",\"regulatory\",\"exposure\",\"spread\",\"forecast\",\"hospitality\",\"willingness\",\"agency\",\"x-rays\",\"esl\",\"pathogenesis\",\"low\",\"student√¢\",\"ethical\",\"consumer\",\"injury\",\"delay\",\"aerosol\",\"efficacy\",\"habits\",\"gamification\",\"resource\",\"local\",\"monetary\",\"oxidative\",\"scale\",\"ncov\",\"tract\",\"ethics\",\"fractal\",\"complexity\",\"covid-\",\"studies\",\"mellitus\",\"divide\",\"wallet\",\"software\",\"disinfectant\",\"mosque\",\"post-acute\",\"graph\",\"health-promoting\",\"structures\",\"cruise\",\"haematology\",\"t-cell\",\"kidney\",\"aedes\",\"microbiome\",\"aec\",\"anatomy\",\"framing\",\"atrial\",\"kit\",\"absolute\",\"shrinkage\",\"lasso\",\"ultrasound\",\"fracture\",\"expectancy\",\"peptides\",\"website\",\"liver\",\"corticosteroid\",\"motility\",\"space\",\"referees\",\"structure\",\"publication\",\"ensemble\",\"commodities\",\"sexual\",\"sri\",\"lanka\",\"behaviours\",\"outdoors\",\"play\",\"cytomegalovirus\",\"ministry\",\"private\",\"fcv-19s\",\"infodemiology\",\"diploma\",\"production\",\"pls-sem\",\"conventional\",\"outpatient\",\"entry\",\"hajj\",\"graphene\",\"value-added\",\"purchasing\",\"cost\",\"platform\",\"epilepsy\",\"turkey\",\"japan\",\"nanomaterials\",\"fossil\",\"fuel\",\"peptide\",\"bioinformatics\",\"descriptive\",\"child\",\"thoracic\",\"communications\",\"reliability\",\"validity\",\"antigen\",\"foreign\",\"main\",\"restrictions\",\"repair\",\"success\",\"projects\",\"coagulopathy\",\"immunomodulatory\",\"obstructive\",\"immunomodulation\",\"procedures\",\"department\",\"spacer\",\"loneliness\",\"bayesian\",\"size\",\"reactions\",\"crowding\",\"culture\",\"disinfectants\",\"effects\",\"entrepreneurs\",\"coastal\",\"therapeutic\",\"biomedical\",\"balance\",\"cross-cultural\",\"empathy\",\"individualism\",\"power\",\"multilevel\",\"equation\",\"paediatric\",\"responses\",\"equilibrium\",\"adaptation\",\"relations\",\"electrochemical\",\"region\",\"seafood\",\"prices\",\"handwashing\",\"planned\",\"mixed\",\"taste\",\"road\",\"transport\",\"technical\",\"video\",\"geofencing\",\"location\",\"tracking\",\"andrology\",\"adult\",\"capitalists\",\"handling\",\"mutual\",\"assistance\",\"advantages\",\"eigentriples\",\"length\",\"fourth\",\"adolescent\",\"long-covid\",\"phenoconversion\",\"correlations\",\"non-rational\",\"self-isolation\",\"synthetic\",\"affect\",\"bibliometric\",\"optical\",\"zoonosis\",\"blended\",\"cluster\",\"nsp15\",\"phase\",\"prognostic\",\"indicators\",\"post\",\"dentists\",\"goal\",\"promotion\",\"set\",\"guidance\",\"nanomedicine\",\"reinfection\",\"sirs\",\"qtc\",\"prolongation\",\"commitment\",\"trends\",\"recognition\",\"measurement\",\"firm\",\"intelligent\",\"marine\",\"agricultural\",\"regulation\",\"anti-covid-19\",\"traditional\",\"e-government\",\"reasoned\",\"theories\",\"b40\",\"household\",\"disposal\",\"metabolic\",\"swab\",\"medicinal\",\"plants\",\"converting\",\"turnover\",\"lmics\",\"quantitative\",\"binding\",\"comparative\",\"flavonoid\",\"electrocardiogram\",\"prolonged\",\"susceptibility\",\"dining\",\"experiencescape\",\"female\",\"travelers\",\"compliance\",\"wuhan\",\"mpro\",\"layer\",\"myanmar\",\"prioritisation\",\"serological\",\"lupus\",\"erythematosus\",\"harm\",\"reduction\",\"agonist\",\"disorder\",\"borneo\",\"sociodemographic\",\"wellness\",\"migration\",\"database\",\"arm\",\"arduino\",\"nano\",\"confinement\",\"kap\",\"asymptomatic\",\"mann-kendall\",\"rf\",\"ssa\",\"anthropogenic\",\"aquatic\",\"tuberculosis\",\"peritraumatic\",\"operation\",\"blockchain\",\"integrity\",\"particle\",\"swarm\",\"domain\",\"oropharyngeal\",\"smell\",\"capital\",\"sensorineural\",\"s-o-r\",\"renal\",\"failure\",\"transfusion\"],\"freq\":[225,191,189,179,175,166,119,114,114,103,94,83,77,74,72,70,68,66,65,65,63,63,61,58,58,57,57,56,56,56,55,55,54,54,53,52,50,49,49,49,49,49,48,47,47,46,46,45,45,45,45,45,44,44,44,43,43,43,42,42,41,41,41,41,38,38,37,37,37,37,37,36,36,36,35,35,35,34,34,34,34,34,33,33,33,33,33,33,33,32,32,32,32,32,32,32,32,31,31,31,31,31,31,31,31,31,30,30,30,29,29,29,29,29,29,29,28,28,27,27,27,27,27,27,27,26,26,26,26,26,26,26,26,26,26,25,25,25,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,23,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,21,21,21,21,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,16,16,16,16,16,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,14,14,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,12,12,12,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\"fontFamily\":\"Segoe UI\",\"fontWeight\":\"bold\",\"color\":\"random-dark\",\"minSize\":0,\"weightFactor\":0.8,\"backgroundColor\":\"white\",\"gridSize\":0,\"minRotation\":-0.785398163397448,\"maxRotation\":0.785398163397448,\"shuffle\":true,\"rotateRatio\":0.4,\"shape\":\"circle\",\"ellipticity\":0.65,\"figBase64\":null,\"hover\":null},\"evals\":[],\"jsHooks\":[]}\rFigure 3: Top 1000 terms extracted from the author‚Äôs keywords\r\rwordcloud2(covid_wc$index_keywords%\u0026gt;% slice(1:1000) %\u0026gt;% mutate(frequency = round(frequency)))\r\r\r{\"x\":{\"word\":[\"health\",\"disease\",\"coronavirus\",\"care\",\"virus\",\"adult\",\"drug\",\"study\",\"aged\",\"pneumonia\",\"infection\",\"male\",\"female\",\"malaysia\",\"betacoronavirus\",\"covid-19\",\"risk\",\"pandemic\",\"control\",\"human\",\"syndrome\",\"respiratory\",\"clinical\",\"humans\",\"analysis\",\"social\",\"middle\",\"article\",\"protein\",\"sars-cov-2\",\"learning\",\"acute\",\"pandemics\",\"personnel\",\"viral\",\"cross-sectional\",\"mental\",\"transmission\",\"agent\",\"patient\",\"severe\",\"management\",\"hospital\",\"factor\",\"system\",\"infections\",\"waste\",\"blood\",\"medical\",\"anxiety\",\"education\",\"therapy\",\"reaction\",\"public\",\"priority\",\"journal\",\"chain\",\"mortality\",\"review\",\"stress\",\"questionnaire\",\"assessment\",\"polymerase\",\"prevention\",\"studies\",\"angiotensin\",\"child\",\"epidemiology\",\"air\",\"epidemic\",\"time\",\"adolescent\",\"controlled\",\"practice\",\"enzyme\",\"acid\",\"procedures\",\"cell\",\"diagnosis\",\"interleukin\",\"tomography\",\"quality\",\"receptor\",\"organization\",\"severity\",\"quarantine\",\"lung\",\"letter\",\"china\",\"depression\",\"nonhuman\",\"outcome\",\"vaccine\",\"surveys\",\"asia\",\"global\",\"behavior\",\"data\",\"pollution\",\"factors\",\"research\",\"environmental\",\"surgery\",\"rate\",\"topic\",\"binding\",\"virology\",\"isolation\",\"computer\",\"communicable\",\"attitude\",\"major\",\"diagnostic\",\"information\",\"psychology\",\"disorder\",\"hydroxychloroquine\",\"decision\",\"reverse\",\"treatment\",\"occupational\",\"monitoring\",\"psychological\",\"scale\",\"activity\",\"economic\",\"systems\",\"safety\",\"screening\",\"equipment\",\"protective\",\"transcription\",\"service\",\"detection\",\"complication\",\"prevalence\",\"media\",\"united\",\"cancer\",\"policy\",\"inhibitor\",\"immunity\",\"antiviral\",\"food\",\"pakistan\",\"trial\",\"questionnaires\",\"diabetes\",\"emergency\",\"molecular\",\"rna\",\"testing\",\"mellitus\",\"computed\",\"student\",\"immune\",\"population\",\"comorbidity\",\"cytokine\",\"artificial\",\"status\",\"model\",\"agents\",\"neural\",\"machine\",\"fever\",\"antibody\",\"laboratory\",\"sensitivity\",\"viruses\",\"vaccination\",\"age\",\"perception\",\"distress\",\"brain\",\"x-ray\",\"image\",\"intensive\",\"retrospective\",\"test\",\"hypertension\",\"converting\",\"diseases\",\"government\",\"world\",\"spike\",\"immunoglobulin\",\"chronic\",\"delivery\",\"failure\",\"techniques\",\"mass\",\"cost\",\"survey\",\"development\",\"networks\",\"hospitalization\",\"guideline\",\"ventilation\",\"income\",\"distancing\",\"kidney\",\"support\",\"response\",\"liver\",\"internet\",\"effect\",\"knowledge\",\"impact\",\"lopinavir\",\"tract\",\"real\",\"interferon\",\"ritonavir\",\"azithromycin\",\"radiography\",\"personal\",\"online\",\"life\",\"dipeptidyl\",\"antivirus\",\"students\",\"asymptomatic\",\"genetic\",\"east\",\"exposure\",\"body\",\"thrombosis\",\"lymphocyte\",\"hand\",\"telemedicine\",\"deep\",\"specificity\",\"purification\",\"international\",\"teaching\",\"illness\",\"elderly\",\"level\",\"sars\",\"coughing\",\"imaging\",\"examination\",\"sex\",\"assisted\",\"remdesivir\",\"systematic\",\"planning\",\"bangladesh\",\"university\",\"sleep\",\"cardiovascular\",\"europe\",\"contact\",\"medicine\",\"infectious\",\"heart\",\"gene\",\"glycoprotein\",\"metabolism\",\"communication\",\"models\",\"tumor\",\"africa\",\"chloroquine\",\"carboxypeptidase\",\"thorax\",\"indonesia\",\"country\",\"mutation\",\"physical\",\"immunodeficiency\",\"unit\",\"elective\",\"feature\",\"animal\",\"influenza\",\"insulin\",\"energy\",\"oxygen\",\"industry\",\"distance\",\"incidence\",\"injury\",\"sequence\",\"meta\",\"extract\",\"association\",\"community\",\"surgical\",\"structure\",\"vaccines\",\"classification\",\"forecasting\",\"services\",\"thromboembolism\",\"report\",\"corticosteroid\",\"design\",\"comparative\",\"genetics\",\"economics\",\"death\",\"antagonist\",\"gastrointestinal\",\"distribution\",\"low\",\"particulate\",\"matter\",\"index\",\"interaction\",\"unclassified\",\"accuracy\",\"venous\",\"cohort\",\"simulation\",\"pressure\",\"symptom\",\"biological\",\"technology\",\"aerosol\",\"note\",\"physiology\",\"coronaviruses\",\"movement\",\"network\",\"statistical\",\"heparin\",\"hepatitis\",\"environment\",\"derivative\",\"pregnancy\",\"asthma\",\"dynamics\",\"physician\",\"efficacy\",\"lockdown\",\"pathogenicity\",\"weight\",\"randomized\",\"training\",\"necrosis\",\"home\",\"method\",\"dyspnea\",\"immunology\",\"genome\",\"educational\",\"sustainable\",\"newborn\",\"animals\",\"plasma\",\"release\",\"antigen\",\"change\",\"facility\",\"surveillance\",\"obesity\",\"mobile\",\"intelligence\",\"aspect\",\"water\",\"construction\",\"function\",\"access\",\"singapore\",\"industrial\",\"adverse\",\"replication\",\"vitamin\",\"algorithm\",\"fear\",\"cerebrovascular\",\"plastic\",\"washing\",\"follow\",\"swab\",\"approach\",\"expression\",\"throat\",\"multiple\",\"disorders\",\"process\",\"methods\",\"endoscopy\",\"immunization\",\"antibiotic\",\"load\",\"infant\",\"nasopharynx\",\"developing\",\"count\",\"satisfaction\",\"kingdom\",\"outbreaks\",\"pathophysiology\",\"coping\",\"fatigue\",\"nucleic\",\"disinfection\",\"pain\",\"performance\",\"prediction\",\"compliance\",\"awareness\",\"angiotensin-converting\",\"inhibitors\",\"qualitative\",\"critical\",\"professional\",\"tocilizumab\",\"difference\",\"reduction\",\"multicenter\",\"anticoagulant\",\"prognosis\",\"geographic\",\"engineering\",\"north\",\"diarrhea\",\"algorithms\",\"amino\",\"innate\",\"production\",\"coronary\",\"pharmacy\",\"dioxide\",\"dependent\",\"phylogeny\",\"e-learning\",\"malaysian\",\"reactive\",\"socioeconomics\",\"favipiravir\",\"school\",\"computing\",\"activities\",\"nucleocapsid\",\"prospective\",\"repositioning\",\"cooperation\",\"dna\",\"preschool\",\"processing\",\"nasopharyngeal\",\"amplification\",\"consensus\",\"nitrogen\",\"variation\",\"entry\",\"south\",\"job\",\"carbon\",\"transfusion\",\"digital\",\"adaptive\",\"mathematical\",\"nose\",\"loss\",\"procedure\",\"effectiveness\",\"culture\",\"supply\",\"proteins\",\"travel\",\"participation\",\"tuberculosis\",\"america\",\"alanine\",\"attitudes\",\"chemistry\",\"demography\",\"fatality\",\"dexamethasone\",\"antiinflammatory\",\"convolutional\",\"hospitals\",\"spread\",\"technique\",\"workforce\",\"workplace\",\"security\",\"administration\",\"clustering\",\"aminotransferase\",\"phase\",\"resilience\",\"tertiary\",\"cross\",\"based\",\"quantitative\",\"storm\",\"chinese\",\"measurement\",\"handling\",\"obstructive\",\"reproduction\",\"spatial\",\"southeast\",\"sector\",\"correlation\",\"local\",\"pathology\",\"financial\",\"neoplasm\",\"inflammation\",\"nucleotide\",\"italy\",\"enhancement\",\"sequencing\",\"hemorrhage\",\"intelligent\",\"headache\",\"pyrolysis\",\"interview\",\"protease\",\"plant\",\"survival\",\"clotting\",\"regression\",\"anesthesia\",\"preoperative\",\"sustainability\",\"poverty\",\"temperature\",\"artery\",\"nurse\",\"burden\",\"hearing\",\"literature\",\"software\",\"socioeconomic\",\"applications\",\"deficiency\",\"ward\",\"household\",\"signal\",\"glucose\",\"zinc\",\"neurosurgery\",\"admission\",\"patterns\",\"event\",\"exacerbation\",\"morbidity\",\"application\",\"kinase\",\"predictive\",\"docking\",\"site\",\"biology\",\"complement\",\"proteinase\",\"taiwan\",\"cluster\",\"employment\",\"well-being\",\"devices\",\"gamma\",\"hygiene\",\"evaluation\",\"type\",\"worker\",\"neoplasms\",\"methodology\",\"mechanism\",\"assay\",\"nursing\",\"oil\",\"utilization\",\"scoring\",\"effects\",\"ribavirin\",\"dimer\",\"contamination\",\"tourism\",\"theory\",\"epidemiological\",\"transport\",\"basic\",\"disposal\",\"specimen\",\"stroke\",\"chest\",\"renin\",\"family\",\"vomiting\",\"rating\",\"host\",\"tissue\",\"length\",\"stay\",\"india\",\"staff\",\"videoconferencing\",\"infarction\",\"infertility\",\"metformin\",\"bilirubin\",\"myalgia\",\"resource\",\"conditions\",\"wellbeing\",\"editorial\",\"density\",\"qt\",\"resistance\",\"longitudinal\",\"smoking\",\"sexual\",\"sinus\",\"turkey\",\"malaria\",\"fibrinolytic\",\"thailand\",\"antibodies\",\"dose\",\"intervention\",\"organ\",\"frailty\",\"fuzzy\",\"vector\",\"computerized\",\"aldosterone\",\"extraction\",\"shedding\",\"coronavirinae\",\"radiation\",\"intake\",\"validity\",\"intubation\",\"climate\",\"experiment\",\"hyperglycemia\",\"workload\",\"crowding\",\"mixed\",\"reductase\",\"islam\",\"exercise\",\"oropharynx\",\"rural\",\"postoperative\",\"accident\",\"spatiotemporal\",\"primary\",\"single\",\"observational\",\"nausea\",\"nonstructural\",\"domestic\",\"pacific\",\"monoclonal\",\"japan\",\"passive\",\"republic\",\"adenosine\",\"hemorrhagic\",\"immunosuppressive\",\"ambulatory\",\"healthcare\",\"acceptance\",\"short\",\"emotional\",\"gender\",\"interactions\",\"immunomodulation\",\"trend\",\"macrophage\",\"modeling\",\"center\",\"asian\",\"smear\",\"critically\",\"ill\",\"strategy\",\"standard\",\"outpatient\",\"real-time\",\"anosmia\",\"rhinorrhea\",\"physicians\",\"guidelines\",\"tests\",\"behavioral\",\"cerebral\",\"remote\",\"functional\",\"brazil\",\"herd\",\"embolism\",\"iran\",\"mining\",\"sperm\",\"marketing\",\"epilepsy\",\"immunoassay\",\"biomarkers\",\"promotion\",\"emotion\",\"period\",\"ratio\",\"testis\",\"fractional\",\"spain\",\"antihypertensive\",\"bioinformatics\",\"carrier\",\"1beta\",\"physiological\",\"immunotherapy\",\"vein\",\"west\",\"protection\",\"score\",\"methotrexate\",\"predisposition\",\"atmospheric\",\"consumption\",\"secondary\",\"evidence\",\"serine\",\"muscle\",\"sore\",\"oseltamivir\",\"alpha\",\"beta\",\"lactate\",\"dehydrogenase\",\"arterial\",\"spectrometry\",\"glycemic\",\"burnout\",\"operating\",\"adrenal\",\"hemoglobin\",\"dissemination\",\"spectroscopy\",\"taste\",\"aortic\",\"selection\",\"nervous\",\"dizziness\",\"differential\",\"science\",\"western\",\"concept\",\"intention\",\"database\",\"activation\",\"structural\",\"reproducibility\",\"ncov\",\"laryngoscopy\",\"nanoparticle\",\"bayes\",\"theorem\",\"rheumatic\",\"nanomedicine\",\"competence\",\"frail\",\"violence\",\"ethnic\",\"concentration\",\"preventive\",\"referral\",\"fatty\",\"lavage\",\"transfer\",\"lifestyle\",\"philippines\",\"sneezing\",\"antimalarial\",\"pollutant\",\"motivation\",\"urban\",\"wuhan\",\"pharmacist\",\"fluid\",\"bulgaria\",\"deafness\",\"valve\",\"publication\",\"universities\",\"infectivity\",\"linked\",\"immunosorbent\",\"search\",\"measures\",\"adaptation\",\"structured\",\"trials\",\"size\",\"product\",\"experience\",\"inventory\",\"marker\",\"methylprednisolone\",\"religion\",\"azathioprine\",\"regulation\",\"inflammatory\",\"virtual\",\"consultation\",\"medium\",\"patient-to-professional\",\"medication\",\"recycling\",\"square\",\"palliative\",\"maternal\",\"essential\",\"ischemia\",\"lupus\",\"combination\",\"esophagus\",\"workflow\",\"urology\",\"oral\",\"language\",\"line\",\"cycle\",\"emission\",\"disaster\",\"department\",\"series\",\"history\",\"natural\",\"numerical\",\"gas\",\"particle\",\"australia\",\"transduction\",\"feeding\",\"oxidative\",\"immunocompromised\",\"falciparum\",\"electrochemical\",\"degradation\",\"incineration\",\"morbid\",\"vertigo\",\"abuse\",\"endoscopic\",\"solid\",\"hiv\",\"chains\",\"vulnerable\",\"error\",\"uncertainty\",\"viet\",\"nam\",\"sulfur\",\"convolution\",\"recombinant\",\"parameters\",\"histogram\",\"organizational\",\"program\",\"saudi\",\"arabia\",\"commerce\",\"sampling\",\"occupation\",\"glucocorticoid\",\"risks\",\"iot\",\"teleconsultation\",\"serology\",\"tools\",\"sites\",\"growth\",\"capacity\",\"peptide\",\"bronchoscopy\",\"results\",\"endotracheal\",\"availability\",\"shortage\",\"mask\",\"countries\",\"electronic\",\"placebo\",\"pathogenesis\",\"renin-angiotensin\",\"platforms\",\"affinity\",\"newcastle-ottawa\",\"crisis\",\"chemical\",\"validation\",\"stigma\",\"bleeding\",\"conceptual\",\"pathogen\",\"computational\",\"shop\",\"peptidyl-dipeptidase\",\"malignant\",\"anticoagulants\",\"pharmaceutical\",\"sanitizer\",\"municipal\",\"radiology\",\"architecture\",\"nepal\",\"erythematosus\",\"intestine\",\"toxicity\",\"microbiology\",\"tenofovir\",\"linear\",\"ireland\",\"genital\",\"yemen\",\"immunologic\",\"nigeria\",\"folic\",\"private\",\"biosensing\",\"korea\",\"iv\",\"dental\",\"obstruction\",\"monoxide\",\"politics\",\"humoral\",\"systemic\",\"dietary\",\"germany\",\"toll\",\"business\",\"cardiac\",\"sample\",\"dengue\",\"transplantation\",\"creatinine\",\"entropy\",\"ii\",\"impedance\",\"mapping\",\"power\",\"fracture\",\"people\",\"thoracic\",\"partial\",\"sensing\",\"framework\",\"cognitive\",\"insurance\",\"person\",\"psychosocial\",\"logistic\",\"cultural\",\"inhibition\",\"nearest\",\"myanmar\",\"membrane\",\"strategic\",\"condition\",\"content\",\"traffic\",\"virulence\",\"diet\",\"cd4\",\"tachycardia\",\"ferritin\",\"respiration\",\"comparison\",\"storage\",\"oxygenation\",\"cd8\",\"networking\",\"market\",\"automatic\",\"current\",\"coefficient\",\"ethnicity\",\"transmissions\",\"mitigation\",\"interpersonal\",\"misinformation\",\"oxides\",\"emissions\",\"pulmonary\",\"baricitinib\",\"convalescent\",\"erythrocyte\",\"metabolic\",\"role\",\"protocol\",\"sectors\",\"antiinfective\",\"umifenovir\",\"resources\",\"monocyte\",\"phenomena\",\"southeastern\",\"society\",\"shock\"],\"freq\":[674,530,524,418,377,371,363,362,359,349,335,318,311,310,309,305,295,290,290,287,273,264,262,260,255,250,246,242,237,236,228,223,220,219,218,217,209,201,197,197,194,189,182,181,177,176,174,170,165,164,162,160,159,159,156,156,156,155,155,155,154,153,153,152,147,146,145,144,141,139,137,137,136,136,135,134,133,133,131,130,129,127,127,126,125,124,123,123,123,123,121,120,120,118,115,113,113,112,111,110,109,109,109,108,108,108,108,106,106,106,105,105,105,105,105,104,102,102,102,102,101,100,100,100,100,100,99,98,98,97,97,97,97,96,96,95,95,95,94,94,94,92,92,92,92,91,91,91,90,89,89,88,88,88,88,88,88,87,86,86,86,86,86,86,85,85,85,84,84,82,82,82,82,81,81,80,80,80,80,79,79,78,78,78,77,77,77,77,77,76,76,76,76,76,75,75,74,73,73,72,71,71,71,70,70,70,70,69,68,68,68,68,67,67,67,67,67,66,66,65,65,65,65,65,65,65,65,65,64,64,64,64,64,63,63,63,63,62,62,62,62,62,62,62,62,62,61,61,61,61,61,61,61,61,61,61,61,61,60,60,59,59,59,59,58,57,57,57,57,57,56,56,56,56,56,56,56,55,55,54,54,54,54,54,54,54,54,53,53,53,53,53,52,52,52,52,52,52,51,51,51,51,51,50,50,50,50,50,50,50,50,49,49,49,49,49,49,49,49,49,48,48,48,48,48,48,48,48,48,48,47,47,47,47,47,47,47,47,46,46,46,45,45,45,45,45,45,45,45,45,45,45,45,45,45,44,44,44,44,44,44,44,44,44,44,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,42,42,42,42,42,42,42,42,42,42,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,40,40,40,40,40,40,40,40,40,39,39,39,39,39,39,39,39,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,37,37,37,37,37,37,37,37,36,36,36,36,36,36,36,36,36,36,36,36,36,36,35,35,35,35,35,35,35,35,35,35,35,35,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,30,30,30,30,30,30,30,30,30,30,30,30,30,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,26,26,26,26,26,26,26,26,26,26,26,26,26,26,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17],\"fontFamily\":\"Segoe UI\",\"fontWeight\":\"bold\",\"color\":\"random-dark\",\"minSize\":0,\"weightFactor\":0.267062314540059,\"backgroundColor\":\"white\",\"gridSize\":0,\"minRotation\":-0.785398163397448,\"maxRotation\":0.785398163397448,\"shuffle\":true,\"rotateRatio\":0.4,\"shape\":\"circle\",\"ellipticity\":0.65,\"figBase64\":null,\"hover\":null},\"evals\":[],\"jsHooks\":[]}\rFigure 4: Top 1000 terms extracted from the Scopus‚Äôs keywords\r\rThere are some weird symbols in the plot and the wordcloud, it‚Äôs better remove to it. However, I am to lazy to remove it, so I will leave it üòÉ.\n\rConclusion\rThese are some of the explorative text analysis that can be done. These relevant terms may provide some insight to our current research of COVID-19 in Malaysia. However, by no means its fully reflect our current COVID-19 research.\n\r","date":1631318400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631329730,"objectID":"f00c476f18c3b2f12465c6458e26e871","permalink":"https://tengkuhanis.netlify.app/post/wordcloud-of-covid-19-research-in-malaysia/","publishdate":"2021-09-11T00:00:00Z","relpermalink":"/post/wordcloud-of-covid-19-research-in-malaysia/","section":"post","summary":"Let‚Äôs see how much research has been done in term of COVID-19 in Malaysia. In this analysis, we are going to use Scopus database to access the relevant research or papers.","tags":["text analysis","wordcloud","COVID-19"],"title":"Wordcloud of COVID-19 research in Malaysia","type":"post"},{"authors":[],"categories":["R","Machine Learning"],"content":"\r\rThis post will not go very detail in each of the approach of hyperparameter tuning. This post mainly aims to summarize a few things that I studied for the last couple of days.\rGenerally, there are two approaches to hyperparameter tuning in tidymodels.\nGrid search:\n‚Äì Regular grid search\n‚Äì Random grid search\n\rIterative search:\n‚Äì Bayesian optimization\n‚Äì Simulated annealing\r\rGrid search\rSo, in grid search, we provide the combination of parameters and the algorithm will go through each combination of parameters. There are two types of grid search:\n\rRegular grid search\n‚Äì The algorithm will go through each combinations of parameters.\r\rgrid_regular(mtry(c(1, 13)), trees(), min_n(),\rlevels = 3) # how many from each parameter\r## # A tibble: 27 x 3\r## mtry trees min_n\r## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 1 1 2\r## 2 7 1 2\r## 3 13 1 2\r## 4 1 1000 2\r## 5 7 1000 2\r## 6 13 1000 2\r## 7 1 2000 2\r## 8 7 2000 2\r## 9 13 2000 2\r## 10 1 1 21\r## # ... with 17 more rows\r\rRandom grid search\n‚Äì The algorithm will randomly select a number of combination of parameters instead of go through each of them.\r\rgrid_random(mtry(c(1, 13)),\rtrees(), min_n(), size = 100) # size of parameters combination\r## # A tibble: 100 x 3\r## mtry trees min_n\r## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 5 1216 40\r## 2 8 1374 13\r## 3 9 859 39\r## 4 6 282 12\r## 5 2 1210 9\r## 6 8 1828 39\r## 7 11 550 14\r## 8 13 1157 32\r## 9 5 282 6\r## 10 10 1018 28\r## # ... with 90 more rows\rBy default, tidymodels uses space-filling-design to make sure the combination of parameters are on ‚Äúequidistance‚Äù to each other.\n\rIterative search\rIn iterative search, we need to specify some initial parameters/values to start the search.\n\rBayesian optimization\n‚Äì This algorithm/function will search the next best combination of parameters based on the previous combination of parameters (priori).\rSimulated annealing\n‚Äì Generally, this algorithm works relatively similar to bayesian optimization.\n‚Äì However, as the figure below illustrates this algorithm is able to explore in the worst combination of parameters for a short term (barrier of local search), in order to find the best combination of parameters (global minima).\r\r\rFuther details on iterative search or both methods above can be found here. So, as both iterative methods need a starting parameters, we can actually combine with any of the grid search methods.\n\rOther methods\rBy default, if we do not supply any combination of parameters, tidymodels will randomly pick 10 combinations of parameters from the default range of values from the model. Additionally, we can set this values to other values as shown below:\ntune_grid(\rresamples = dat_cv, # cross validation data set\rgrid = 20, # 20 combinations of parameters\rcontrol = control, # some control parameters\rmetrics = metrics # some metrics parameters (roc_auc, etc)\r)\rThere are another special cases of grid search; tune_race_anova() and tune_race_win_loss(). Both of these methods supposed to be more efficient way of grid search. In general, both methods evaluate the tuning parameters on a small initial set. The combination of parameters with a worst performance will be eliminated. Thus, makes them more efficient in grid search. The main difference between these two methods is how the worst combination of parameters are evaluated and eliminated.\n\rR codes\rLoad the packages.\n# Packages\rlibrary(tidyverse)\rlibrary(tidymodels)\rlibrary(finetune)\rWe will only use a small chunk of the data for ease of computation.\n# Data\rdata(income, package = \u0026quot;kernlab\u0026quot;)\r# Make data smaller for computation\rset.seed(2021)\rincome2 \u0026lt;- income %\u0026gt;% filter(INCOME == \u0026quot;[75.000-\u0026quot; | INCOME == \u0026quot;[50.000-75.000)\u0026quot;) %\u0026gt;% slice_sample(n = 600) %\u0026gt;% mutate(INCOME = fct_drop(INCOME), INCOME = fct_recode(INCOME, rich = \u0026quot;[75.000-\u0026quot;,\rless_rich = \u0026quot;[50.000-75.000)\u0026quot;), INCOME = factor(INCOME, ordered = F)) %\u0026gt;% mutate(across(-INCOME, fct_drop))\r# Summary of data\rglimpse(income2)\r## Rows: 600\r## Columns: 14\r## $ INCOME \u0026lt;fct\u0026gt; less_rich, rich, rich, rich, less_rich, rich, rich, les~\r## $ SEX \u0026lt;fct\u0026gt; F, M, F, M, F, F, F, M, F, M, M, M, F, F, F, F, M, M, M~\r## $ MARITAL.STATUS \u0026lt;fct\u0026gt; Married, Married, Married, Single, Single, NA, Married,~\r## $ AGE \u0026lt;ord\u0026gt; 35-44, 25-34, 45-54, 18-24, 18-24, 14-17, 25-34, 25-34,~\r## $ EDUCATION \u0026lt;ord\u0026gt; 1 to 3 years of college, Grad Study, College graduate, ~\r## $ OCCUPATION \u0026lt;fct\u0026gt; \u0026quot;Professional/Managerial\u0026quot;, \u0026quot;Professional/Managerial\u0026quot;, \u0026quot;~\r## $ AREA \u0026lt;ord\u0026gt; 10+ years, 7-10 years, 10+ years, -1 year, 4-6 years, 7~\r## $ DUAL.INCOMES \u0026lt;fct\u0026gt; Yes, Yes, Yes, Not Married, Not Married, Not Married, N~\r## $ HOUSEHOLD.SIZE \u0026lt;ord\u0026gt; Five, Two, Four, Two, Four, Two, Three, Two, Five, One,~\r## $ UNDER18 \u0026lt;ord\u0026gt; Three, None, None, None, None, None, One, None, Three, ~\r## $ HOUSEHOLDER \u0026lt;fct\u0026gt; Own, Own, Own, Rent, Family, Own, Own, Rent, Own, Own, ~\r## $ HOME.TYPE \u0026lt;fct\u0026gt; House, House, House, House, House, Apartment, House, Ho~\r## $ ETHNIC.CLASS \u0026lt;fct\u0026gt; White, White, White, White, White, White, White, White,~\r## $ LANGUAGE \u0026lt;fct\u0026gt; English, English, English, English, English, NA, Englis~\r# Outcome variable\rtable(income2$INCOME)\r## ## less_rich rich ## 362 238\r# Missing data\rDataExplorer::plot_missing(income)\rSplit the data and create a 10-fold cross validation.\nset.seed(2021)\rdat_index \u0026lt;- initial_split(income2, strata = INCOME)\rdat_train \u0026lt;- training(dat_index)\rdat_test \u0026lt;- testing(dat_index)\r## CV\rset.seed(2021)\rdat_cv \u0026lt;- vfold_cv(dat_train, v = 10, repeats = 1, strata = INCOME)\rWe going to impute the NAs with mode value since all the variable are categorical.\n# Recipe\rdat_rec \u0026lt;- recipe(INCOME ~ ., data = dat_train) %\u0026gt;% step_impute_mode(all_predictors()) %\u0026gt;% step_ordinalscore(AGE, EDUCATION, AREA, HOUSEHOLD.SIZE, UNDER18)\r# Model\rrf_mod \u0026lt;- rand_forest(mtry = tune(),\rtrees = tune(),\rmin_n = tune()) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;)\r# Workflow\rrf_wf \u0026lt;- workflow() %\u0026gt;% add_recipe(dat_rec) %\u0026gt;% add_model(rf_mod)\rParameters for grid search\n# Regular grid\rreg_grid \u0026lt;- grid_regular(mtry(c(1, 13)), trees(), min_n(), levels = 3)\r# Random grid\rrand_grid \u0026lt;- grid_random(mtry(c(1, 13)), trees(), min_n(), size = 100)\rTune models using regular grid search. We going to use doParallel library to do parallel processing.\nctrl \u0026lt;- control_grid(save_pred = T,\rextract = extract_model)\rmeasure \u0026lt;- metric_set(roc_auc) # Parallel for regular grid\rlibrary(doParallel)\r# Create a cluster object and then register: cl \u0026lt;- makePSOCKcluster(4)\rregisterDoParallel(cl)\r# Run tune\rset.seed(2021)\rtune_regular \u0026lt;- rf_wf %\u0026gt;% tune_grid(\rresamples = dat_cv, grid = reg_grid, control = ctrl, metrics = measure)\rstopCluster(cl)\rResult for regular grid search:\nautoplot(tune_regular)\rshow_best(tune_regular)\r## # A tibble: 5 x 9\r## mtry trees min_n .metric .estimator mean n std_err .config ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 7 1000 21 roc_auc binary 0.690 10 0.0148 Preprocessor1_Model14\r## 2 7 1000 40 roc_auc binary 0.689 10 0.0179 Preprocessor1_Model23\r## 3 7 2000 40 roc_auc binary 0.689 10 0.0178 Preprocessor1_Model26\r## 4 7 1000 2 roc_auc binary 0.688 10 0.0173 Preprocessor1_Model05\r## 5 7 2000 21 roc_auc binary 0.688 10 0.0159 Preprocessor1_Model17\rTune models using random grid search.\n# Parallel for random grid\r# Create a cluster object and then register: cl \u0026lt;- makePSOCKcluster(4)\rregisterDoParallel(cl)\r# Run tune\rset.seed(2021)\rtune_random \u0026lt;- rf_wf %\u0026gt;% tune_grid(\rresamples = dat_cv, grid = rand_grid, control = ctrl, metrics = measure)\rstopCluster(cl)\rResult for random grid search:\nautoplot(tune_random)\rshow_best(tune_random)\r## # A tibble: 5 x 9\r## mtry trees min_n .metric .estimator mean n std_err .config ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 4 1016 4 roc_auc binary 0.694 10 0.0164 Preprocessor1_Model0~\r## 2 5 1360 3 roc_auc binary 0.693 10 0.0168 Preprocessor1_Model0~\r## 3 6 129 14 roc_auc binary 0.693 10 0.0164 Preprocessor1_Model0~\r## 4 5 1235 3 roc_auc binary 0.692 10 0.0168 Preprocessor1_Model0~\r## 5 6 160 31 roc_auc binary 0.692 10 0.0172 Preprocessor1_Model0~\rRandom grid search has slightly a better result. Let‚Äôs use this random search result as a base for iterative search. Firstly, we limit the parameters based on the plot from a random grid search.\nrf_param \u0026lt;- rf_wf %\u0026gt;% parameters() %\u0026gt;% update(mtry = mtry(c(5, 13)), trees = trees(c(1, 500)), min_n = min_n(c(5, 30)))\rNow we do a bayesian optimization.\n# Parallel for bayesian optimization\r# Create a cluster object and then register: cl \u0026lt;- makePSOCKcluster(4)\rregisterDoParallel(cl)\r# Run tune\rset.seed(2021)\rbayes_tune \u0026lt;- rf_wf %\u0026gt;% tune_bayes( resamples = dat_cv,\rparam_info = rf_param,\riter = 60,\rinitial = tune_random, # result from random grid search control = control_bayes(no_improve = 30, verbose = T, save_pred = T), metrics = measure)\rstopCluster(cl)\rResult for bayesian optimization.\nautoplot(bayes_tune, \u0026quot;performance\u0026quot;)\rshow_best(bayes_tune)\r## # A tibble: 5 x 10\r## mtry trees min_n .metric .estimator mean n std_err .config .iter\r## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 4 1016 4 roc_auc binary 0.694 10 0.0164 Preprocessor1_~ 0\r## 2 5 1360 3 roc_auc binary 0.693 10 0.0168 Preprocessor1_~ 0\r## 3 6 129 14 roc_auc binary 0.693 10 0.0164 Preprocessor1_~ 0\r## 4 6 189 15 roc_auc binary 0.693 10 0.0153 Iter1 1\r## 5 5 1235 3 roc_auc binary 0.692 10 0.0168 Preprocessor1_~ 0\rWe get a slightly better result from bayesian optimization. I will not do a simulated annealing approach since I get an error, though I am not sure why.\nLastly, we do a race anova.\n# Parallel for race anova\r# Create a cluster object and then register: cl \u0026lt;- makePSOCKcluster(4)\rregisterDoParallel(cl)\r# Run tune\rset.seed(2021)\rtune_efficient \u0026lt;- rf_wf %\u0026gt;% tune_race_anova(\rresamples = dat_cv, grid = rand_grid, control = control_race(verbose_elim = T, save_pred = T), metrics = measure)\rstopCluster(cl)\rWe get a relatively similar result to random grid search but with faster computation.\nautoplot(tune_efficient)\rshow_best(tune_efficient)\r## # A tibble: 5 x 9\r## mtry trees min_n .metric .estimator mean n std_err .config ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 5 1425 5 roc_auc binary 0.695 10 0.0161 Preprocessor1_Model0~\r## 2 11 406 2 roc_auc binary 0.694 10 0.0183 Preprocessor1_Model0~\r## 3 6 631 3 roc_auc binary 0.692 10 0.0171 Preprocessor1_Model0~\r## 4 7 1264 4 roc_auc binary 0.692 10 0.0159 Preprocessor1_Model0~\r## 5 9 1264 3 roc_auc binary 0.692 10 0.0188 Preprocessor1_Model0~\rWe can also compare ROCs of all approaches. All approaches looks more or less similar.\r\r\rShow code\r\r# regular grid\rrf_reg \u0026lt;- tune_regular %\u0026gt;% select_best(metric = \u0026quot;roc_auc\u0026quot;)\rreg_auc \u0026lt;- tune_regular %\u0026gt;% collect_predictions(parameters = rf_reg) %\u0026gt;% roc_curve(INCOME, .pred_less_rich) %\u0026gt;% mutate(model = \u0026quot;regular_grid\u0026quot;)\r# random grid\rrf_rand \u0026lt;- tune_random %\u0026gt;% select_best(metric = \u0026quot;roc_auc\u0026quot;)\rrand_auc \u0026lt;- tune_random %\u0026gt;% collect_predictions(parameters = rf_rand) %\u0026gt;% roc_curve(INCOME, .pred_less_rich) %\u0026gt;% mutate(model = \u0026quot;random_grid\u0026quot;)\r# bayes\rrf_bayes \u0026lt;- bayes_tune %\u0026gt;% select_best(metric = \u0026quot;roc_auc\u0026quot;)\rbayes_auc \u0026lt;- bayes_tune %\u0026gt;% collect_predictions(parameters = rf_bayes) %\u0026gt;% roc_curve(INCOME, .pred_less_rich) %\u0026gt;% mutate(model = \u0026quot;bayes\u0026quot;)\r# race_anova\rrf_eff \u0026lt;- tune_efficient %\u0026gt;% select_best(metric = \u0026quot;roc_auc\u0026quot;)\reff_auc \u0026lt;- tune_efficient %\u0026gt;% collect_predictions(parameters = rf_eff) %\u0026gt;%\rroc_curve(INCOME, .pred_less_rich) %\u0026gt;% mutate(model = \u0026quot;race_anova\u0026quot;)\r# Compare ROC between all tuning approach\rbind_rows(reg_auc, rand_auc, bayes_auc, eff_auc) %\u0026gt;% ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + geom_path(lwd = 1.5, alpha = 0.8) +\rgeom_abline(lty = 3) + coord_equal() + scale_color_viridis_d(option = \u0026quot;plasma\u0026quot;, end = .6) +\rtheme_bw()\r\rFinally, we fit our best model (bayesian optimization) to the testing data.\n# Finalize workflow\rbest_rf \u0026lt;-\rselect_best(bayes_tune, \u0026quot;roc_auc\u0026quot;)\rfinal_wf \u0026lt;- rf_wf %\u0026gt;% finalize_workflow(best_rf)\rfinal_wf\r## == Workflow ====================================================================\r## Preprocessor: Recipe\r## Model: rand_forest()\r## ## -- Preprocessor ----------------------------------------------------------------\r## 2 Recipe Steps\r## ## * step_impute_mode()\r## * step_ordinalscore()\r## ## -- Model -----------------------------------------------------------------------\r## Random Forest Model Specification (classification)\r## ## Main Arguments:\r## mtry = 4\r## trees = 1016\r## min_n = 4\r## ## Computational engine: ranger\r# Last fit\rtest_fit \u0026lt;- final_wf %\u0026gt;%\rlast_fit(dat_index) # Evaluation metrics test_fit %\u0026gt;%\rcollect_metrics()\r## # A tibble: 2 x 4\r## .metric .estimator .estimate .config ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 accuracy binary 0.583 Preprocessor1_Model1\r## 2 roc_auc binary 0.611 Preprocessor1_Model1\rtest_fit %\u0026gt;%\rcollect_predictions() %\u0026gt;% roc_curve(INCOME, .pred_less_rich) %\u0026gt;% autoplot()\r\rConclusion\rThe result is not that good. Our AUC is quite lower. However, we did use only about 8% from the overall data. Nonetheless, the aim of this post is to cover an overview of hyperparameter tuning in tidymodels.\nAdditionally, there are another two function to construct parameter grids that I did not cover in this post; grid_max_entropy() and grid_latin_hypercube(). Both of these functions do not have much resources explaining them (or at least I did not found it), however, for those interested, a good start will be the tidymodels website.\nReferences:\nhttps://www.tmwr.org/grid-search.html\nhttps://www.tmwr.org/iterative-search.html\nhttps://oliviergimenez.github.io/learning-machine-learning/#\nhttps://towardsdatascience.com/optimization-techniques-simulated-annealing-d6a4785a1de7\n\r","date":1630800000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630824332,"objectID":"8110dc6d9444708148250c7c56beef54","permalink":"https://tengkuhanis.netlify.app/post/hyperparameter-tuning-in-tidymodels/","publishdate":"2021-09-05T00:00:00Z","relpermalink":"/post/hyperparameter-tuning-in-tidymodels/","section":"post","summary":"This post will not go very detail in each of the approach of hyperparameter tuning. This post mainly aims to summarize a few things that I studied for the last couple of days.","tags":["tidymodels"],"title":"Hyperparameter tuning in tidymodels","type":"post"},{"authors":null,"categories":["R"],"content":"\r\rThese are some of the packages that I find useful for data exploration. Basically, this post serves more as my note for future reference. I will list out packages (and some awesome functions from that particular package) rather than specific functions. Further, base R and tidyverse packages will not be included specifically in this list.\nLoad supporting packages\nlibrary(tidyverse)\rThe data we are going to use is from dlookr package:\nglimpse(heartfailure)\r## Rows: 299\r## Columns: 13\r## $ age \u0026lt;int\u0026gt; 75, 55, 65, 50, 65, 90, 75, 60, 65, 80, 75, 62, 45, ~\r## $ anaemia \u0026lt;fct\u0026gt; No, No, No, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, N~\r## $ cpk_enzyme \u0026lt;dbl\u0026gt; 582, 7861, 146, 111, 160, 47, 246, 315, 157, 123, 81~\r## $ diabetes \u0026lt;fct\u0026gt; No, No, No, No, Yes, No, No, Yes, No, No, No, No, No~\r## $ ejection_fraction \u0026lt;dbl\u0026gt; 20, 38, 20, 20, 20, 40, 15, 60, 65, 35, 38, 25, 30, ~\r## $ hblood_pressure \u0026lt;fct\u0026gt; Yes, No, No, No, No, Yes, No, No, No, Yes, Yes, Yes,~\r## $ platelets \u0026lt;dbl\u0026gt; 265000, 263358, 162000, 210000, 327000, 204000, 1270~\r## $ creatinine \u0026lt;dbl\u0026gt; 1.90, 1.10, 1.30, 1.90, 2.70, 2.10, 1.20, 1.10, 1.50~\r## $ sodium \u0026lt;dbl\u0026gt; 130, 136, 129, 137, 116, 132, 137, 131, 138, 133, 13~\r## $ sex \u0026lt;fct\u0026gt; Male, Male, Male, Male, Female, Male, Male, Male, Fe~\r## $ smoking \u0026lt;fct\u0026gt; No, No, Yes, No, No, Yes, No, Yes, No, Yes, Yes, Yes~\r## $ time \u0026lt;int\u0026gt; 4, 6, 7, 7, 8, 8, 10, 10, 10, 10, 10, 10, 11, 11, 12~\r## $ death_event \u0026lt;fct\u0026gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye~\rWe will create a few NAs in our data.\nset.seed(2021)\rheartfailure[sample(seq(nrow(heartfailure)), 20), \u0026quot;age\u0026quot;] \u0026lt;- NA\rheartfailure[sample(seq(nrow(heartfailure)), 10), \u0026quot;sex\u0026quot;] \u0026lt;- NA\r1) dataMaid\nlibrary(dataMaid)\rOne of the very useful function in dataMaid is makeDataReport() which give report on the data. By default it will give a pdf output, but other output options such as word and html are also available.\nmakeDataReport(heartfailure, replace = T)\rThis is the output example in pdf.\n2) DataExplorer\nlibrary(DataExplorer)\rGeneral visualization:\nheartfailure %\u0026gt;% plot_intro()\rSince we have missing data, we can further visualize it:\nheartfailure %\u0026gt;% plot_missing()\rheartfailure %\u0026gt;% profile_missing()\r## feature num_missing pct_missing\r## 1 age 20 0.06688963\r## 2 anaemia 0 0.00000000\r## 3 cpk_enzyme 0 0.00000000\r## 4 diabetes 0 0.00000000\r## 5 ejection_fraction 0 0.00000000\r## 6 hblood_pressure 0 0.00000000\r## 7 platelets 0 0.00000000\r## 8 creatinine 0 0.00000000\r## 9 sodium 0 0.00000000\r## 10 sex 10 0.03344482\r## 11 smoking 0 0.00000000\r## 12 time 0 0.00000000\r## 13 death_event 0 0.00000000\rWe can also do a correlation plot\nheartfailure %\u0026gt;% select_if(is.numeric) %\u0026gt;% drop_na() %\u0026gt;% plot_correlation()\rHowever, I do think correlation plot from corrplot packages gives a better and clean plot. Here is a plot from corrplot.\nlibrary(corrplot)\rheartfailure %\u0026gt;% select_if(is.numeric) %\u0026gt;% drop_na() %\u0026gt;% cor() %\u0026gt;% corrplot(type = \u0026quot;upper\u0026quot;)\rFinally, we can get an overall html report from DataExplorer package using the function create_report().\n3) dlookr\nlibrary(dlookr)\rWe can assess normality of the data using this package. The code below will plot normality for all numeric variable.\nheartfailure %\u0026gt;% plot_normality()\rHowever, for the sake of the simplicity in this post, we will run only for one variable.\nheartfailure %\u0026gt;% plot_normality(age)\rWe can also get a correlation matrix plot from this package, and no need to remove the NAs and filter the numeric variable before running the function.\nheartfailure %\u0026gt;% plot_correlate()\rLastly, from dlookr we can get the overall report of the data exploration in pdf (and other formats as well). This report is quite comprehensive, have a look.\nheartfailure %\u0026gt;% eda_paged_report(target = \u0026quot;death_event\u0026quot;)\r4) skimr\nskimr package, especially skim() function did not display correctly when using the blogdown. Hence, I included the screenshot of the result that we will typically see in the R console.\nlibrary(skimr)\rskim(heartfailure) \rSo, from skimr we can get an overview that includes the histogram for numerical data as well.\n5) outliertree\nThis package identify outlier using a decision tree. I will not go in detail about the approach, but for those who want to read further.\nlibrary(outliertree)\routlier.tree(heartfailure)\r## Reporting top 2 outliers [out of 2 found]\r## ## row [251] - suspicious column: [creatinine] - suspicious value: [0.50]\r## distribution: 96.000% \u0026gt;= 0.70 - [mean: 1.35] - [sd: 1.22] - [norm. obs: 24]\r## given:\r## [cpk_enzyme] \u0026gt; [1610.00] (value: 2522.00)\r## ## ## row [32] - suspicious column: [cpk_enzyme] - suspicious value: [23.00]\r## distribution: 98.958% \u0026gt;= 47.00 - [mean: 677.01] - [sd: 1321.86] - [norm. obs: 95]\r## given:\r## [death_event] = [Yes]\r## Outlier Tree model\r## Numeric variables: 7\r## Categorical variables: 6\r## ## Consists of 369 clusters, spread across 48 tree branches\rWe can further explore the detected outliers using histogram and boxplot. Let‚Äôs do for variable creatinine.\n# histogram\rhist(heartfailure$creatinine, breaks = 50, col = \u0026quot;navy\u0026quot;,\rxlab = \u0026quot;Creatinine\u0026quot;, main = \u0026quot;Creatinine level\u0026quot;)\r# boxplot\rboxplot(heartfailure$creatinine)\rProbably in the future I will delve into more detail about outlier detection and any awesome packages in R related to it. If I ever written any post about it, I will link it here.\nConclusion\rThese are some useful package that I find. I may edit this post in the future to add more additional data exploration package. Furthermore, there are shiny apps for data exploration as well, though I think it‚Äôs better to sticks with coded approach in data analysis/exploration. Thus, I did not explore those apps in this post. Another thing to remember is to set the variable type accordingly prior to the data exploration.\nHope this is useful!\nReferences:\nhttps://github.com/ekstroem/dataMaid\nhttps://finnstats.com/index.php/2021/05/04/exploratory-data-analysis/\nhttps://cran.r-project.org/web/packages/dlookr/vignettes/EDA.html\nhttps://cran.r-project.org/web/packages/outliertree/vignettes/Introducing_OutlierTree.html\n\r","date":1629590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629580254,"objectID":"70cd61b0f4b27096c2966c6a7320e76a","permalink":"https://tengkuhanis.netlify.app/post/data-exploration-in-r/","publishdate":"2021-08-22T00:00:00Z","relpermalink":"/post/data-exploration-in-r/","section":"post","summary":"These are some of the packages that I find useful for data exploration. Basically, this post serves more as my note for future reference. I will list out packages (and some awesome functions from that particular package) rather than specific functions.","tags":["Data exploration"],"title":"Data exploration in R","type":"post"},{"authors":[],"categories":[],"content":"","date":1629277200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636038393,"objectID":"f99994586d3336a0e53d188b7ddbb3d9","permalink":"https://tengkuhanis.netlify.app/talk/webinar-basic-of-meta-analysis-using-r/","publishdate":"2021-08-18T09:00:00Z","relpermalink":"/talk/webinar-basic-of-meta-analysis-using-r/","section":"talk","summary":"This webinar was organised by [Unit of biostatistics and research methodology](https://medic.usm.my/biostat/en/aboutus/about), School of medical sciences, USM.","tags":[],"title":"Webinar - Basic of meta-analysis using R","type":"talk"},{"authors":[],"categories":[],"content":"","date":1628173800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636037568,"objectID":"03abe37986a080895fc4e0a343a10df1","permalink":"https://tengkuhanis.netlify.app/talk/webinar-an-introduction-to-meta-analysis-using-r/","publishdate":"2021-08-05T14:30:00Z","relpermalink":"/talk/webinar-an-introduction-to-meta-analysis-using-r/","section":"talk","summary":"This webinar was organised by USM Epidemiological Group.","tags":[],"title":"Webinar - An introduction to meta-analysis using R","type":"talk"},{"authors":[],"categories":["R"],"content":"\r\rI just watched a youtube video by Andrew Couch about his commonly used function in readr, stringr, and forcats packages. Although, I have used forcats package before, I realised that I have not fully utilised all of its function.\nSo, in this post, I have summarised main function of forcats that I find useful in my day-to-day R coding. Basically, more like a note to myself.\nMain functions\rWe will use mtcars data to demonstrate each function. forcats is part of tiyverse packages. So, it will load, once we load the tidyverse packages.\nlibrary(tidyverse)\rglimpse(mtcars)\r## Rows: 32\r## Columns: 11\r## $ mpg \u0026lt;dbl\u0026gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,~\r## $ cyl \u0026lt;dbl\u0026gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,~\r## $ disp \u0026lt;dbl\u0026gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16~\r## $ hp \u0026lt;dbl\u0026gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180~\r## $ drat \u0026lt;dbl\u0026gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,~\r## $ wt \u0026lt;dbl\u0026gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.~\r## $ qsec \u0026lt;dbl\u0026gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18~\r## $ vs \u0026lt;dbl\u0026gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,~\r## $ am \u0026lt;dbl\u0026gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,~\r## $ gear \u0026lt;dbl\u0026gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,~\r## $ carb \u0026lt;dbl\u0026gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,~\rThere are 9 forcats functions that I think very useful.\nfactor()\r\rfactor() changes variable type into a factor or categorical type\nmtcars$carb \u0026lt;- factor(mtcars$carb)\rglimpse(mtcars)\r## Rows: 32\r## Columns: 11\r## $ mpg \u0026lt;dbl\u0026gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,~\r## $ cyl \u0026lt;dbl\u0026gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,~\r## $ disp \u0026lt;dbl\u0026gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16~\r## $ hp \u0026lt;dbl\u0026gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180~\r## $ drat \u0026lt;dbl\u0026gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,~\r## $ wt \u0026lt;dbl\u0026gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.~\r## $ qsec \u0026lt;dbl\u0026gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18~\r## $ vs \u0026lt;dbl\u0026gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,~\r## $ am \u0026lt;dbl\u0026gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,~\r## $ gear \u0026lt;dbl\u0026gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,~\r## $ carb \u0026lt;fct\u0026gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,~\rfct_inorder()\r\rThis function sorts factor levels based on the order of appearance in the dataset.\nlevels(mtcars$carb) # original levels\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot;\rfct_inorder(mtcars$carb) # levels based on the order of appearance\r## [1] 4 4 1 1 2 1 4 2 2 4 4 3 3 3 4 4 4 1 2 1 1 2 2 4 2 1 2 2 4 6 8 2\r## Levels: 4 1 2 3 6 8\rfct_infreq()\r\rThis function sorts factor levels based on the frequency of values.\nfct_count(mtcars$carb) # this is forcats function as well, count factor level\r## # A tibble: 6 x 2\r## f n\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt;\r## 1 1 7\r## 2 2 10\r## 3 3 3\r## 4 4 10\r## 5 6 1\r## 6 8 1\rlevels(mtcars$carb) # original levels\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot;\rfct_infreq(mtcars$carb) # levels based on the frequency values\r## [1] 4 4 1 1 2 1 4 2 2 4 4 3 3 3 4 4 4 1 2 1 1 2 2 4 2 1 2 2 4 6 8 2\r## Levels: 2 4 1 3 6 8\rfct_relevel()\r\rThis function can be used to change the order manually.\nlevels(mtcars$carb) # original levels\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot;\rfct_relevel(mtcars$carb, c(\u0026quot;8\u0026quot;, \u0026quot;6\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;3\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;1\u0026quot;)) # manually changed new levels\r## [1] 4 4 1 1 2 1 4 2 2 4 4 3 3 3 4 4 4 1 2 1 1 2 2 4 2 1 2 2 4 6 8 2\r## Levels: 8 6 4 3 2 1\rfct_relevel() can also be used to change one factor level only.\nlevels(mtcars$carb) # original levels\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot;\rfct_relevel(mtcars$carb, \u0026quot;8\u0026quot;, after = 2) # change level 8 to the third place\r## [1] 4 4 1 1 2 1 4 2 2 4 4 3 3 3 4 4 4 1 2 1 1 2 2 4 2 1 2 2 4 6 8 2\r## Levels: 1 2 8 3 4 6\rfct_reorder()\r\rThis function changes the order based on another variable. Let‚Äôs change variable carb‚Äôs levels based on value of variable disp.\nlevels(mtcars$carb) # original levels\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot;\rfct_reorder(mtcars$carb, mtcars$disp, .fun = sum, .desc = TRUE) # new level based on disp value\r## [1] 4 4 1 1 2 1 4 2 2 4 4 3 3 3 4 4 4 1 2 1 1 2 2 4 2 1 2 2 4 6 8 2\r## Levels: 4 2 1 3 8 6\rmtcars %\u0026gt;% group_by(carb) %\u0026gt;% summarise(sum_disp = sum(disp)) %\u0026gt;% arrange(desc(sum_disp)) # this is basically what we do with fct_reorder() above\r## # A tibble: 6 x 2\r## carb sum_disp\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 4 3088.\r## 2 2 2082.\r## 3 1 940.\r## 4 3 827.\r## 5 8 301 ## 6 6 145\rAdditionally, fct_reorder() can be used with plotting as well.\n# Original plot\rggplot(mtcars, aes(x = carb, y = disp)) +\rgeom_col()\r# Plot with changed levels\rmtcars %\u0026gt;% mutate(carb = fct_reorder(carb, disp, .fun = sum, .desc = TRUE)) %\u0026gt;% ggplot(aes(x = carb, y = disp)) +\rgeom_col()\rfct_lump()\r\rThis function lumps factor levels into other factors. There are 5 variants of this function:\n\rfct_lump()\rfct_lump_min()\rfct_lump_n()\rfct_lump_lowfreq()\r\rThe remaining one variant is fct_lump_prop(). It is not in the example below as I do not find it useful at least for my current R coding routine.\nfct_lump() automatically lump small frequency factor group into one group.\nfct_count(mtcars$carb) # this is forcats function as well, count factor level\r## # A tibble: 6 x 2\r## f n\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt;\r## 1 1 7\r## 2 2 10\r## 3 3 3\r## 4 4 10\r## 5 6 1\r## 6 8 1\rfct_lump(mtcars$carb) %\u0026gt;% fct_count() \r## # A tibble: 4 x 2\r## f n\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt;\r## 1 1 7\r## 2 2 10\r## 3 4 10\r## 4 Other 5\rfct_lump_min() lump factor group into one group based on the given value.\ntable(fct_lump_min(mtcars$carb, min = 2)) # group 6 and 8 lump into one group\r## ## 1 2 3 4 Other ## 7 10 3 10 2\rfct_lump_n() lump all level except for the n most frequent factor groups.\ntable(fct_lump_n(mtcars$carb, n = 2)) # 2 frequent group only, others in one group\r## ## 2 4 Other ## 10 10 12\rfct_lump_lowfreq() lump small frequent groups into one group, while making sure that particular one group is still the smallest.\ntable(fct_lump_lowfreq(mtcars$carb, other_level = \u0026quot;low\u0026quot;)) # group low is still the smallest\r## ## 1 2 4 low ## 7 10 10 5\rfct_other()\r\rfct_other() is much like fct_lump(), except we manually choose which factor groups to be combined.\ntable(fct_other(mtcars$carb, keep = c(\u0026quot;8\u0026quot;, \u0026quot;6\u0026quot;))) \r## ## 6 8 Other ## 1 1 30\rfct_recode()\r\rThis function is used to rename or relabel the factor group.\ntable(fct_recode(mtcars$carb, hanis = \u0026quot;8\u0026quot;)) \r## ## 1 2 3 4 6 hanis ## 7 10 3 10 1 1\rfct_relabel()\r\rfct_relabel() is extremely useful if we want to rename quite a number of factor groups.\ntable(mtcars$carb) # original groups\r## ## 1 2 3 4 6 8 ## 7 10 3 10 1 1\rtable(fct_relabel(mtcars$carb, ~ c(\u0026quot;abu\u0026quot;, \u0026quot;ali\u0026quot;, \u0026quot;chong\u0026quot;, \u0026quot;siti\u0026quot;, \u0026quot;krish\u0026quot;, \u0026quot;lee\u0026quot;))) # new named groups\r## ## abu ali chong siti krish lee ## 7 10 3 10 1 1\rReference:\nhttps://forcats.tidyverse.org/index.html\n\r","date":1621296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621340126,"objectID":"0e7aac2e7cf98aeec9e4ada6fd43b9fb","permalink":"https://tengkuhanis.netlify.app/post/a-summary-of-forcats-package/","publishdate":"2021-05-18T00:00:00Z","relpermalink":"/post/a-summary-of-forcats-package/","section":"post","summary":"I just watched a youtube video by Andrew Couch about his commonly used function in readr, stringr, and forcats packages. Although, I have used forcats package before, I realised that I have not fully utilised all of its function.","tags":["tidyverse"],"title":"A summary of forcats package","type":"post"},{"authors":["Tengku Muhammad Hanis","Najib Majdi Yaacob","Suhaily Mohd Hairon","Sarimah Abdullah"],"categories":null,"content":"","date":1621296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621296000,"objectID":"007ee8dff817178caa701b863676958c","permalink":"https://tengkuhanis.netlify.app/publication/2021-01-01_net_survival_differe/","publishdate":"2021-05-18T00:00:00Z","relpermalink":"/publication/2021-01-01_net_survival_differe/","section":"publication","summary":"","tags":[],"title":"Net survival differences of breast cancer between stages at diagnosis and age groups in the east coast region of West Malaysia: a retrospective cohort study","type":"publication"},{"authors":[],"categories":["R","Machine Learning"],"content":"\r\rOverview\rImbalance data happens when there is unequal distribution of data within a categorical outcome variable. Imbalance data occurs due to several reasons such as biased sampling method and measurement errors. However, the imbalance may also be the inherent characteristic of the data. For example, a rare disease predictive model, in this case, the imbalance is expected.\nGenerally, there are two types of imbalanced problem:\n\rSlight imbalance: the imbalance is small, like 4:6\rSevere imbalance: the imbalance is large, like 1:100 or more\r\rIn slight imbalanced cases, usually it is not a concern, while severe imbalanced cases require a more specialised method to to build a predictive model.\n\rThe problem\rWhat‚Äôs the problem with the imbalanced data?\nFirstly, a predictive model of an imbalanced data is bias towards the majority class. The minority class becomes harder to predict as there are few data from this class. So, the detection rate for a minority class will be very low.\rSecondly, accuracy is not a good measure in this case. We may get a good accuracy,but in reality the accuracy does not reflect the unequal distribution of the data. This is known as an accuracy paradox. Imagine we have 90% of data belong to the majority class, while the remaining 10% belong to the minority class. So, just by predicting all data as a majority class, the model can easily get 90% accuracy.\n\rHandling approach\rThe easiest approach is to collect more data, though this may not be practical in all situation. Fortunately, there are a few machine learning techniques available to tackle this problem.\nHere is a summary of resampling techniques available in themis package.\nOver-sampling approach is preferred when the dataset is small. The under-sampling approach can be used when the dataset is large, though this approach may lead to loss of information. Additionally, ensemble technique such as random forest is said to be able to model the imbalanced data, though some references/blogs say otherwise.\nSo, we are going to compare four of over-sampling techniques (upsample, SMOTE, ADASYN, and ROSE), and three of under-sampling techniques (downsample, nearmiss and tomek). The base model is a decision tree, which will be used for all the techniques. The decision trees are not going to be extensively hyperparameter tuned, for the sake of simplicity. Additionally, random forest is also going to be included in the comparison.\nThe dataset is from here. This is a summary of the dataset.\nsummary(df)\r## admit gre gpa rank ## 0:273 Min. :220.0 Min. :2.260 1: 61 ## 1:127 1st Qu.:520.0 1st Qu.:3.130 2:151 ## Median :580.0 Median :3.395 3:121 ## Mean :587.7 Mean :3.390 4: 67 ## 3rd Qu.:660.0 3rd Qu.:3.670 ## Max. :800.0 Max. :4.000\rAs we can see from the summary, variable admit has a moderate imbalanced data about 1:3 ratio.\nggplot(df, aes(admit)) + geom_bar() +\rtheme_bw()\rBelow is the code for each model.\n\r\rShow code\r\r# Packages\rlibrary(tidyverse)\rlibrary(magrittr)\rlibrary(tidymodels)\rlibrary(themis)\r# Data\rdf \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/finnstats/finnstats/main/binary.csv\u0026quot;)\r# Split data\rset.seed(1234)\rdf_split \u0026lt;- initial_split(df)\rdf_train \u0026lt;- training(df_split)\rdf_test \u0026lt;- testing(df_split)\r# 1) Decision tree ----\r# Recipe\rdt_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank)\rdf_train_rec \u0026lt;- dt_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec \u0026lt;- dt_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv \u0026lt;- vfold_cv(df_train_rec)\r# Tune and finalize workflow\r## Specify model\rdt_mod \u0026lt;- decision_tree(\rcost_complexity = tune(),\rtree_depth = tune(),\rmin_n = tune()\r) %\u0026gt;% set_engine(\u0026quot;rpart\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;)\r## Specify workflow\rdt_wf \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune \u0026lt;- dt_wf %\u0026gt;% tune_grid(resamples = df_cv,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune \u0026lt;- dt_tune %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final \u0026lt;- dt_wf %\u0026gt;% finalize_workflow(best_tune)\r# Fit on train data\rdt_train \u0026lt;- dt_wf_final %\u0026gt;% fit(data = df_train_rec)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train, new_data = df_test_rec)) %\u0026gt;% rename(pred = .pred_class)\r# 2) Oversampling ----\r## step_upsample() ----\r# Recipe\rup_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_upsample(admit,\rseed = 1234)\rdf_train_up \u0026lt;- up_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_up \u0026lt;- up_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_up \u0026lt;- vfold_cv(df_train_up)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_up \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_up \u0026lt;- dt_wf_up %\u0026gt;% tune_grid(resamples = df_cv_up,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_up \u0026lt;- dt_tune_up %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_up \u0026lt;- dt_wf_up %\u0026gt;% finalize_workflow(best_tune_up)\r# Fit on train data\rdt_train_up \u0026lt;- dt_wf_final_up %\u0026gt;% fit(data = df_train_up)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_up, new_data = df_test_rec_up)) %\u0026gt;% rename(pred_up = .pred_class)\r## step_smote() ----\r# Recipe\rsmote_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_smote(admit, seed = 1234)\rdf_train_smote \u0026lt;- smote_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_smote \u0026lt;- smote_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_smote \u0026lt;- vfold_cv(df_train_smote)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_smote \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_smote \u0026lt;- dt_wf_smote %\u0026gt;% tune_grid(resamples = df_cv_smote,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_smote \u0026lt;- dt_tune_smote %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_smote \u0026lt;- dt_wf_smote %\u0026gt;% finalize_workflow(best_tune_smote)\r# Fit on train data\rdt_train_smote \u0026lt;- dt_wf_final_smote %\u0026gt;% fit(data = df_train_smote)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_smote, new_data = df_test_rec_smote)) %\u0026gt;% rename(pred_smote = .pred_class)\r## step_rose() ----\r# Recipe\rrose_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_rose(admit, seed = 1234)\rdf_train_rose \u0026lt;- rose_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_rose \u0026lt;- rose_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_rose \u0026lt;- vfold_cv(df_train_rose)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_rose \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_rose \u0026lt;- dt_wf_rose %\u0026gt;% tune_grid(resamples = df_cv_rose,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_rose \u0026lt;- dt_tune_rose %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_rose \u0026lt;- dt_wf_rose %\u0026gt;% finalize_workflow(best_tune_rose)\r# Fit on train data\rdt_train_rose \u0026lt;- dt_wf_final_rose %\u0026gt;% fit(data = df_train_rose)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_rose, new_data = df_test_rec_rose)) %\u0026gt;% rename(pred_rose = .pred_class)\r## step_adasyn() ----\r# Recipe\radasyn_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_adasyn(admit, seed = 1234)\rdf_train_adasyn \u0026lt;- adasyn_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_adasyn \u0026lt;- adasyn_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_adasyn \u0026lt;- vfold_cv(df_train_adasyn)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_adasyn \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_adasyn \u0026lt;- dt_wf_adasyn %\u0026gt;% tune_grid(resamples = df_cv_adasyn,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_adasyn \u0026lt;- dt_tune_adasyn %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_adasyn \u0026lt;- dt_wf_adasyn %\u0026gt;% finalize_workflow(best_tune_adasyn)\r# Fit on train data\rdt_train_adasyn \u0026lt;- dt_wf_final_adasyn %\u0026gt;% fit(data = df_train_adasyn)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_adasyn, new_data = df_test_rec_adasyn)) %\u0026gt;% rename(pred_adasyn = .pred_class)\r# 3) Undersampling ----\r## step_downsample() ----\r# Recipe\rdown_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_downsample(admit,\rseed = 1234)\rdf_train_down \u0026lt;- down_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_down \u0026lt;- down_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_down \u0026lt;- vfold_cv(df_train_down)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_down \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_down \u0026lt;- dt_wf_down %\u0026gt;% tune_grid(resamples = df_cv_down,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_down \u0026lt;- dt_tune_down %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_down \u0026lt;- dt_wf_down %\u0026gt;% finalize_workflow(best_tune_down)\r# Fit on train data\rdt_train_down \u0026lt;- dt_wf_final_down %\u0026gt;% fit(data = df_train_down)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_down, new_data = df_test_rec_down)) %\u0026gt;% rename(pred_down = .pred_class)\r## step_nearmiss() ----\r# Recipe\rnearmiss_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_nearmiss(admit,\rseed = 1234)\rdf_train_nearmiss \u0026lt;- nearmiss_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_nearmiss \u0026lt;- nearmiss_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_nearmiss \u0026lt;- vfold_cv(df_train_nearmiss)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_nearmiss \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_nearmiss \u0026lt;- dt_wf_nearmiss %\u0026gt;% tune_grid(resamples = df_cv_nearmiss,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_nearmiss \u0026lt;- dt_tune_nearmiss %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_nearmiss \u0026lt;- dt_wf_nearmiss %\u0026gt;% finalize_workflow(best_tune_nearmiss)\r# Fit on train data\rdt_train_nearmiss \u0026lt;- dt_wf_final_nearmiss %\u0026gt;% fit(data = df_train_nearmiss)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_nearmiss, new_data = df_test_rec_nearmiss)) %\u0026gt;% rename(pred_nearmiss = .pred_class)\r## step_tomek() ----\r# Recipe\rtomek_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_tomek(admit,\rseed = 1234)\rdf_train_tomek \u0026lt;- tomek_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_tomek \u0026lt;- tomek_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_tomek \u0026lt;- vfold_cv(df_train_tomek)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_tomek \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_tomek \u0026lt;- dt_wf_tomek %\u0026gt;% tune_grid(resamples = df_cv_tomek,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_tomek \u0026lt;- dt_tune_tomek %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_tomek \u0026lt;- dt_wf_tomek %\u0026gt;% finalize_workflow(best_tune_tomek)\r# Fit on train data\rdt_train_tomek \u0026lt;- dt_wf_final_tomek %\u0026gt;% fit(data = df_train_tomek)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_tomek, new_data = df_test_rec_tomek)) %\u0026gt;% rename(pred_tomek = .pred_class)\r# 4) Ensemble approach: random forest ----\r## 10-folds CV\rset.seed(1234)\rdf_cv \u0026lt;- vfold_cv(df_train_rec)\r# Tune and finalize workflow\r## Specify model\rrf_mod \u0026lt;- rand_forest(\rmtry = tune(),\rtrees = tune(),\rmin_n = tune()\r) %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;)\r## Specify workflow\rrf_wf \u0026lt;- workflow() %\u0026gt;% add_model(rf_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rrf_tune \u0026lt;- rf_wf %\u0026gt;% tune_grid(resamples = df_cv,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune \u0026lt;- rf_tune %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rrf_wf_final \u0026lt;- rf_wf %\u0026gt;% finalize_workflow(best_tune)\r# Fit on train data\rrf_train \u0026lt;- rf_wf_final %\u0026gt;% fit(data = df_train_rec)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(rf_train, new_data = df_test_rec)) %\u0026gt;% rename(pred_rf = .pred_class)\r\rNow, let‚Äôs get the accuracy, sensitivity, specificity, and Mathews Correlation Coefficient (MCC) for each model.\n\r\rShow code\r\r# Get all measurements\rdf_test$admit %\u0026lt;\u0026gt;% as_factor()\rpred_col \u0026lt;- colnames(df_test)[5:13]\rresult \u0026lt;- vector(\u0026quot;list\u0026quot;, 0)\rsensi \u0026lt;- vector(\u0026quot;list\u0026quot;, 0)\rspecif \u0026lt;- vector(\u0026quot;list\u0026quot;, 0)\rmathew \u0026lt;- vector(\u0026quot;list\u0026quot;, 0)\rfor (i in seq_along(pred_col)) {\r# accuracy\rresult[[i]] \u0026lt;-\rdf_test %\u0026gt;% accuracy(admit, df_test[,pred_col[i]])\r# sensitivity\rsensi[[i]] \u0026lt;-\rdf_test %\u0026gt;% sensitivity(admit, df_test[,pred_col[i]])\r# specificity\rspecif[[i]] \u0026lt;-\rdf_test %\u0026gt;% specificity(admit, df_test[,pred_col[i]])\r# MCC\rmathew[[i]] \u0026lt;-\rdf_test %\u0026gt;% mcc(admit, df_test[,pred_col[i]])\r}\r## Turn into dataframe\rresult %\u0026lt;\u0026gt;% enframe() %\u0026gt;% unnest(cols = c(\u0026quot;value\u0026quot;)) %\u0026gt;% rename(model = name, accuracy = .estimate) %\u0026gt;% select(model, accuracy) %\u0026gt;% mutate(model = factor(model,labels = c(\r\u0026quot;1\u0026quot; = \u0026quot;base\u0026quot;,\r\u0026quot;2\u0026quot; = \u0026quot;upsample\u0026quot;,\r\u0026quot;3\u0026quot; = \u0026quot;smote\u0026quot;,\r\u0026quot;4\u0026quot; = \u0026quot;rose\u0026quot;,\r\u0026quot;5\u0026quot; = \u0026quot;adasyn\u0026quot;,\r\u0026quot;6\u0026quot; = \u0026quot;downsample\u0026quot;,\r\u0026quot;7\u0026quot; = \u0026quot;nearmiss\u0026quot;,\r\u0026quot;8\u0026quot; = \u0026quot;tomek\u0026quot;,\r\u0026quot;9\u0026quot; = \u0026quot;random_forest\u0026quot;\r)\r))\rsensi %\u0026lt;\u0026gt;% enframe() %\u0026gt;% unnest(cols = c(\u0026quot;value\u0026quot;))\rspecif %\u0026lt;\u0026gt;% enframe() %\u0026gt;% unnest(cols = c(\u0026quot;value\u0026quot;))\rmathew %\u0026lt;\u0026gt;% enframe() %\u0026gt;% unnest(cols = c(\u0026quot;value\u0026quot;))\rresult %\u0026lt;\u0026gt;% bind_cols(sensitive = sensi$.estimate, specific = specif$.estimate, mathew = mathew$.estimate)\r# Plot the result\rresult %\u0026gt;% pivot_longer(cols = 2:5, names_to = \u0026quot;measure\u0026quot;) %\u0026gt;% ggplot(aes(x = model, y = value, fill = measure)) +\rgeom_bar(position = \u0026quot;dodge\u0026quot;, stat = \u0026quot;identity\u0026quot;) +\rtheme_bw() +\rcoord_flip() +\rgeom_text(aes(label = paste0(round(value*100, digits = 1), \u0026quot;%\u0026quot;)), position = position_dodge(0.9), vjust = 0.3, size = 2.7, hjust = -0.1) +\rlabs(title = \u0026quot;Comparison of unbalanced data techniques\u0026quot;, x = \u0026quot;Techniques\u0026quot;, y = \u0026quot;Performance\u0026quot;) +\rscale_fill_discrete(name = \u0026quot;Metrics:\u0026quot;,\rlabels = c(\u0026quot;Accuracy\u0026quot;, \u0026quot;MCC\u0026quot;, \u0026quot;Sensitivity\u0026quot;, \u0026quot;Specificity\u0026quot;)) +\rtheme(legend.position = \u0026quot;bottom\u0026quot;)\r\rWe can see from the above plot, the base model (decision tree) clearly has a low detection rate for a minority class (specificity). All methods able to increase the specificity, while sacrificing the accuracy and sensitivity. As mentioned earlier, accuracy is not a good metrics for this kind of model (ie; accuracy paradox). MCC on the other hand, takes into account all values of confusion matrix; true positive, false positive, true negative, and false negative. Hence, MCC is more informative compared to accuracy (and F score, which has not been included in the plot, for the sake of simplicity).\nA more balanced model probably downsample approach based on MCC, specificity, and sensitivity. However, this does not mean that downsample technique is the best as I believes each technique behaves differently from one data to another.\nReferences:\nhttps://themis.tidymodels.org/reference/index.html\n\rhttps://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n\rhttps://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-6413-7\r\r\r","date":1620950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620955774,"objectID":"0bee3c59d405022795d0c23d2d3cc134","permalink":"https://tengkuhanis.netlify.app/post/handling-imbalanced-data/","publishdate":"2021-05-14T00:00:00Z","relpermalink":"/post/handling-imbalanced-data/","section":"post","summary":"Overview\rImbalance data happens when there is unequal distribution of data within a categorical outcome variable. Imbalance data occurs due to several reasons such as biased sampling method and measurement errors.","tags":["Machine Learning"],"title":"Handling imbalanced data","type":"post"},{"authors":[],"categories":["R","Deep Learning"],"content":"\r\rI have been reading about lost functions and optimisers in deep learning for the last couple of days when I stumble upon the term Exponentially Weighted Average (EWA). So, in this post I aims to explain my understanding of EWA.\nOverview of EWA\rEWA basically is an important concept in deep learning and have been used in several optimisers to smoothen the noise of the data.\nLet‚Äôs see the formula for EWA:\nVt is some smoothen value at point t, while St is a data point at point t. B here is a hyperparameter that we need to tune in our network. So, the choice of B will determine how many data points that we average the value of Vt as shown below:\n\rEWA in deep learnings‚Äô optimiser\rSo, some of the optimisers that adopt the approach of EWA are (red box indicates the EWA part in each formula):\nStochastic gradient descent (SGD) with momentum\r\rThe issue with SGD is the present of noise while searching for global minima. So, SGD with momentum integrated the EWA, which reduces these noises and helps the network converges faster.\nAdaptive delta (Adadelta) and Root Mean Square Propagation (RMSprop)\r\rAdadelta and RMSprop are proposed in attempt to solve the issue of diminishing learning rate of adaptive gradient (Adagrad) optimiser. The use of EWA in both optimisers actually helps to achieve this. Both optimisers have quite a similar formula, but attached below is the formula for Adadelta.\nAdaptive moment estimation (ADAM)\r\rADAM basically combined the SGD with momentum with Adadelta. As shown earlier, both optimisers use EWA.\n\rMore details on EWA\rNow, let‚Äôs go back to EWA. Here is the example of calculation of EWA:\nKeep in mind that t3 is the latest time point, followed by t2 and t1, respectively. So, if we want to calculate V3:\nSo, if we were to varies the value of B across the equation (while the values of a1‚Ä¶an remain constant), we can do so in R.\nlibrary(tidyverse) func \u0026lt;- function(b) (1 - b) * b^((20:1) - 1)\rbeta \u0026lt;- seq(0.1, 0.9, by=0.2)\rdat \u0026lt;- t(sapply(beta, func)) %\u0026gt;% as.data.frame()\rcolnames(dat)[1:20] \u0026lt;- 1:20\rdat %\u0026gt;% mutate(beta = as_factor(beta)) %\u0026gt;%\rpivot_longer(cols = 1:20, names_to = \u0026quot;data_point\u0026quot;, values_to = \u0026quot;weight\u0026quot;) %\u0026gt;% ggplot(aes(x=as.numeric(data_point), y=weight, color=beta)) +\rgeom_line() +\rgeom_point() +\rscale_x_continuous(breaks = 1:20) +\rlabs(title = \u0026quot;Change of Exponentially Weighted Average function\u0026quot;, subtitle = \u0026quot;Time at t20 is the recent time, and t1 is the initial time\u0026quot;) +\rscale_colour_discrete(\u0026quot;Beta:\u0026quot;) +\rxlab(\u0026quot;Time(t)\u0026quot;) +\rylab(\u0026quot;Weights/Coefficients\u0026quot;) +\rtheme_bw()\rNote that time at t20 is the recent time, and t1 is the initial time. Thus, two main points from the above plot are:\nThe EWA function acts in a decaying manner.\n\rAs beta, B increases we actually put more emphasize on the recent data point.\r\rSide note: I have tried to do the plot in plotly, not sure why it did not work üòï\nReferences:\n1) https://towardsdatascience.com/deep-learning-optimizers-436171c9e23f (all the equations are from this reference)\n2) https://youtu.be/NxTFlzBjS-4\n3) https://medium.com/@dhartidhami/exponentially-weighted-averages-5de212b5be46\n\r","date":1620518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620542031,"objectID":"877769ed65026a159fb60d317a14861d","permalink":"https://tengkuhanis.netlify.app/post/exponentially-weighted-average-in-deep-learning/","publishdate":"2021-05-09T00:00:00Z","relpermalink":"/post/exponentially-weighted-average-in-deep-learning/","section":"post","summary":"I have been reading about lost functions and optimisers in deep learning for the last couple of days when I stumble upon the term Exponentially Weighted Average (EWA). So, in this post I aims to explain my understanding of EWA.","tags":["Deep Learning"],"title":"Exponentially Weighted Average in Deep Learning","type":"post"},{"authors":null,"categories":["R"],"content":"\r\rFirst of all, this write up is mean for a beginner in R.\nThings can be done in many ways in R. In facts, R has been very flexible in this regard compared to other statistical softwares. Basic things such as selecting a column, slicing a row, filtering a data based on certain condition can be done using a base R function. However, all these things can also be done using a tidyverse approach.\nTidyverse basically, a collection of packages that can be loaded in a line of function.\nlibrary(tidyverse)\rTidyverse is developed by ‚ÄúRStudio people‚Äù pioneered by Hadley Wickham, which means that these packages will be continuously maintained and updated.\nSo, without further ado, these are the comparisons between these two approaches for some very basic thingy:\nSelect or deselect a column and a row\r\r# Base R\riris[1:5, c(\u0026quot;Sepal.Length\u0026quot;, \u0026quot;Sepal.Width\u0026quot;)]\riris[1:5,c(1,2)] # similar to above\riris[1:5, -1]\r# Tidyverse\riris %\u0026gt;% select(Sepal.Length, Sepal.Width) %\u0026gt;% slice(1:5)\riris %\u0026gt;% select(-Sepal.Length) %\u0026gt;% slice(1:5)\rFilter based on condition\r\r# Base R\riris[iris$Species == \u0026quot;setosa\u0026quot;, ]\r# Tidyverse\riris %\u0026gt;% filter(Species == \u0026quot;setosa\u0026quot;)\rMutate (transmute replace the variable)\r\r# Base R\riris$SL_minus10 \u0026lt;- iris$Sepal.Length - 10\r# Tidyverse\riris %\u0026gt;% mutate(SL_minus10 = Sepal.Length - 10)\rSort variable\r\r# Base R\riris[order(-iris$Sepal.Width),]\r# Tidyverse\riris %\u0026gt;% arrange(desc(Sepal.Length))\rGroup by (and get mean for variable Sepal.Width)\r\r# Not really base R\rdoBy::summaryBy(Sepal.Width~Species, iris, FUN = mean) # Tidyverse\riris %\u0026gt;% group_by(Species) %\u0026gt;% summarise(mean_SW = mean(Sepal.Width))\rRename variable\r\r# Base R\rcolnames(iris)[6] \u0026lt;- \u0026quot;hanis\u0026quot;\r# Tidyverse\riris %\u0026gt;% rename(Species = hanis)\rSo, that‚Äôs it. Overall, tidyverse give a clarity in understanding the code as it reads from left to right. On the contrary, the base R approach reads from inside to outside, especially for a more complicated code.\n","date":1620086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620118129,"objectID":"df69df02c5380516ce81421c1f0ba162","permalink":"https://tengkuhanis.netlify.app/post/2021-05-04-base-r-vs-tidyverse/","publishdate":"2021-05-04T00:00:00Z","relpermalink":"/post/2021-05-04-base-r-vs-tidyverse/","section":"post","summary":"First of all, this write up is mean for a beginner in R.\nThings can be done in many ways in R. In facts, R has been very flexible in this regard compared to other statistical softwares.","tags":["base R","comparison","tidyverse"],"title":"Base R vs tidyverse","type":"post"},{"authors":[],"categories":["R"],"content":"\r\rI have heard quite a several times that apply function is faster than loop function in R. Loop function is said to be inefficient, though in certain situation loop is the only way.\nLet‚Äôs compare between loop function and apply function in R.\nFirst, make a very big fake data contain a list of vector.\nset.seed(2021)\rxlist \u0026lt;- list(col1 = rnorm(10000000), col2 = rnorm(10000000),\rcol3 = rnorm(100000000),\rcol4 = rnorm(1000000)) # this will take a few seconds\rThen, calculate the mean of each vector using for loop().\nptm \u0026lt;- proc.time() #-- start the clock\rmean_loop \u0026lt;- vector(\u0026quot;list\u0026quot;, 0) # place holder for a value\rfor (i in seq_along(xlist)) {\rmean_loop[[i]] \u0026lt;- mean(xlist[[i]])\r}\rproc.time() - ptm #-- stop the clock (time in seconds)\r## user system elapsed ## 0.38 0.00 0.37\rNow, using lapply() function.\nptm \u0026lt;- proc.time() #-- start the clock\rmean_apply \u0026lt;- lapply(xlist, mean)\rproc.time() - ptm #-- stop the clock\r## user system elapsed ## 0.34 0.00 0.35\rSo, lapply() is a little bit faster. Obviously, with a very big dataset and a more complicated objective, lapply() is the right choice, but for a ‚Äúnormal‚Äù size dataset, the use of any of the two functions probably do not make much different.\n","date":1620086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620121215,"objectID":"c64e5453754ef81813c7c52a0bf44ddb","permalink":"https://tengkuhanis.netlify.app/post/loop-vs-apply-in-r/","publishdate":"2021-05-04T00:00:00Z","relpermalink":"/post/loop-vs-apply-in-r/","section":"post","summary":"I have heard quite a several times that apply function is faster than loop function in R. Loop function is said to be inefficient, though in certain situation loop is the only way.","tags":["base R","comparison"],"title":"Loop vs apply in R","type":"post"},{"authors":["Kamarul Imran Musa","Wan Nor Arifin","Mohd Hafiz Mohd","Mohammad Subhi Jamiluddin","Noor Atinah Ahmad","Xin Wee Chen","Tengku Muhammad Hanis","Awang Bulgiba"],"categories":null,"content":"","date":1616371200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616371200,"objectID":"0902ea25141818de58a51b1a493337c1","permalink":"https://tengkuhanis.netlify.app/publication/2021-01-01_measuring_time-varyi/","publishdate":"2021-03-22T00:00:00Z","relpermalink":"/publication/2021-01-01_measuring_time-varyi/","section":"publication","summary":"","tags":[],"title":"Measuring time-varying effective reproduction numbers for COVID-19 and their relationship with movement control order in Malaysia","type":"publication"},{"authors":null,"categories":["Epidemiology"],"content":"\r\rRecently I have read an article that the Malaysian government have made a deal with Pfizer for 6.4 million Malaysian to be vaccinated. So, I am wondering what is the minimal number of people should be vaccinated.\nI have also come across this interesting article, which explains how we can calculate a minimal number of people to be vaccinated to achieves herd immunity based on R naught (R0).\nR naught (R0)\nThe basic idea of R0 or basic reproduction number is quite simple. It describes how many secondary infections will derive from the first case. I think Figure 1 below describes this idea very well.\n\rFigure 1: Basic idea of R0(image from https://www.atrainceu.com/content/3-basic-reproduction-number-r-naught)\r\rSo, R0 can be affected by a few factors, such as:\n\rproportion of susceptible people at the initial outbreak\rinfectiousness of the virus or the disease\rrate of recovery or death\rand a few other factors\r\rAs R0 increases more than 1, the spread of the disease will increases, while R0 below 1 indicates the spread of the disease will decrease and eventually dies out.\nHowever, I noticed that quite a few including KKM (Ministry of Health, Malaysia) have used the term R0 in their reports instead of Re or Rt which is the effective reproduction number or time-varying reproduction number. R0 refers to the initial reproduction number at the beginning of the outbreak. The ‚Äúnaught‚Äù or ‚Äúzero‚Äù in R naught (R0) is referring to population condition that has zero immunity to the disease.\nHerd immunity\nHerd immunity is said to occur when a significant proportion of the population is immunized. Subsequently, those whose susceptible (not immunized) will be protected.\nHow many should be vaccinated\nSo, back to the initial topic. We can use the formula below to answer this question.\n\\[P_i \u0026gt; 1 - \\frac{1}{R_0}\\]\nPi refers to the number of proportion that should be immunized or in this case, vaccinated.\nSo, after googling, I found one calculation by my lecturer in Biostat Unit, USM, Dr Wan Arifin and his colleague. The R0 based on his calculation is 2.673. Also, I found another article reported that the R0 is 3.55 in March, according to KKM.\nMalaysian‚Äôs population is estimated at 32.7 million by the Department of Statistics, Malaysia (DOSM). So, using the formula above, about 63% to 72% of Malaysian population should vaccinated, and this translates to about 20.6 to 23.5 million people.\nThe deal that the Malaysian government made with Pfizer is far from enough, but of course, this is a very good and quick decision. We also have other vaccines like Moderna‚Äôs vaccine coming up.\nDisclaimer: This is just my opinion. Please take it with a massive grain of salt.\n","date":1607299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607310799,"objectID":"071dd7efb1b705ccf90c1ab3a155e21a","permalink":"https://tengkuhanis.netlify.app/post/how-many-malaysian-should-be-vaccinated-to-get-herd-immunity-from-covid-19/","publishdate":"2020-12-07T00:00:00Z","relpermalink":"/post/how-many-malaysian-should-be-vaccinated-to-get-herd-immunity-from-covid-19/","section":"post","summary":"Recently I have read an article that the Malaysian government have made a deal with Pfizer for 6.4 million Malaysian to be vaccinated. So, I am wondering what is the minimal number of people should be vaccinated.","tags":["COVID-19","Vaccine"],"title":"How many Malaysian should be vaccinated to get herd immunity from COVID-19?","type":"post"},{"authors":[],"categories":[],"content":"","date":1606049100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636028825,"objectID":"8fbac1502ec3991f46c9a06827ff880d","permalink":"https://tengkuhanis.netlify.app/talk/talks-an-introduction-to-relative-survival-analysis-using-r/","publishdate":"2020-11-22T12:45:00Z","relpermalink":"/talk/talks-an-introduction-to-relative-survival-analysis-using-r/","section":"talk","summary":"This talk was part of R confeRence 2020 organised by [Malaysia R User Group (MyRUG)](https://www.facebook.com/rusergroupmalaysia/).","tags":[],"title":"Talks - An introduction to relative survival analysis using R","type":"talk"},{"authors":[],"categories":[],"content":"","date":1600983000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636036582,"objectID":"1344267e341513a1b596ea22f90de0a4","permalink":"https://tengkuhanis.netlify.app/talk/webinar-an-introduction-to-bibliometric-analysis-using-r/","publishdate":"2020-09-24T21:30:00Z","relpermalink":"/talk/webinar-an-introduction-to-bibliometric-analysis-using-r/","section":"talk","summary":"This webinar was organised by Epidemiology and statistical modelling team, USM.","tags":[],"title":"Webinar - An introduction to bibliometric analysis using R","type":"talk"},{"authors":["Tengku Muhammad Hanis","Najib Majdi Yaacob","Suhaily Mohd Hairon","Sarimah Abdullah","Noorfariza Nordin","Noor Hashimah Abdullah","Mohd Faiz Md Ariffin"],"categories":null,"content":"","date":1577664000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577664000,"objectID":"b40262326b7767b21e859212dc02a932","permalink":"https://tengkuhanis.netlify.app/publication/2019-01-01_modelling_excess_mor/","publishdate":"2019-12-30T00:00:00Z","relpermalink":"/publication/2019-01-01_modelling_excess_mor/","section":"publication","summary":"","tags":[],"title":"Modelling excess mortality among breast cancer patients in the north east region of peninsular Malaysia, 2007-2011: a population-based study","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://tengkuhanis.netlify.app/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://tengkuhanis.netlify.app/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":[],"title":"External Project","type":"project"}]