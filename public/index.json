[{"authors":null,"categories":null,"content":"From medical graduate to data enthusiast\nI am a PhD student in the field of public health epidemiology in Universiti Sains Malaysia under the supervision of one of the well-established medical epidemiologist and biostatistician in Malaysia, Assoc Prof Kamarul Imran Musa.\nI did my degree in medicine. However, I believed that working as a doctor in a clinical setting is not for me. Thus, I continued my study in medical statistics. Data and analysis has sparked my interest since then. I believes that coming from medical background, give me an edge to see data in a new perspective.\n  Download my resumé.\n","date":1554595200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://tengkuhanis.netlify.app/author/tengku-muhammad-hanis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tengku-muhammad-hanis/","section":"authors","summary":"From medical graduate to data enthusiast\nI am a PhD student in the field of public health epidemiology in Universiti Sains Malaysia under the supervision of one of the well-established medical epidemiologist and biostatistician in Malaysia, Assoc Prof Kamarul Imran Musa.","tags":null,"title":"Tengku Muhammad Hanis","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bb560906b6a99893cc21387348c0b074","permalink":"https://tengkuhanis.netlify.app/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://tengkuhanis.netlify.app/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://tengkuhanis.netlify.app/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://tengkuhanis.netlify.app/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://tengkuhanis.netlify.app/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":["R"],"content":"\r\rThese are some of the packages that I find useful for data exploration. Basically, this post serves more as my note for future reference. I will list out packages (and some awesome functions from that particular package) rather than specific functions. Further, base R and tidyverse packages will not be included specifically in this list.\nLoad supporting packages\nlibrary(tidyverse)\rThe data we are going to use is from dlookr package:\nglimpse(heartfailure)\r## Rows: 299\r## Columns: 13\r## $ age \u0026lt;int\u0026gt; 75, 55, 65, 50, 65, 90, 75, 60, 65, 80, 75, 62, 45, ~\r## $ anaemia \u0026lt;fct\u0026gt; No, No, No, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, N~\r## $ cpk_enzyme \u0026lt;dbl\u0026gt; 582, 7861, 146, 111, 160, 47, 246, 315, 157, 123, 81~\r## $ diabetes \u0026lt;fct\u0026gt; No, No, No, No, Yes, No, No, Yes, No, No, No, No, No~\r## $ ejection_fraction \u0026lt;dbl\u0026gt; 20, 38, 20, 20, 20, 40, 15, 60, 65, 35, 38, 25, 30, ~\r## $ hblood_pressure \u0026lt;fct\u0026gt; Yes, No, No, No, No, Yes, No, No, No, Yes, Yes, Yes,~\r## $ platelets \u0026lt;dbl\u0026gt; 265000, 263358, 162000, 210000, 327000, 204000, 1270~\r## $ creatinine \u0026lt;dbl\u0026gt; 1.90, 1.10, 1.30, 1.90, 2.70, 2.10, 1.20, 1.10, 1.50~\r## $ sodium \u0026lt;dbl\u0026gt; 130, 136, 129, 137, 116, 132, 137, 131, 138, 133, 13~\r## $ sex \u0026lt;fct\u0026gt; Male, Male, Male, Male, Female, Male, Male, Male, Fe~\r## $ smoking \u0026lt;fct\u0026gt; No, No, Yes, No, No, Yes, No, Yes, No, Yes, Yes, Yes~\r## $ time \u0026lt;int\u0026gt; 4, 6, 7, 7, 8, 8, 10, 10, 10, 10, 10, 10, 11, 11, 12~\r## $ death_event \u0026lt;fct\u0026gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye~\rWe will create a few NAs in our data.\nset.seed(2021)\rheartfailure[sample(seq(nrow(heartfailure)), 20), \u0026quot;age\u0026quot;] \u0026lt;- NA\rheartfailure[sample(seq(nrow(heartfailure)), 10), \u0026quot;sex\u0026quot;] \u0026lt;- NA\r1) dataMaid\nlibrary(dataMaid)\rOne of the very useful function in dataMaid is makeDataReport which give report on the data. By default it will give a pdf output, but other output option such as word and html can be set.\nmakeDataReport(heartfailure, replace = T)\rThis is the output exampple in pdf.\n2) DataExplorer\nlibrary(DataExplorer)\rGeneral visualization:\nheartfailure %\u0026gt;% plot_intro()\rSince we have missing data, we can further visualize it:\nheartfailure %\u0026gt;% plot_missing()\rheartfailure %\u0026gt;% profile_missing()\r## feature num_missing pct_missing\r## 1 age 20 0.06688963\r## 2 anaemia 0 0.00000000\r## 3 cpk_enzyme 0 0.00000000\r## 4 diabetes 0 0.00000000\r## 5 ejection_fraction 0 0.00000000\r## 6 hblood_pressure 0 0.00000000\r## 7 platelets 0 0.00000000\r## 8 creatinine 0 0.00000000\r## 9 sodium 0 0.00000000\r## 10 sex 10 0.03344482\r## 11 smoking 0 0.00000000\r## 12 time 0 0.00000000\r## 13 death_event 0 0.00000000\rWe can also do a correlation plot\nheartfailure %\u0026gt;% select_if(is.numeric) %\u0026gt;% drop_na() %\u0026gt;% plot_correlation()\rHowever, I do think correlation plot from corrplot packages gives a better and clean plot. Here is a plot from corrplot.\nlibrary(corrplot)\rheartfailure %\u0026gt;% select_if(is.numeric) %\u0026gt;% drop_na() %\u0026gt;% cor() %\u0026gt;% corrplot(type = \u0026quot;upper\u0026quot;)\rFinally, we can get an overall html report from DataExplorer package using the function create_report().\n3) dlookr\nlibrary(dlookr)\rWe can assess normality of the data using this package. The code below will plot normality for all numeric variable.\nheartfailure %\u0026gt;% plot_normality()\rHowever, for the sake of the simplicity in this post, we will run only for one variable.\nheartfailure %\u0026gt;% plot_normality(age)\rWe can also get a correlation matrix plot from this package, and no need to remove the NAs and filter the numeric variable before running the function.\nheartfailure %\u0026gt;% plot_correlate()\rLastly, from dlookr we can get the overall report of the data exploration in pdf (and other formats as well). This report is quite comprehensive, have a look.\nheartfailure %\u0026gt;% eda_paged_report(target = \u0026quot;death_event\u0026quot;)\r4) outliertree\nThis package identify outlier using a decision tree. I will not go in detail about the approach, but for those who want to read further.\nlibrary(outliertree)\routlier.tree(heartfailure)\r## Reporting top 2 outliers [out of 2 found]\r## ## row [251] - suspicious column: [creatinine] - suspicious value: [0.50]\r## distribution: 96.000% \u0026gt;= 0.70 - [mean: 1.35] - [sd: 1.22] - [norm. obs: 24]\r## given:\r## [cpk_enzyme] \u0026gt; [1610.00] (value: 2522.00)\r## ## ## row [32] - suspicious column: [cpk_enzyme] - suspicious value: [23.00]\r## distribution: 98.958% \u0026gt;= 47.00 - [mean: 677.01] - [sd: 1321.86] - [norm. obs: 95]\r## given:\r## [death_event] = [Yes]\r## Outlier Tree model\r## Numeric variables: 7\r## Categorical variables: 6\r## ## Consists of 369 clusters, spread across 48 tree branches\rWe can further explore the detected outliers using histogram and boxplot. Let’s do for variable creatinine.\n# histogram\rhist(heartfailure$creatinine, breaks = 50, col = \u0026quot;navy\u0026quot;,\rxlab = \u0026quot;Creatinine\u0026quot;, main = \u0026quot;Creatinine level\u0026quot;)\r# boxplot\rboxplot(heartfailure$creatinine)\rProbably in the future I will delve into more detail about outlier detection and any awesome packages in R related to it. If I ever written any post about it, I will link it here.\nConclusion\rThese are some useful package that I find. I may edit this post in the future to add more additional data exploration package. Furthermore, there are shiny apps for data exploration as well, though I think it’s better to sticks with coded approach in data analysis/exploration. Thus, I did not explore those apps in this post. Another thing to remember is to set the variable type accordingly prior to the data exploration.\nHope this is useful!\nReferences:\nhttps://github.com/ekstroem/dataMaid\nhttps://finnstats.com/index.php/2021/05/04/exploratory-data-analysis/\nhttps://cran.r-project.org/web/packages/dlookr/vignettes/EDA.html\nhttps://cran.r-project.org/web/packages/outliertree/vignettes/Introducing_OutlierTree.html\n\r","date":1629590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629580254,"objectID":"70cd61b0f4b27096c2966c6a7320e76a","permalink":"https://tengkuhanis.netlify.app/post/data-exploration-in-r/","publishdate":"2021-08-22T00:00:00Z","relpermalink":"/post/data-exploration-in-r/","section":"post","summary":"These are some of the packages that I find useful for data exploration. Basically, this post serves more as my note for future reference. I will list out packages (and some awesome functions from that particular package) rather than specific functions.","tags":["Data exploration"],"title":"Data exploration in R","type":"post"},{"authors":[],"categories":["R"],"content":"\r\rI just watched a youtube video by Andrew Couch about his commonly used function in readr, stringr, and forcats packages. Although, I have used forcats package before, I realised that I have not fully utilised all of its function.\nSo, in this post, I have summarised main function of forcats that I find useful in my day-to-day R coding. Basically, more like a note to myself.\nMain functions\rWe will use mtcars data to demonstrate each function. forcats is part of tiyverse packages. So, it will load, once we load the tidyverse packages.\nlibrary(tidyverse)\rglimpse(mtcars)\r## Rows: 32\r## Columns: 11\r## $ mpg \u0026lt;dbl\u0026gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,~\r## $ cyl \u0026lt;dbl\u0026gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,~\r## $ disp \u0026lt;dbl\u0026gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16~\r## $ hp \u0026lt;dbl\u0026gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180~\r## $ drat \u0026lt;dbl\u0026gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,~\r## $ wt \u0026lt;dbl\u0026gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.~\r## $ qsec \u0026lt;dbl\u0026gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18~\r## $ vs \u0026lt;dbl\u0026gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,~\r## $ am \u0026lt;dbl\u0026gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,~\r## $ gear \u0026lt;dbl\u0026gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,~\r## $ carb \u0026lt;dbl\u0026gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,~\rThere are 9 forcats functions that I think very useful.\nfactor()\r\rfactor() changes variable type into a factor or categorical type\nmtcars$carb \u0026lt;- factor(mtcars$carb)\rglimpse(mtcars)\r## Rows: 32\r## Columns: 11\r## $ mpg \u0026lt;dbl\u0026gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,~\r## $ cyl \u0026lt;dbl\u0026gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,~\r## $ disp \u0026lt;dbl\u0026gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16~\r## $ hp \u0026lt;dbl\u0026gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180~\r## $ drat \u0026lt;dbl\u0026gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,~\r## $ wt \u0026lt;dbl\u0026gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.~\r## $ qsec \u0026lt;dbl\u0026gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18~\r## $ vs \u0026lt;dbl\u0026gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,~\r## $ am \u0026lt;dbl\u0026gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,~\r## $ gear \u0026lt;dbl\u0026gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,~\r## $ carb \u0026lt;fct\u0026gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,~\rfct_inorder()\r\rThis function sorts factor levels based on the order of appearance in the dataset.\nlevels(mtcars$carb) # original levels\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot;\rfct_inorder(mtcars$carb) # levels based on the order of appearance\r## [1] 4 4 1 1 2 1 4 2 2 4 4 3 3 3 4 4 4 1 2 1 1 2 2 4 2 1 2 2 4 6 8 2\r## Levels: 4 1 2 3 6 8\rfct_infreq()\r\rThis function sorts factor levels based on the frequency of values.\nfct_count(mtcars$carb) # this is forcats function as well, count factor level\r## # A tibble: 6 x 2\r## f n\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt;\r## 1 1 7\r## 2 2 10\r## 3 3 3\r## 4 4 10\r## 5 6 1\r## 6 8 1\rlevels(mtcars$carb) # original levels\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot;\rfct_infreq(mtcars$carb) # levels based on the frequency values\r## [1] 4 4 1 1 2 1 4 2 2 4 4 3 3 3 4 4 4 1 2 1 1 2 2 4 2 1 2 2 4 6 8 2\r## Levels: 2 4 1 3 6 8\rfct_relevel()\r\rThis function can be used to change the order manually.\nlevels(mtcars$carb) # original levels\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot;\rfct_relevel(mtcars$carb, c(\u0026quot;8\u0026quot;, \u0026quot;6\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;3\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;1\u0026quot;)) # manually changed new levels\r## [1] 4 4 1 1 2 1 4 2 2 4 4 3 3 3 4 4 4 1 2 1 1 2 2 4 2 1 2 2 4 6 8 2\r## Levels: 8 6 4 3 2 1\rfct_relevel() can also be used to change one factor level only.\nlevels(mtcars$carb) # original levels\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot;\rfct_relevel(mtcars$carb, \u0026quot;8\u0026quot;, after = 2) # change level 8 to the third place\r## [1] 4 4 1 1 2 1 4 2 2 4 4 3 3 3 4 4 4 1 2 1 1 2 2 4 2 1 2 2 4 6 8 2\r## Levels: 1 2 8 3 4 6\rfct_reorder()\r\rThis function changes the order based on another variable. Let’s change variable carb’s levels based on value of variable disp.\nlevels(mtcars$carb) # original levels\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot;\rfct_reorder(mtcars$carb, mtcars$disp, .fun = sum, .desc = TRUE) # new level based on disp value\r## [1] 4 4 1 1 2 1 4 2 2 4 4 3 3 3 4 4 4 1 2 1 1 2 2 4 2 1 2 2 4 6 8 2\r## Levels: 4 2 1 3 8 6\rmtcars %\u0026gt;% group_by(carb) %\u0026gt;% summarise(sum_disp = sum(disp)) %\u0026gt;% arrange(desc(sum_disp)) # this is basically what we do with fct_reorder() above\r## # A tibble: 6 x 2\r## carb sum_disp\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 4 3088.\r## 2 2 2082.\r## 3 1 940.\r## 4 3 827.\r## 5 8 301 ## 6 6 145\rAdditionally, fct_reorder() can be used with plotting as well.\n# Original plot\rggplot(mtcars, aes(x = carb, y = disp)) +\rgeom_col()\r# Plot with changed levels\rmtcars %\u0026gt;% mutate(carb = fct_reorder(carb, disp, .fun = sum, .desc = TRUE)) %\u0026gt;% ggplot(aes(x = carb, y = disp)) +\rgeom_col()\rfct_lump()\r\rThis function lumps factor levels into other factors. There are 5 variants of this function:\n\rfct_lump()\rfct_lump_min()\rfct_lump_n()\rfct_lump_lowfreq()\r\rThe remaining one variant is fct_lump_prop(). It is not in the example below as I do not find it useful at least for my current R coding routine.\nfct_lump() automatically lump small frequency factor group into one group.\nfct_count(mtcars$carb) # this is forcats function as well, count factor level\r## # A tibble: 6 x 2\r## f n\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt;\r## 1 1 7\r## 2 2 10\r## 3 3 3\r## 4 4 10\r## 5 6 1\r## 6 8 1\rfct_lump(mtcars$carb) %\u0026gt;% fct_count() \r## # A tibble: 4 x 2\r## f n\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt;\r## 1 1 7\r## 2 2 10\r## 3 4 10\r## 4 Other 5\rfct_lump_min() lump factor group into one group based on the given value.\ntable(fct_lump_min(mtcars$carb, min = 2)) # group 6 and 8 lump into one group\r## ## 1 2 3 4 Other ## 7 10 3 10 2\rfct_lump_n() lump all level except for the n most frequent factor groups.\ntable(fct_lump_n(mtcars$carb, n = 2)) # 2 frequent group only, others in one group\r## ## 2 4 Other ## 10 10 12\rfct_lump_lowfreq() lump small frequent groups into one group, while making sure that particular one group is still the smallest.\ntable(fct_lump_lowfreq(mtcars$carb, other_level = \u0026quot;low\u0026quot;)) # group low is still the smallest\r## ## 1 2 4 low ## 7 10 10 5\rfct_other()\r\rfct_other() is much like fct_lump(), except we manually choose which factor groups to be combined.\ntable(fct_other(mtcars$carb, keep = c(\u0026quot;8\u0026quot;, \u0026quot;6\u0026quot;))) \r## ## 6 8 Other ## 1 1 30\rfct_recode()\r\rThis function is used to rename or relabel the factor group.\ntable(fct_recode(mtcars$carb, hanis = \u0026quot;8\u0026quot;)) \r## ## 1 2 3 4 6 hanis ## 7 10 3 10 1 1\rfct_relabel()\r\rfct_relabel() is extremely useful if we want to rename quite a number of factor groups.\ntable(mtcars$carb) # original groups\r## ## 1 2 3 4 6 8 ## 7 10 3 10 1 1\rtable(fct_relabel(mtcars$carb, ~ c(\u0026quot;abu\u0026quot;, \u0026quot;ali\u0026quot;, \u0026quot;chong\u0026quot;, \u0026quot;siti\u0026quot;, \u0026quot;krish\u0026quot;, \u0026quot;lee\u0026quot;))) # new named groups\r## ## abu ali chong siti krish lee ## 7 10 3 10 1 1\rReference:\nhttps://forcats.tidyverse.org/index.html\n\r","date":1621296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621340126,"objectID":"0e7aac2e7cf98aeec9e4ada6fd43b9fb","permalink":"https://tengkuhanis.netlify.app/post/a-summary-of-forcats-package/","publishdate":"2021-05-18T00:00:00Z","relpermalink":"/post/a-summary-of-forcats-package/","section":"post","summary":"I just watched a youtube video by Andrew Couch about his commonly used function in readr, stringr, and forcats packages. Although, I have used forcats package before, I realised that I have not fully utilised all of its function.","tags":["tidyverse"],"title":"A summary of forcats package","type":"post"},{"authors":[],"categories":["R","Machine Learning"],"content":"\r\rOverview\rImbalance data happens when there is unequal distribution of data within a categorical outcome variable. Imbalance data occurs due to several reasons such as biased sampling method and measurement errors. However, the imbalance may also be the inherent characteristic of the data. For example, a rare disease predictive model, in this case, the imbalance is expected.\nGenerally, there are two types of imbalanced problem:\n\rSlight imbalance: the imbalance is small, like 4:6\rSevere imbalance: the imbalance is large, like 1:100 or more\r\rIn slight imbalanced cases, usually it is not a concern, while severe imbalanced cases require a more specialised method to to build a predictive model.\n\rThe problem\rWhat’s the problem with the imbalanced data?\nFirstly, a predictive model of an imbalanced data is bias towards the majority class. The minority class becomes harder to predict as there are few data from this class. So, the detection rate for a minority class will be very low.\rSecondly, accuracy is not a good measure in this case. We may get a good accuracy,but in reality the accuracy does not reflect the unequal distribution of the data. This is known as an accuracy paradox. Imagine we have 90% of data belong to the majority class, while the remaining 10% belong to the minority class. So, just by predicting all data as a majority class, the model can easily get 90% accuracy.\n\rHandling approach\rThe easiest approach is to collect more data, though this may not be practical in all situation. Fortunately, there are a few machine learning techniques available to tackle this problem.\nHere is a summary of resampling techniques available in themis package.\nOver-sampling approach is preferred when the dataset is small. The under-sampling approach can be used when the dataset is large, though this approach may lead to loss of information. Additionally, ensemble technique such as random forest is said to be able to model the imbalanced data, though some references/blogs say otherwise.\nSo, we are going to compare four of over-sampling techniques (upsample, SMOTE, ADASYN, and ROSE), and three of under-sampling techniques (downsample, nearmiss and tomek). The base model is a decision tree, which will be used for all the techniques. The decision trees are not going to be extensively hyperparameter tuned, for the sake of simplicity. Additionally, random forest is also going to be included in the comparison.\nThe dataset is from here. This is a summary of the dataset.\nsummary(df)\r## admit gre gpa rank ## 0:273 Min. :220.0 Min. :2.260 1: 61 ## 1:127 1st Qu.:520.0 1st Qu.:3.130 2:151 ## Median :580.0 Median :3.395 3:121 ## Mean :587.7 Mean :3.390 4: 67 ## 3rd Qu.:660.0 3rd Qu.:3.670 ## Max. :800.0 Max. :4.000\rAs we can see from the summary, variable admit has a moderate imbalanced data about 1:3 ratio.\nggplot(df, aes(admit)) + geom_bar() +\rtheme_bw()\rBelow is the code for each model.\n\r\rShow code\r\r# Packages\rlibrary(tidyverse)\rlibrary(magrittr)\rlibrary(tidymodels)\rlibrary(themis)\r# Data\rdf \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/finnstats/finnstats/main/binary.csv\u0026quot;)\r# Split data\rset.seed(1234)\rdf_split \u0026lt;- initial_split(df)\rdf_train \u0026lt;- training(df_split)\rdf_test \u0026lt;- testing(df_split)\r# 1) Decision tree ----\r# Recipe\rdt_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank)\rdf_train_rec \u0026lt;- dt_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec \u0026lt;- dt_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv \u0026lt;- vfold_cv(df_train_rec)\r# Tune and finalize workflow\r## Specify model\rdt_mod \u0026lt;- decision_tree(\rcost_complexity = tune(),\rtree_depth = tune(),\rmin_n = tune()\r) %\u0026gt;% set_engine(\u0026quot;rpart\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;)\r## Specify workflow\rdt_wf \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune \u0026lt;- dt_wf %\u0026gt;% tune_grid(resamples = df_cv,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune \u0026lt;- dt_tune %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final \u0026lt;- dt_wf %\u0026gt;% finalize_workflow(best_tune)\r# Fit on train data\rdt_train \u0026lt;- dt_wf_final %\u0026gt;% fit(data = df_train_rec)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train, new_data = df_test_rec)) %\u0026gt;% rename(pred = .pred_class)\r# 2) Oversampling ----\r## step_upsample() ----\r# Recipe\rup_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_upsample(admit,\rseed = 1234)\rdf_train_up \u0026lt;- up_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_up \u0026lt;- up_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_up \u0026lt;- vfold_cv(df_train_up)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_up \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_up \u0026lt;- dt_wf_up %\u0026gt;% tune_grid(resamples = df_cv_up,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_up \u0026lt;- dt_tune_up %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_up \u0026lt;- dt_wf_up %\u0026gt;% finalize_workflow(best_tune_up)\r# Fit on train data\rdt_train_up \u0026lt;- dt_wf_final_up %\u0026gt;% fit(data = df_train_up)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_up, new_data = df_test_rec_up)) %\u0026gt;% rename(pred_up = .pred_class)\r## step_smote() ----\r# Recipe\rsmote_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_smote(admit, seed = 1234)\rdf_train_smote \u0026lt;- smote_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_smote \u0026lt;- smote_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_smote \u0026lt;- vfold_cv(df_train_smote)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_smote \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_smote \u0026lt;- dt_wf_smote %\u0026gt;% tune_grid(resamples = df_cv_smote,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_smote \u0026lt;- dt_tune_smote %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_smote \u0026lt;- dt_wf_smote %\u0026gt;% finalize_workflow(best_tune_smote)\r# Fit on train data\rdt_train_smote \u0026lt;- dt_wf_final_smote %\u0026gt;% fit(data = df_train_smote)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_smote, new_data = df_test_rec_smote)) %\u0026gt;% rename(pred_smote = .pred_class)\r## step_rose() ----\r# Recipe\rrose_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_rose(admit, seed = 1234)\rdf_train_rose \u0026lt;- rose_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_rose \u0026lt;- rose_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_rose \u0026lt;- vfold_cv(df_train_rose)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_rose \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_rose \u0026lt;- dt_wf_rose %\u0026gt;% tune_grid(resamples = df_cv_rose,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_rose \u0026lt;- dt_tune_rose %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_rose \u0026lt;- dt_wf_rose %\u0026gt;% finalize_workflow(best_tune_rose)\r# Fit on train data\rdt_train_rose \u0026lt;- dt_wf_final_rose %\u0026gt;% fit(data = df_train_rose)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_rose, new_data = df_test_rec_rose)) %\u0026gt;% rename(pred_rose = .pred_class)\r## step_adasyn() ----\r# Recipe\radasyn_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_adasyn(admit, seed = 1234)\rdf_train_adasyn \u0026lt;- adasyn_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_adasyn \u0026lt;- adasyn_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_adasyn \u0026lt;- vfold_cv(df_train_adasyn)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_adasyn \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_adasyn \u0026lt;- dt_wf_adasyn %\u0026gt;% tune_grid(resamples = df_cv_adasyn,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_adasyn \u0026lt;- dt_tune_adasyn %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_adasyn \u0026lt;- dt_wf_adasyn %\u0026gt;% finalize_workflow(best_tune_adasyn)\r# Fit on train data\rdt_train_adasyn \u0026lt;- dt_wf_final_adasyn %\u0026gt;% fit(data = df_train_adasyn)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_adasyn, new_data = df_test_rec_adasyn)) %\u0026gt;% rename(pred_adasyn = .pred_class)\r# 3) Undersampling ----\r## step_downsample() ----\r# Recipe\rdown_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_downsample(admit,\rseed = 1234)\rdf_train_down \u0026lt;- down_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_down \u0026lt;- down_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_down \u0026lt;- vfold_cv(df_train_down)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_down \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_down \u0026lt;- dt_wf_down %\u0026gt;% tune_grid(resamples = df_cv_down,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_down \u0026lt;- dt_tune_down %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_down \u0026lt;- dt_wf_down %\u0026gt;% finalize_workflow(best_tune_down)\r# Fit on train data\rdt_train_down \u0026lt;- dt_wf_final_down %\u0026gt;% fit(data = df_train_down)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_down, new_data = df_test_rec_down)) %\u0026gt;% rename(pred_down = .pred_class)\r## step_nearmiss() ----\r# Recipe\rnearmiss_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_nearmiss(admit,\rseed = 1234)\rdf_train_nearmiss \u0026lt;- nearmiss_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_nearmiss \u0026lt;- nearmiss_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_nearmiss \u0026lt;- vfold_cv(df_train_nearmiss)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_nearmiss \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_nearmiss \u0026lt;- dt_wf_nearmiss %\u0026gt;% tune_grid(resamples = df_cv_nearmiss,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_nearmiss \u0026lt;- dt_tune_nearmiss %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_nearmiss \u0026lt;- dt_wf_nearmiss %\u0026gt;% finalize_workflow(best_tune_nearmiss)\r# Fit on train data\rdt_train_nearmiss \u0026lt;- dt_wf_final_nearmiss %\u0026gt;% fit(data = df_train_nearmiss)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_nearmiss, new_data = df_test_rec_nearmiss)) %\u0026gt;% rename(pred_nearmiss = .pred_class)\r## step_tomek() ----\r# Recipe\rtomek_rec \u0026lt;- recipe(admit ~., data = df_train) %\u0026gt;% step_mutate_at(c(\u0026quot;admit\u0026quot;, \u0026quot;rank\u0026quot;), fn = as_factor) %\u0026gt;% step_dummy(rank) %\u0026gt;% step_tomek(admit,\rseed = 1234)\rdf_train_tomek \u0026lt;- tomek_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = NULL)\rdf_test_rec_tomek \u0026lt;- tomek_rec %\u0026gt;% prep() %\u0026gt;% bake(new_data = df_test)\r## 10-folds CV\rset.seed(1234)\rdf_cv_tomek \u0026lt;- vfold_cv(df_train_tomek)\r# Tune and finalize workflow\r## Specify model\r# same as before\r## Specify workflow\rdt_wf_tomek \u0026lt;- workflow() %\u0026gt;% add_model(dt_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rdt_tune_tomek \u0026lt;- dt_wf_tomek %\u0026gt;% tune_grid(resamples = df_cv_tomek,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune_tomek \u0026lt;- dt_tune_tomek %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rdt_wf_final_tomek \u0026lt;- dt_wf_tomek %\u0026gt;% finalize_workflow(best_tune_tomek)\r# Fit on train data\rdt_train_tomek \u0026lt;- dt_wf_final_tomek %\u0026gt;% fit(data = df_train_tomek)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(dt_train_tomek, new_data = df_test_rec_tomek)) %\u0026gt;% rename(pred_tomek = .pred_class)\r# 4) Ensemble approach: random forest ----\r## 10-folds CV\rset.seed(1234)\rdf_cv \u0026lt;- vfold_cv(df_train_rec)\r# Tune and finalize workflow\r## Specify model\rrf_mod \u0026lt;- rand_forest(\rmtry = tune(),\rtrees = tune(),\rmin_n = tune()\r) %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;)\r## Specify workflow\rrf_wf \u0026lt;- workflow() %\u0026gt;% add_model(rf_mod) %\u0026gt;% add_formula(admit ~.)\r## Tune model\rset.seed(1234)\rrf_tune \u0026lt;- rf_wf %\u0026gt;% tune_grid(resamples = df_cv,\rmetrics = metric_set(accuracy))\r## Select best model\rbest_tune \u0026lt;- rf_tune %\u0026gt;% select_best(\u0026quot;accuracy\u0026quot;)\r## Finalize workflow\rrf_wf_final \u0026lt;- rf_wf %\u0026gt;% finalize_workflow(best_tune)\r# Fit on train data\rrf_train \u0026lt;- rf_wf_final %\u0026gt;% fit(data = df_train_rec)\r# Fit on test data and get accuracy\rdf_test %\u0026lt;\u0026gt;% bind_cols(predict(rf_train, new_data = df_test_rec)) %\u0026gt;% rename(pred_rf = .pred_class)\r\rNow, let’s get the accuracy, sensitivity, specificity, and Mathews Correlation Coefficient (MCC) for each model.\n\r\rShow code\r\r# Get all measurements\rdf_test$admit %\u0026lt;\u0026gt;% as_factor()\rpred_col \u0026lt;- colnames(df_test)[5:13]\rresult \u0026lt;- vector(\u0026quot;list\u0026quot;, 0)\rsensi \u0026lt;- vector(\u0026quot;list\u0026quot;, 0)\rspecif \u0026lt;- vector(\u0026quot;list\u0026quot;, 0)\rmathew \u0026lt;- vector(\u0026quot;list\u0026quot;, 0)\rfor (i in seq_along(pred_col)) {\r# accuracy\rresult[[i]] \u0026lt;-\rdf_test %\u0026gt;% accuracy(admit, df_test[,pred_col[i]])\r# sensitivity\rsensi[[i]] \u0026lt;-\rdf_test %\u0026gt;% sensitivity(admit, df_test[,pred_col[i]])\r# specificity\rspecif[[i]] \u0026lt;-\rdf_test %\u0026gt;% specificity(admit, df_test[,pred_col[i]])\r# MCC\rmathew[[i]] \u0026lt;-\rdf_test %\u0026gt;% mcc(admit, df_test[,pred_col[i]])\r}\r## Turn into dataframe\rresult %\u0026lt;\u0026gt;% enframe() %\u0026gt;% unnest(cols = c(\u0026quot;value\u0026quot;)) %\u0026gt;% rename(model = name, accuracy = .estimate) %\u0026gt;% select(model, accuracy) %\u0026gt;% mutate(model = factor(model,labels = c(\r\u0026quot;1\u0026quot; = \u0026quot;base\u0026quot;,\r\u0026quot;2\u0026quot; = \u0026quot;upsample\u0026quot;,\r\u0026quot;3\u0026quot; = \u0026quot;smote\u0026quot;,\r\u0026quot;4\u0026quot; = \u0026quot;rose\u0026quot;,\r\u0026quot;5\u0026quot; = \u0026quot;adasyn\u0026quot;,\r\u0026quot;6\u0026quot; = \u0026quot;downsample\u0026quot;,\r\u0026quot;7\u0026quot; = \u0026quot;nearmiss\u0026quot;,\r\u0026quot;8\u0026quot; = \u0026quot;tomek\u0026quot;,\r\u0026quot;9\u0026quot; = \u0026quot;random_forest\u0026quot;\r)\r))\rsensi %\u0026lt;\u0026gt;% enframe() %\u0026gt;% unnest(cols = c(\u0026quot;value\u0026quot;))\rspecif %\u0026lt;\u0026gt;% enframe() %\u0026gt;% unnest(cols = c(\u0026quot;value\u0026quot;))\rmathew %\u0026lt;\u0026gt;% enframe() %\u0026gt;% unnest(cols = c(\u0026quot;value\u0026quot;))\rresult %\u0026lt;\u0026gt;% bind_cols(sensitive = sensi$.estimate, specific = specif$.estimate, mathew = mathew$.estimate)\r# Plot the result\rresult %\u0026gt;% pivot_longer(cols = 2:5, names_to = \u0026quot;measure\u0026quot;) %\u0026gt;% ggplot(aes(x = model, y = value, fill = measure)) +\rgeom_bar(position = \u0026quot;dodge\u0026quot;, stat = \u0026quot;identity\u0026quot;) +\rtheme_bw() +\rcoord_flip() +\rgeom_text(aes(label = paste0(round(value*100, digits = 1), \u0026quot;%\u0026quot;)), position = position_dodge(0.9), vjust = 0.3, size = 2.7, hjust = -0.1) +\rlabs(title = \u0026quot;Comparison of unbalanced data techniques\u0026quot;, x = \u0026quot;Techniques\u0026quot;, y = \u0026quot;Performance\u0026quot;) +\rscale_fill_discrete(name = \u0026quot;Metrics:\u0026quot;,\rlabels = c(\u0026quot;Accuracy\u0026quot;, \u0026quot;MCC\u0026quot;, \u0026quot;Sensitivity\u0026quot;, \u0026quot;Specificity\u0026quot;)) +\rtheme(legend.position = \u0026quot;bottom\u0026quot;)\r\rWe can see from the above plot, the base model (decision tree) clearly has a low detection rate for a minority class (specificity). All methods able to increase the specificity, while sacrificing the accuracy and sensitivity. As mentioned earlier, accuracy is not a good metrics for this kind of model (ie; accuracy paradox). MCC on the other hand, takes into account all values of confusion matrix; true positive, false positive, true negative, and false negative. Hence, MCC is more informative compared to accuracy (and F score, which has not been included in the plot, for the sake of simplicity).\nA more balanced model probably downsample approach based on MCC, specificity, and sensitivity. However, this does not mean that downsample technique is the best as I believes each technique behaves differently from one data to another.\nReferences:\nhttps://themis.tidymodels.org/reference/index.html\n\rhttps://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n\rhttps://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-6413-7\r\r\r","date":1620950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620955774,"objectID":"0bee3c59d405022795d0c23d2d3cc134","permalink":"https://tengkuhanis.netlify.app/post/handling-imbalanced-data/","publishdate":"2021-05-14T00:00:00Z","relpermalink":"/post/handling-imbalanced-data/","section":"post","summary":"Overview\rImbalance data happens when there is unequal distribution of data within a categorical outcome variable. Imbalance data occurs due to several reasons such as biased sampling method and measurement errors.","tags":["Machine Learning"],"title":"Handling imbalanced data","type":"post"},{"authors":[],"categories":["R","Deep Learning"],"content":"\r\rI have been reading about lost functions and optimisers in deep learning for the last couple of days when I stumble upon the term Exponentially Weighted Average (EWA). So, in this post I aims to explain my understanding of EWA.\nOverview of EWA\rEWA basically is an important concept in deep learning and have been used in several optimisers to smoothen the noise of the data.\nLet’s see the formula for EWA:\nVt is some smoothen value at point t, while St is a data point at point t. B here is a hyperparameter that we need to tune in our network. So, the choice of B will determine how many data points that we average the value of Vt as shown below:\n\rEWA in deep learnings’ optimiser\rSo, some of the optimisers that adopt the approach of EWA are (red box indicates the EWA part in each formula):\nStochastic gradient descent (SGD) with momentum\r\rThe issue with SGD is the present of noise while searching for global minima. So, SGD with momentum integrated the EWA, which reduces these noises and helps the network converges faster.\nAdaptive delta (Adadelta) and Root Mean Square Propagation (RMSprop)\r\rAdadelta and RMSprop are proposed in attempt to solve the issue of diminishing learning rate of adaptive gradient (Adagrad) optimiser. The use of EWA in both optimisers actually helps to achieve this. Both optimisers have quite a similar formula, but attached below is the formula for Adadelta.\nAdaptive moment estimation (ADAM)\r\rADAM basically combined the SGD with momentum with Adadelta. As shown earlier, both optimisers use EWA.\n\rMore details on EWA\rNow, let’s go back to EWA. Here is the example of calculation of EWA:\nKeep in mind that t3 is the latest time point, followed by t2 and t1, respectively. So, if we want to calculate V3:\nSo, if we were to varies the value of B across the equation (while the values of a1…an remain constant), we can do so in R.\nlibrary(tidyverse) func \u0026lt;- function(b) (1 - b) * b^((20:1) - 1)\rbeta \u0026lt;- seq(0.1, 0.9, by=0.2)\rdat \u0026lt;- t(sapply(beta, func)) %\u0026gt;% as.data.frame()\rcolnames(dat)[1:20] \u0026lt;- 1:20\rdat %\u0026gt;% mutate(beta = as_factor(beta)) %\u0026gt;%\rpivot_longer(cols = 1:20, names_to = \u0026quot;data_point\u0026quot;, values_to = \u0026quot;weight\u0026quot;) %\u0026gt;% ggplot(aes(x=as.numeric(data_point), y=weight, color=beta)) +\rgeom_line() +\rgeom_point() +\rscale_x_continuous(breaks = 1:20) +\rlabs(title = \u0026quot;Change of Exponentially Weighted Average function\u0026quot;, subtitle = \u0026quot;Time at t20 is the recent time, and t1 is the initial time\u0026quot;) +\rscale_colour_discrete(\u0026quot;Beta:\u0026quot;) +\rxlab(\u0026quot;Time(t)\u0026quot;) +\rylab(\u0026quot;Weights/Coefficients\u0026quot;) +\rtheme_bw()\rNote that time at t20 is the recent time, and t1 is the initial time. Thus, two main points from the above plot are:\nThe EWA function acts in a decaying manner.\n\rAs beta, B increases we actually put more emphasize on the recent data point.\r\rSide note: I have tried to do the plot in plotly, not sure why it did not work 😕\nReferences:\n1) https://towardsdatascience.com/deep-learning-optimizers-436171c9e23f (all the equations are from this reference)\n2) https://youtu.be/NxTFlzBjS-4\n3) https://medium.com/@dhartidhami/exponentially-weighted-averages-5de212b5be46\n\r","date":1620518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620542031,"objectID":"877769ed65026a159fb60d317a14861d","permalink":"https://tengkuhanis.netlify.app/post/exponentially-weighted-average-in-deep-learning/","publishdate":"2021-05-09T00:00:00Z","relpermalink":"/post/exponentially-weighted-average-in-deep-learning/","section":"post","summary":"I have been reading about lost functions and optimisers in deep learning for the last couple of days when I stumble upon the term Exponentially Weighted Average (EWA). So, in this post I aims to explain my understanding of EWA.","tags":["Deep Learning"],"title":"Exponentially Weighted Average in Deep Learning","type":"post"},{"authors":null,"categories":["R"],"content":"\r\rFirst of all, this write up is mean for a beginner in R.\nThings can be done in many ways in R. In facts, R has been very flexible in this regard compared to other statistical softwares. Basic things such as selecting a column, slicing a row, filtering a data based on certain condition can be done using a base R function. However, all these things can also be done using a tidyverse approach.\nTidyverse basically, a collection of packages that can be loaded in a line of function.\nlibrary(tidyverse)\rTidyverse is developed by “RStudio people” pioneered by Hadley Wickham, which means that these packages will be continuously maintained and updated.\nSo, without further ado, these are the comparisons between these two approaches for some very basic thingy:\nSelect or deselect a column and a row\r\r# Base R\riris[1:5, c(\u0026quot;Sepal.Length\u0026quot;, \u0026quot;Sepal.Width\u0026quot;)]\riris[1:5,c(1,2)] # similar to above\riris[1:5, -1]\r# Tidyverse\riris %\u0026gt;% select(Sepal.Length, Sepal.Width) %\u0026gt;% slice(1:5)\riris %\u0026gt;% select(-Sepal.Length) %\u0026gt;% slice(1:5)\rFilter based on condition\r\r# Base R\riris[iris$Species == \u0026quot;setosa\u0026quot;, ]\r# Tidyverse\riris %\u0026gt;% filter(Species == \u0026quot;setosa\u0026quot;)\rMutate (transmute replace the variable)\r\r# Base R\riris$SL_minus10 \u0026lt;- iris$Sepal.Length - 10\r# Tidyverse\riris %\u0026gt;% mutate(SL_minus10 = Sepal.Length - 10)\rSort variable\r\r# Base R\riris[order(-iris$Sepal.Width),]\r# Tidyverse\riris %\u0026gt;% arrange(desc(Sepal.Length))\rGroup by (and get mean for variable Sepal.Width)\r\r# Not really base R\rdoBy::summaryBy(Sepal.Width~Species, iris, FUN = mean) # Tidyverse\riris %\u0026gt;% group_by(Species) %\u0026gt;% summarise(mean_SW = mean(Sepal.Width))\rRename variable\r\r# Base R\rcolnames(iris)[6] \u0026lt;- \u0026quot;hanis\u0026quot;\r# Tidyverse\riris %\u0026gt;% rename(Species = hanis)\rSo, that’s it. Overall, tidyverse give a clarity in understanding the code as it reads from left to right. On the contrary, the base R approach reads from inside to outside, especially for a more complicated code.\n","date":1620086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620118129,"objectID":"df69df02c5380516ce81421c1f0ba162","permalink":"https://tengkuhanis.netlify.app/post/2021-05-04-base-r-vs-tidyverse/","publishdate":"2021-05-04T00:00:00Z","relpermalink":"/post/2021-05-04-base-r-vs-tidyverse/","section":"post","summary":"First of all, this write up is mean for a beginner in R.\nThings can be done in many ways in R. In facts, R has been very flexible in this regard compared to other statistical softwares.","tags":["base R","comparison","tidyverse"],"title":"Base R vs tidyverse","type":"post"},{"authors":[],"categories":["R"],"content":"\r\rI have heard quite a several times that apply function is faster than loop function in R. Loop function is said to be inefficient, though in certain situation loop is the only way.\nLet’s compare between loop function and apply function in R.\nFirst, make a very big fake data contain a list of vector.\nset.seed(2021)\rxlist \u0026lt;- list(col1 = rnorm(10000000), col2 = rnorm(10000000),\rcol3 = rnorm(100000000),\rcol4 = rnorm(1000000)) # this will take a few seconds\rThen, calculate the mean of each vector using for loop().\nptm \u0026lt;- proc.time() #-- start the clock\rmean_loop \u0026lt;- vector(\u0026quot;list\u0026quot;, 0) # place holder for a value\rfor (i in seq_along(xlist)) {\rmean_loop[[i]] \u0026lt;- mean(xlist[[i]])\r}\rproc.time() - ptm #-- stop the clock (time in seconds)\r## user system elapsed ## 0.38 0.00 0.38\rNow, using lapply() function.\nptm \u0026lt;- proc.time() #-- start the clock\rmean_apply \u0026lt;- lapply(xlist, mean)\rproc.time() - ptm #-- stop the clock\r## user system elapsed ## 0.36 0.00 0.35\rSo, lapply() is a little bit faster. Obviously, with a very big dataset and a more complicated objective, lapply() is the right choice, but for a “normal” size dataset, the use of any of the two functions probably do not make much different.\n","date":1620086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620121215,"objectID":"c64e5453754ef81813c7c52a0bf44ddb","permalink":"https://tengkuhanis.netlify.app/post/loop-vs-apply-in-r/","publishdate":"2021-05-04T00:00:00Z","relpermalink":"/post/loop-vs-apply-in-r/","section":"post","summary":"I have heard quite a several times that apply function is faster than loop function in R. Loop function is said to be inefficient, though in certain situation loop is the only way.","tags":["base R","comparison"],"title":"Loop vs apply in R","type":"post"},{"authors":null,"categories":["Epidemiology"],"content":"\rRecently I have read an article that the Malaysian government have made a deal with Pfizer for 6.4 million Malaysian to be vaccinated. So, I am wondering what is the minimal number of people should be vaccinated.\nI have also come across this interesting article, which explains how we can calculate a minimal number of people to be vaccinated to achieves herd immunity based on R naught (R0).\nR naught (R0)\nThe basic idea of R0 or basic reproduction number is quite simple. It describes how many secondary infections will derive from the first case. I think Figure 1 below describes this idea very well.\n\rFigure 1: Basic idea of R0(image from https://www.atrainceu.com/content/3-basic-reproduction-number-r-naught)\r\rSo, R0 can be affected by a few factors, such as:\n\rproportion of susceptible people at the initial outbreak\rinfectiousness of the virus or the disease\rrate of recovery or death\rand a few other factors\r\rAs R0 increases more than 1, the spread of the disease will increases, while R0 below 1 indicates the spread of the disease will decrease and eventually dies out.\nHowever, I noticed that quite a few including KKM (Ministry of Health, Malaysia) have used the term R0 in their reports instead of Re or Rt which is the effective reproduction number or time-varying reproduction number. R0 refers to the initial reproduction number at the beginning of the outbreak. The “naught” or “zero” in R naught (R0) is referring to population condition that has zero immunity to the disease.\nHerd immunity\nHerd immunity is said to occur when a significant proportion of the population is immunized. Subsequently, those whose susceptible (not immunized) will be protected.\nHow many should be vaccinated\nSo, back to the initial topic. We can use the formula below to answer this question.\n\\[P_i \u0026gt; 1 - \\frac{1}{R_0}\\]\nPi refers to the number of proportion that should be immunized or in this case, vaccinated.\nSo, after googling, I found one calculation by my lecturer in Biostat Unit, USM, Dr Wan Arifin and his colleague. The R0 based on his calculation is 2.673. Also, I found another article reported that the R0 is 3.55 in March, according to KKM.\nMalaysian’s population is estimated at 32.7 million by the Department of Statistics, Malaysia (DOSM). So, using the formula above, about 63% to 72% of Malaysian population should vaccinated, and this translates to about 20.6 to 23.5 million people.\nThe deal that the Malaysian government made with Pfizer is far from enough, but of course, this is a very good and quick decision. We also have other vaccines like Moderna’s vaccine coming up.\nDisclaimer: This is just my opinion. Please take it with a massive grain of salt.\n","date":1607299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607310799,"objectID":"071dd7efb1b705ccf90c1ab3a155e21a","permalink":"https://tengkuhanis.netlify.app/post/how-many-malaysian-should-be-vaccinated-to-get-herd-immunity-from-covid-19/","publishdate":"2020-12-07T00:00:00Z","relpermalink":"/post/how-many-malaysian-should-be-vaccinated-to-get-herd-immunity-from-covid-19/","section":"post","summary":"Recently I have read an article that the Malaysian government have made a deal with Pfizer for 6.4 million Malaysian to be vaccinated. So, I am wondering what is the minimal number of people should be vaccinated.","tags":["COVID-19","Vaccine"],"title":"How many Malaysian should be vaccinated to get herd immunity from COVID-19?","type":"post"},{"authors":["Tengku Muhammad Hanis"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://tengkuhanis.netlify.app/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://tengkuhanis.netlify.app/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://tengkuhanis.netlify.app/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":[],"title":"External Project","type":"project"},{"authors":["Tengku Muhammad Hanis","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://tengkuhanis.netlify.app/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example journal article","type":"publication"},{"authors":["Tengku Muhammad Hanis","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://tengkuhanis.netlify.app/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"}]