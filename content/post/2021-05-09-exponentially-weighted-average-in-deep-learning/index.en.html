---
title: Exponentially Weighted Average in Deep Learning
author: Tengku Hanis
date: '2021-05-09'
slug: exponentially-weighted-average-in-deep-learning
categories:
  - R
  - Deep Learning
tags:
  - Deep Learning
subtitle: ''
summary: ''
authors: []
lastmod: '2021-05-09T14:33:51+08:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<p>I have been reading about lost functions and optimisers in deep learning for the last couple of days when I stumble upon the term Exponentially Weighted Average (EWA). So, in this post I aims to explain my understanding of EWA.</p>
<div id="overview-of-ewa" class="section level2">
<h2>Overview of EWA</h2>
<p>EWA basically is an important concept in deep learning and have been used in several optimisers to smoothen the noise of the data.</p>
<p>Letâ€™s see the formula for EWA:</p>
<p><img src="formula.png" width="60%" style="display: block; margin: auto;" /></p>
<p><em>V<sub>t</sub></em> is some smoothen value at point <em>t</em>, while <em>S<sub>t</sub></em> is a data point at point <em>t</em>. <em>B</em> here is a hyperparameter that we need to tune in our network. So, the choice of <em>B</em> will determine how many data points that we average the value of <em>V<sub>t</sub></em> as shown below:</p>
<p><img src="beta.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<div id="ewa-in-deep-learnings-optimiser" class="section level2">
<h2>EWA in deep learningsâ€™ optimiser</h2>
<p>So, some of the optimisers that adopt the approach of EWA are (red box indicates the EWA part in each formula):</p>
<ol style="list-style-type: decimal">
<li>Stochastic gradient descent (SGD) with momentum</li>
</ol>
<p>The issue with SGD is the present of noise while searching for global minima. So, SGD with momentum integrated the EWA, which reduces these noises and helps the network converges faster.</p>
<p><img src="SGD-momentum2.png" width="80%" style="display: block; margin: auto;" /></p>
<ol start="2" style="list-style-type: decimal">
<li>Adaptive delta (Adadelta) and Root Mean Square Propagation (RMSprop)</li>
</ol>
<p>Adadelta and RMSprop are proposed in attempt to solve the issue of diminishing learning rate of adaptive gradient (Adagrad) optimiser. The use of EWA in both optimisers actually helps to achieve this. Both optimisers have quite a similar formula, but attached below is the formula for Adadelta.</p>
<p><img src="adadelta2.png" width="80%" style="display: block; margin: auto;" /></p>
<ol start="3" style="list-style-type: decimal">
<li>Adaptive moment estimation (ADAM)</li>
</ol>
<p>ADAM basically combined the SGD with momentum with Adadelta. As shown earlier, both optimisers use EWA.</p>
</div>
<div id="more-details-on-ewa" class="section level2">
<h2>More details on EWA</h2>
<p>Now, letâ€™s go back to EWA. Here is the example of calculation of EWA:</p>
<p><img src="seq1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Keep in mind that <em>t<sub>3</sub></em> is the latest time point, followed by <em>t<sub>2</sub></em> and <em>t<sub>1</sub></em>, respectively. So, if we want to calculate <em>V<sub>3</sub></em>:</p>
<p><img src="seq2.png" width="90%" style="display: block; margin: auto;" /></p>
<p>So, if we were to varies the value of <em>B</em> across the equation (while the values of <em>a<sub>1</sub>â€¦a<sub>n</sub></em> remain constant), we can do so in R.</p>
<pre class="r"><code>library(tidyverse) 

func &lt;- function(b) (1 - b) * b^((20:1) - 1)
beta &lt;- seq(0.1, 0.9, by=0.2)

dat &lt;- t(sapply(beta, func)) %&gt;% 
  as.data.frame()
colnames(dat)[1:20] &lt;- 1:20

dat %&gt;%  
  mutate(beta = as_factor(beta)) %&gt;%
  pivot_longer(cols = 1:20, names_to = &quot;data_point&quot;, values_to = &quot;weight&quot;) %&gt;% 
  ggplot(aes(x=as.numeric(data_point), y=weight, color=beta)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:20) +
  labs(title = &quot;Change of Exponentially Weighted Average function&quot;, 
       subtitle = &quot;Time at t20 is the recent time, and t1 is the initial time&quot;) +
  scale_colour_discrete(&quot;Beta:&quot;) +
  xlab(&quot;Time(t)&quot;) +
  ylab(&quot;Weights/Coefficients&quot;) +
  theme_bw()</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Note that time at t<sub>20</sub> is the recent time, and t<sub>1</sub> is the initial time. Thus, two main points from the above plot are:</p>
<ol style="list-style-type: decimal">
<li>The EWA function acts in a decaying manner.<br />
</li>
<li>As beta, <em>B</em> increases we actually put more emphasize on the recent data point.</li>
</ol>
<p><em>Side note: I have tried to do the plot in plotly, not sure why it did not work</em> ðŸ˜•</p>
<p>References:<br />
1) <a href="https://towardsdatascience.com/deep-learning-optimizers-436171c9e23f" class="uri">https://towardsdatascience.com/deep-learning-optimizers-436171c9e23f</a> (all the equations are from this reference)<br />
2) <a href="https://youtu.be/NxTFlzBjS-4" class="uri">https://youtu.be/NxTFlzBjS-4</a><br />
3) <a href="https://medium.com/@dhartidhami/exponentially-weighted-averages-5de212b5be46" class="uri">https://medium.com/@dhartidhami/exponentially-weighted-averages-5de212b5be46</a></p>
</div>
