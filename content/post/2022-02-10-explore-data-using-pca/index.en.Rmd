---
title: Explore data using PCA
author: ''
date: '2022-02-09'
categories:
  - Machine Learning
tags:
  - Data exploration
  - Machine Learning
  - tidymodels
slug: explore-data-using-pca
lastmod: '2022-02-10T02:46:09+08:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Principal component analysis (PCA)

PCA is a dimension reduction techniques. So, if we have a large number of predictors, instead of using all the predictors for modelling or other analysis, we can compressed all the information from the variables and create a new set of variables. This new set of variables are known as components or principal component (PC). So, now we have a smaller number of variables which contain the information from the original variables. 

PCA usually used for a dataset with a large features or predictors like genomic data. Additionally, PCA is a good pre-processing option if you have a correlated variable or have a multicollinearity issue in the model. Also,  we can use PCA for exploration of the data and have a better understanding of our data. 

For those who want to study the theoretical side of PCA can further read on this [link](http://strata.uga.edu/8370/lecturenotes/principalComponents.html). We going to focus more on the coding part in the machine learning framework (using `tidymodels` package) in this post.

## Example in R

These are the packages that we going to use.

```{r warning=FALSE, message=FALSE}
library(tidymodels)
library(tidyverse)
library(mlbench) #data
```

We going to use diabetes dataset. The outcome is binary; positive = diabetes and negative = non-diabetes/healthy. All other variables are numerical values.

```{r}
data("PimaIndiansDiabetes")
glimpse(PimaIndiansDiabetes)
```

We going to split the data and extract the training dataset. We going to explore only the training set since we going to do this in a machine learning framework.

```{r}
set.seed(1)

ind <- initial_split(PimaIndiansDiabetes)
dat_train <- training(ind)
```

We create a recipe and apply normalization and PCA techniques. Then, we prep it.

```{r}
# Recipe
pca_rec <- 
  recipe(diabetes ~ ., data = dat_train) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_pca(all_numeric_predictors())

# Prep
pca_prep <- prep(pca_rec)
```

So, we can extract the PCA data using `tidy()`. `type = "coef"` indicates that we want the loadings values. So, the values in the data are the loadings. 

```{r}
pca_tidied <- tidy(pca_prep, 2, type = "coef")
pca_tidied
```

So, basically the loadings indicate how much each variable contributes to each component (PC). A large loading (positive or negative) indicates a strong relationship between the variables and the related components. The sign indicates a negative or positive correlation between the variables and components.

We can further visualise these loadings.

```{r}
pca_tidied %>% 
  ggplot(aes(value, terms, fill = terms)) +
  geom_col(show.legend = F) +
  facet_wrap(~ component) +
  ylab("") +
  xlab("Loadings") + 
  theme_minimal()
```

Besides the loadings, we can also get a variance information. Variance of each component (or PC) measures how much that particular component explains the variability in the data. For example, PC1 explain 26.2% variance in the data.  

```{r}
pca_tidied2 <- tidy(pca_prep, 2, type = "variance")

pca_tidied2 %>% 
  pivot_wider(names_from = component, values_from = value, names_prefix = "PC") %>% 
  select(-id) %>% 
  mutate_if(is.numeric, round, digits = 1) %>% 
  kableExtra::kable("simple")
```

Next, we can visualise PC1 and PC2 in a scatter plot and see how each variable influences both PCs. First, we need to extract the loadings and convert into a wide format for our arrow coordinate in the scatter plot.

```{r}
pca_tidied3 <- 
  pca_tidied %>% 
  filter(component %in% c("PC1", "PC2")) %>% 
  select(-id) %>% 
  pivot_wider(names_from = component, values_from = value)
pca_tidied3
```

Now, we can make a scatter plot using training set data (`juice(pca_prep)`) and the loadings data (`pca_tidied3`). Also, we going to add percentage of variance for PC1 and PC2 in the axis labels.

```{r}
juice(pca_prep) %>% 
  ggplot(aes(PC1, PC2)) +
  geom_point(aes(color = diabetes, shape = diabetes), size = 2, alpha = 0.6) +
  geom_segment(data = pca_tidied3, 
               aes(x = 0, y = 0, xend = PC1 * 5, yend = PC2 * 5), 
               arrow = arrow(length = unit(1/2, "picas")),
               color = "blue") +
  annotate("text", 
           x = pca_tidied3$PC1 * 5.2, 
           y = pca_tidied3$PC2 * 5.2, 
           label = pca_tidied3$terms) +
  theme_minimal() +
  xlab("PC1 (26.2%)") +
  ylab("PC2 (21.5%)") 
```

So, from this scatter plot we learn that:

- (triceps, insulin, pedigree and mass), (glucose and pressure) and (pregnant and age) are correlated as their lines are close to each other\
- As PC1 and PC2 increase, triceps, insulin, pedigree and mass also increase
- As PC2 decreases, pregnant and age increase

References:

- <http://strata.uga.edu/8370/lecturenotes/principalComponents.html>\
- <https://juliasilge.com/blog/cocktail-recipes-umap/>
